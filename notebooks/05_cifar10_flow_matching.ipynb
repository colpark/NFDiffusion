{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 Sparse Reconstruction: Flow Matching\n",
    "\n",
    "## Overview\n",
    "\n",
    "**Approach**: Conditional Flow Matching (Modern Approach!)\n",
    "\n",
    "**Key Concept**: Learn vector field that transports noise distribution â†’ data distribution via straight paths.\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "p_0 (Noise) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> p_1 (Data)\n",
    "    x_0          Flow Ï†_t       x_1\n",
    "                  â†‘\n",
    "         Velocity Field v_Î¸(x_t, t | input)\n",
    "```\n",
    "\n",
    "## Theory: Conditional Flow Matching\n",
    "\n",
    "### Flow Equation\n",
    "$$\\frac{dx_t}{dt} = v_\\theta(x_t, t | x^{in})$$\n",
    "\n",
    "where $v_\\theta$ is the learned velocity field conditioned on input pixels.\n",
    "\n",
    "### Conditional Flow (Straight Paths)\n",
    "$$\\phi_t(x_1) = (1-t) \\cdot x_0 + t \\cdot x_1$$\n",
    "\n",
    "where $x_0 \\sim \\mathcal{N}(0, I)$ (noise), $x_1$ (data)\n",
    "\n",
    "### Target Velocity\n",
    "$$u_t(x_1) = \\frac{d\\phi_t}{dt} = x_1 - x_0$$\n",
    "\n",
    "Simply the difference between data and noise!\n",
    "\n",
    "### Training Objective (Simple!)\n",
    "$$\\mathcal{L} = \\mathbb{E}_{t, x_0, x_1} \\left[\\| v_\\theta(\\phi_t(x_1), t | x^{in}) - (x_1 - x_0) \\|^2 \\right]$$\n",
    "\n",
    "Match predicted velocity to true velocity!\n",
    "\n",
    "### Why Flow Matching?\n",
    "- âœ… **Simplest training**: Directly match velocities\n",
    "- âœ… **Fastest sampling**: Straight paths = fewer steps\n",
    "- âœ… **Modern**: Used in Stable Diffusion 3, Flux\n",
    "- âœ… **No score matching**: Simpler than score-based\n",
    "- âœ… **ODE solving**: Deterministic, controllable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "# Import shared components\n",
    "from core.neural_fields.perceiver import PerceiverIO, FourierFeatures\n",
    "from core.sparse.cifar10_sparse import SparseCIFAR10Dataset\n",
    "from core.sparse.metrics import MetricsTracker, print_metrics, visualize_predictions\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Conditional Flow\n",
    "\n",
    "Linear interpolation between noise and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def conditional_flow(x_0, x_1, t):\n    \"\"\"\n    Conditional flow: Ï†_t(x_1) = (1-t) * x_0 + t * x_1\n    \n    Args:\n        x_0: (B, N, 3) noise\n        x_1: (B, N, 3) data\n        t: (B, 1, 1) time in [0, 1]\n    \n    Returns:\n        x_t: (B, N, 3) interpolated state\n    \"\"\"\n    return (1 - t) * x_0 + t * x_1\n\n\ndef target_velocity(x_0, x_1):\n    \"\"\"\n    Target velocity: u_t = x_1 - x_0\n    \n    Args:\n        x_0: (B, N, 3) noise\n        x_1: (B, N, 3) data\n    \n    Returns:\n        u_t: (B, N, 3) target velocity (constant along path!)\n    \"\"\"\n    return x_1 - x_0\n\n\n# Visualize conditional flow\nx_0 = torch.randn(1, 1, 3)\nx_1 = torch.rand(1, 1, 3)\nts = torch.linspace(0, 1, 100).view(-1, 1, 1)\n\nx_t = conditional_flow(x_0, x_1, ts)\n\nplt.figure(figsize=(10, 4))\n\n# Plot each RGB channel separately\nfor i, (color, name) in enumerate(zip(['red', 'green', 'blue'], ['R', 'G', 'B'])):\n    plt.plot(ts.squeeze().numpy(), x_t.squeeze().numpy()[:, i], \n             color=color, alpha=0.7, linewidth=2, label=f'{name} channel')\n    plt.scatter([0], x_0.squeeze().numpy()[i], c=color, s=100, \n                marker='o', edgecolors='black', linewidths=2, zorder=10)\n    plt.scatter([1], x_1.squeeze().numpy()[i], c=color, s=100, \n                marker='s', edgecolors='black', linewidths=2, zorder=10)\n\nplt.xlabel('Time t')\nplt.ylabel('Value')\nplt.title('Conditional Flow: Straight Path from Noise to Data')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Velocity Field Network\n",
    "\n",
    "Learns to predict velocity: v_Î¸(x_t, t | input) â†’ velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalTimeEmbedding(nn.Module):\n",
    "    \"\"\"Sinusoidal time embedding for t in [0, 1]\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    \n",
    "    def forward(self, t):\n",
    "        \"\"\"t: (B,) time in [0, 1]\"\"\"\n",
    "        device = t.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat([emb.sin(), emb.cos()], dim=-1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "class VelocityField(nn.Module):\n",
    "    \"\"\"\n",
    "    Velocity Field: v_Î¸(x_t, coords, t | input) â†’ velocity\n",
    "    \n",
    "    Predicts how to transport x_t toward data x_1\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_latents=512,\n",
    "        latent_dim=512,\n",
    "        num_fourier_feats=256,\n",
    "        num_blocks=6,\n",
    "        num_heads=8,\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Perceiver IO backbone\n",
    "        self.perceiver = PerceiverIO(\n",
    "            input_channels=3,\n",
    "            output_channels=3,\n",
    "            num_latents=num_latents,\n",
    "            latent_dim=latent_dim,\n",
    "            num_fourier_feats=num_fourier_feats,\n",
    "            num_blocks=num_blocks,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Time embedding\n",
    "        self.time_embed = SinusoidalTimeEmbedding(latent_dim)\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(latent_dim, latent_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(latent_dim, latent_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x_t, output_coords, t, input_coords, input_values):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_t: (B, N_out, 3) current state on flow path\n",
    "            output_coords: (B, N_out, 2) output coordinates\n",
    "            t: (B,) time in [0, 1]\n",
    "            input_coords: (B, N_in, 2) input coordinates\n",
    "            input_values: (B, N_in, 3) input values (conditioning)\n",
    "        \n",
    "        Returns:\n",
    "            velocity: (B, N_out, 3) predicted velocity\n",
    "        \"\"\"\n",
    "        B = x_t.shape[0]\n",
    "        \n",
    "        # Time embedding\n",
    "        t_emb = self.time_mlp(self.time_embed(t))  # (B, latent_dim)\n",
    "        \n",
    "        # Add time to pixel values\n",
    "        time_signal = t_emb[:, :3].unsqueeze(1)  # (B, 1, 3)\n",
    "        \n",
    "        input_values_t = input_values + time_signal\n",
    "        x_t_emb = x_t + time_signal\n",
    "        \n",
    "        # Concatenate input (conditioning) and output (current state)\n",
    "        all_coords = torch.cat([input_coords, output_coords], dim=1)\n",
    "        all_values = torch.cat([input_values_t, x_t_emb], dim=1)\n",
    "        \n",
    "        # Predict velocity at output coordinates\n",
    "        velocity = self.perceiver(all_coords, all_values, output_coords)\n",
    "        \n",
    "        return velocity\n",
    "\n",
    "\n",
    "# Test velocity field\n",
    "model = VelocityField().to(device)\n",
    "test_x_t = torch.rand(4, 204, 3).to(device)\n",
    "test_output_coords = torch.rand(4, 204, 2).to(device)\n",
    "test_t = torch.rand(4).to(device)\n",
    "test_input_coords = torch.rand(4, 204, 2).to(device)\n",
    "test_input_values = torch.rand(4, 204, 3).to(device)\n",
    "\n",
    "test_vel = model(test_x_t, test_output_coords, test_t, test_input_coords, test_input_values)\n",
    "print(f\"Velocity field test: {test_vel.shape}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Sampling: ODE Solver\n\nDefine sampling functions before training\n\nSolve dx/dt = v_Î¸(x_t, t | input) from t=0 to t=1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@torch.no_grad()\ndef euler_sample(\n    model,\n    output_coords,\n    input_coords,\n    input_values,\n    num_steps=50,\n    device='cuda'\n):\n    \"\"\"\n    Euler method for ODE: dx/dt = v_Î¸(x_t, t | input)\n    \n    Args:\n        model: Trained velocity field\n        output_coords: (B, N_out, 2)\n        input_coords: (B, N_in, 2)\n        input_values: (B, N_in, 3) conditioning\n        num_steps: Number of integration steps\n    \n    Returns:\n        x_1: (B, N_out, 3) predicted data at t=1\n    \"\"\"\n    B = output_coords.shape[0]\n    N_out = output_coords.shape[1]\n    \n    # Start from noise at t=0\n    x_t = torch.randn(B, N_out, 3, device=device)\n    \n    # Time discretization\n    dt = 1.0 / num_steps\n    ts = torch.linspace(0, 1 - dt, num_steps)\n    \n    for t_val in tqdm(ts, desc=\"Sampling (Euler)\", leave=False):\n        t = torch.full((B,), t_val.item(), device=device)\n        \n        # Predict velocity\n        velocity = model(x_t, output_coords, t, input_coords, input_values)\n        \n        # Euler update: x_{t+dt} = x_t + dt * v_Î¸(x_t, t)\n        x_t = x_t + dt * velocity\n    \n    return torch.clamp(x_t, 0, 1)\n\n\n@torch.no_grad()\ndef heun_sample(\n    model,\n    output_coords,\n    input_coords,\n    input_values,\n    num_steps=50,\n    device='cuda'\n):\n    \"\"\"\n    Heun's method (improved Euler) for better accuracy\n    \n    Args:\n        Same as euler_sample\n    \n    Returns:\n        x_1: (B, N_out, 3) predicted data at t=1\n    \"\"\"\n    B = output_coords.shape[0]\n    N_out = output_coords.shape[1]\n    \n    x_t = torch.randn(B, N_out, 3, device=device)\n    \n    dt = 1.0 / num_steps\n    ts = torch.linspace(0, 1 - dt, num_steps)\n    \n    for t_val in tqdm(ts, desc=\"Sampling (Heun)\", leave=False):\n        t = torch.full((B,), t_val.item(), device=device)\n        t_next = torch.full((B,), t_val.item() + dt, device=device)\n        \n        # First step (Euler predictor)\n        v1 = model(x_t, output_coords, t, input_coords, input_values)\n        x_next_pred = x_t + dt * v1\n        \n        # Second step (corrector)\n        v2 = model(x_next_pred, output_coords, t_next, input_coords, input_values)\n        \n        # Heun update (average of two velocities)\n        x_t = x_t + dt * 0.5 * (v1 + v2)\n    \n    return torch.clamp(x_t, 0, 1)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Training: Conditional Flow Matching with Evaluation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_flow_matching(\n    model,\n    train_loader,\n    test_loader,\n    epochs=100,\n    lr=1e-4,\n    device='cuda',\n    visualize_every=5,\n    eval_every=2\n):\n    \"\"\"Train velocity field with conditional flow matching\"\"\"\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n    \n    losses = []\n    \n    # Get a fixed batch for visualization\n    viz_batch = next(iter(train_loader))\n    viz_input_coords = viz_batch['input_coords'][:4].to(device)\n    viz_input_values = viz_batch['input_values'][:4].to(device)\n    viz_output_coords = viz_batch['output_coords'][:4].to(device)\n    viz_output_values = viz_batch['output_values'][:4].to(device)\n    viz_full_images = viz_batch['full_image'][:4].to(device)\n    \n    model.train()\n    for epoch in range(epochs):\n        epoch_loss = 0\n        \n        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n            input_coords = batch['input_coords'].to(device)\n            input_values = batch['input_values'].to(device)\n            output_coords = batch['output_coords'].to(device)\n            output_values = batch['output_values'].to(device)  # x_1 (data)\n            \n            B = input_coords.shape[0]\n            \n            # Sample random time t ~ Uniform(0, 1)\n            t = torch.rand(B, device=device)\n            \n            # Sample noise x_0 ~ N(0, I)\n            x_0 = torch.randn_like(output_values)\n            x_1 = output_values\n            \n            # Conditional flow: x_t = (1-t) * x_0 + t * x_1\n            t_broadcast = t.view(B, 1, 1)\n            x_t = conditional_flow(x_0, x_1, t_broadcast)\n            \n            # Target velocity: u_t = x_1 - x_0 (constant!)\n            u_t = target_velocity(x_0, x_1)\n            \n            # Predict velocity\n            v_pred = model(x_t, output_coords, t, input_coords, input_values)\n            \n            # Loss: match predicted velocity to target velocity\n            loss = F.mse_loss(v_pred, u_t)\n            \n            # Backward\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            \n            epoch_loss += loss.item()\n        \n        avg_loss = epoch_loss / len(train_loader)\n        losses.append(avg_loss)\n        scheduler.step()\n        \n        print(f\"Epoch {epoch+1}: Loss = {avg_loss:.6f}, LR = {scheduler.get_last_lr()[0]:.6f}\")\n        \n        # Evaluate every N epochs\n        if (epoch + 1) % eval_every == 0 or epoch == 0:\n            model.eval()\n            with torch.no_grad():\n                tracker = MetricsTracker()\n                \n                # Evaluate on small subset for speed\n                for i, batch in enumerate(test_loader):\n                    if i >= 10:\n                        break\n                    \n                    input_coords = batch['input_coords'].to(device)\n                    input_values = batch['input_values'].to(device)\n                    output_coords = batch['output_coords'].to(device)\n                    output_values = batch['output_values'].to(device)\n                    \n                    # Generate predictions\n                    pred_values = heun_sample(\n                        model, output_coords, input_coords, input_values,\n                        num_steps=50, device=device\n                    )\n                    \n                    tracker.update(pred_values, output_values)\n                \n                results = tracker.compute()\n                print(f\"  Eval - MSE: {results['mse']:.6f}, MAE: {results['mae']:.6f}\")\n            \n            model.train()\n        \n        # Visualize predictions every N epochs\n        if (epoch + 1) % visualize_every == 0 or epoch == 0:\n            model.eval()\n            with torch.no_grad():\n                # Generate predictions using Heun ODE solver\n                pred_values = heun_sample(\n                    model, viz_output_coords, \n                    viz_input_coords, viz_input_values,\n                    num_steps=50, device=device\n                )\n                \n                # Visualize\n                fig = visualize_predictions(\n                    viz_input_coords, viz_input_values,\n                    viz_output_coords, pred_values, viz_output_values,\n                    viz_full_images, n_samples=4\n                )\n                plt.suptitle(f'Flow Matching - Epoch {epoch+1}/{epochs}', fontsize=14, y=1.02)\n                plt.savefig(f'flow_matching_epoch_{epoch+1:03d}.png', dpi=150, bbox_inches='tight')\n                plt.show()\n                plt.close()\n            \n            model.train()\n        \n        # Save checkpoint\n        if (epoch + 1) % 10 == 0:\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'loss': avg_loss,\n            }, f'flow_matching_epoch_{epoch+1}.pt')\n    \n    return losses\n\n\n# Create dataset\nprint(\"Loading CIFAR-10...\")\ntrain_dataset = SparseCIFAR10Dataset(\n    root='../data',\n    train=True,\n    input_ratio=0.2,\n    output_ratio=0.2,\n    download=True,\n    seed=42\n)\n\ntest_dataset = SparseCIFAR10Dataset(\n    root='../data',\n    train=False,\n    input_ratio=0.2,\n    output_ratio=0.2,\n    download=True,\n    seed=42\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)\n\nprint(f\"Train dataset size: {len(train_dataset)}\")\nprint(f\"Test dataset size: {len(test_dataset)}\")\n\n# Initialize\nmodel = VelocityField(\n    num_latents=512,\n    latent_dim=512,\n    num_fourier_feats=256,\n    num_blocks=6,\n    num_heads=8\n).to(device)\n\n# Train\nprint(\"\\nStarting training...\")\nlosses = train_flow_matching(model, train_loader, test_loader, \n                             epochs=100, lr=1e-4, device=device, \n                             visualize_every=5, eval_every=2)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Final Evaluation: Full Image Reconstruction"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot training loss\nplt.figure(figsize=(10, 4))\nplt.plot(losses, linewidth=2)\nplt.xlabel('Epoch')\nplt.ylabel('Velocity Matching Loss')\nplt.title('Training Loss: Flow Matching')\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# Final Evaluation: Reconstruct FULL images (all 1024 pixels)\nprint(\"\\n\" + \"=\"*70)\nprint(\"FINAL EVALUATION: Full Image Reconstruction (1024 pixels)\")\nprint(\"=\"*70)\n\nmodel.eval()\n\n# Create full grid of coordinates for 32x32 image\ndef create_full_grid(image_size=32, device='cuda'):\n    \"\"\"Create coordinate grid for full image\"\"\"\n    y, x = torch.meshgrid(\n        torch.linspace(0, 1, image_size),\n        torch.linspace(0, 1, image_size),\n        indexing='ij'\n    )\n    coords = torch.stack([x.flatten(), y.flatten()], dim=-1)  # (1024, 2)\n    return coords.to(device)\n\nfull_coords = create_full_grid(32, device)  # (1024, 2)\n\n# Evaluate on test set - reconstruct FULL images\ntracker_full = MetricsTracker()\n\nfor i, batch in enumerate(tqdm(test_loader, desc=\"Full Image Reconstruction\")):\n    if i >= 50:  # Evaluate on 50 batches = 800 images\n        break\n    \n    input_coords = batch['input_coords'].to(device)\n    input_values = batch['input_values'].to(device)\n    full_images = batch['full_image'].to(device)\n    \n    B = input_coords.shape[0]\n    \n    # Predict ALL pixels (1024) conditioned on sparse input (204)\n    full_coords_batch = full_coords.unsqueeze(0).expand(B, -1, -1)  # (B, 1024, 2)\n    \n    pred_values = heun_sample(\n        model, full_coords_batch, input_coords, input_values,\n        num_steps=100, device=device\n    )\n    \n    # Reshape predictions to image format\n    pred_images = pred_values.view(B, 32, 32, 3).permute(0, 3, 1, 2)  # (B, 3, 32, 32)\n    \n    # Compute metrics on full images\n    tracker_full.update(None, None, pred_images, full_images)\n\n# Print results\nresults_full = tracker_full.compute()\nprint(\"\\nFull Image Reconstruction Results:\")\nprint(f\"  PSNR: {results_full['psnr']:.2f} dB\")\nprint(f\"  SSIM: {results_full['ssim']:.4f}\")\nprint(\"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Visualize Full Image Reconstructions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize full image reconstructions\nsample_batch = next(iter(test_loader))\ninput_coords = sample_batch['input_coords'][:4].to(device)\ninput_values = sample_batch['input_values'][:4].to(device)\nfull_images = sample_batch['full_image'][:4].to(device)\n\nB = input_coords.shape[0]\nfull_coords_batch = full_coords.unsqueeze(0).expand(B, -1, -1)\n\n# Generate FULL image predictions\npred_values = heun_sample(\n    model, full_coords_batch, input_coords, input_values,\n    num_steps=100, device=device\n)\n\npred_images = pred_values.view(B, 32, 32, 3).permute(0, 3, 1, 2)\n\n# Visualize\nfig, axes = plt.subplots(4, 3, figsize=(12, 16))\n\nfor i in range(4):\n    # Ground truth\n    gt_img = full_images[i].permute(1, 2, 0).cpu().numpy()\n    axes[i, 0].imshow(gt_img)\n    axes[i, 0].set_title('Ground Truth')\n    axes[i, 0].axis('off')\n    \n    # Sparse input (visualize the 20% input pixels)\n    input_img = torch.zeros(3, 32, 32, device=device)\n    input_idx = sample_batch['input_indices'][i].to(device)\n    input_img.view(3, -1)[:, input_idx] = input_values[i].T\n    axes[i, 1].imshow(input_img.permute(1, 2, 0).cpu().numpy())\n    axes[i, 1].set_title(f'Input (20% = {len(input_idx)} pixels)')\n    axes[i, 1].axis('off')\n    \n    # Full reconstruction\n    pred_img = pred_images[i].permute(1, 2, 0).cpu().numpy()\n    axes[i, 2].imshow(np.clip(pred_img, 0, 1))\n    axes[i, 2].set_title('Reconstructed (100%)')\n    axes[i, 2].axis('off')\n\nplt.suptitle('Flow Matching: Full Image Reconstruction from 20% Sparse Input', fontsize=14, y=0.995)\nplt.tight_layout()\nplt.savefig('flow_matching_full_reconstruction.png', dpi=150, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### âœ… Implemented\n",
    "- Conditional flow matching\n",
    "- Straight-path interpolation\n",
    "- Velocity field prediction\n",
    "- Heun ODE solver (2nd order)\n",
    "\n",
    "### ðŸ“Š Results\n",
    "See metrics above for:\n",
    "- MSE/MAE on output pixels\n",
    "- PSNR/SSIM on full images\n",
    "\n",
    "### âš–ï¸ Strengths & Weaknesses\n",
    "\n",
    "**Strengths**:\n",
    "- âœ… **Simplest training**: Direct velocity matching\n",
    "- âœ… **Fastest sampling**: Straight paths, 20-50 steps\n",
    "- âœ… **Modern approach**: Used in SD3, Flux\n",
    "- âœ… **Deterministic**: ODE solving, reproducible\n",
    "- âœ… **Flexible**: Can use various ODE solvers\n",
    "\n",
    "**Potential Weaknesses**:\n",
    "- âš ï¸ Less explored for sparse conditioning\n",
    "- âš ï¸ May need careful solver selection\n",
    "\n",
    "### ðŸ”„ Comparison with Other Approaches\n",
    "\n",
    "| Metric | Score-Based | NF Denoiser | Flow Matching |\n",
    "|--------|-------------|-------------|---------------|\n",
    "| Training | Complex (score matching) | Simple (MSE) | Simplest (velocity MSE) |\n",
    "| Sampling | Slow (Langevin) | Fast (DDIM) | Fastest (ODE) |\n",
    "| Steps | 100-1000 | 50-100 | 20-50 |\n",
    "| Theory | Mature | Established | Modern |\n",
    "\n",
    "### ðŸ† Final Verdict\n",
    "\n",
    "Run all 3 notebooks and compare quantitative metrics!\n",
    "\n",
    "**Expected Winner**:\n",
    "- **Quality**: All three should be similar\n",
    "- **Speed**: Flow Matching â‰ˆ NF Denoiser > Score-Based\n",
    "- **Simplicity**: Flow Matching > NF Denoiser > Score-Based\n",
    "\n",
    "**Best Overall**: Likely Flow Matching or NF Denoiser depending on your priorities!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}