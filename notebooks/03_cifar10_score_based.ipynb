{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 Sparse Reconstruction: Score-Based Neural Field\n",
    "\n",
    "## Overview\n",
    "\n",
    "**Approach**: Score-based diffusion with Perceiver IO + Fourier features\n",
    "\n",
    "**Task**: Predict 20% output pixels from different 20% input pixels\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "Input: 204 input pixels (coords + RGB)\n",
    "Output: 204 different pixels (coords only)\n",
    "       ‚Üì\n",
    "Score Network s_Œ∏(x_t, coords, t | input)\n",
    "       ‚Üì\n",
    "Predicted Score: ‚àá_x log p_t(x | input)\n",
    "       ‚Üì\n",
    "Langevin Sampling ‚Üí Clean RGB predictions\n",
    "```\n",
    "\n",
    "## Theory: Score-Based Generative Modeling\n",
    "\n",
    "### Forward SDE (Add Noise)\n",
    "$$dx = -\\frac{\\beta_t}{2} x dt + \\sqrt{\\beta_t} dw$$\n",
    "\n",
    "where $w$ is Brownian motion.\n",
    "\n",
    "### Reverse SDE (Remove Noise)\n",
    "$$dx = \\left[-\\frac{\\beta_t}{2} x - \\beta_t \\nabla_x \\log p_t(x)\\right] dt + \\sqrt{\\beta_t} dw$$\n",
    "\n",
    "### Score Function\n",
    "Learn $s_\\theta(x, t) \\approx \\nabla_x \\log p_t(x)$ (gradient of log-density)\n",
    "\n",
    "### Training Objective (Denoising Score Matching)\n",
    "$$\\mathcal{L} = \\mathbb{E}_{t, x_0, \\epsilon} \\left[\\lambda(t) \\left\\| s_\\theta(x_t, t) + \\frac{\\epsilon}{\\sigma_t} \\right\\|^2 \\right]$$\n",
    "\n",
    "where $x_t = \\alpha_t x_0 + \\sigma_t \\epsilon$, $\\epsilon \\sim \\mathcal{N}(0, I)$\n",
    "\n",
    "### Why Score-Based?\n",
    "- ‚úÖ Principled probabilistic framework\n",
    "- ‚úÖ Flexible noise schedules\n",
    "- ‚úÖ Well-studied theory (Song et al., 2021)\n",
    "- ‚ö†Ô∏è Langevin sampling slower than ODE solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "# Import shared components\n",
    "from core.neural_fields.perceiver import PerceiverIO, FourierFeatures\n",
    "from core.sparse.cifar10_sparse import SparseCIFAR10Dataset\n",
    "from core.sparse.metrics import MetricsTracker, print_metrics, visualize_predictions\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Noise Schedule\n",
    "\n",
    "Variance Preserving (VP) SDE with cosine schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    \"\"\"Cosine schedule from Improved DDPM\"\"\"\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0.0001, 0.9999)\n",
    "\n",
    "\n",
    "class NoiseSchedule:\n",
    "    \"\"\"VP-SDE noise schedule\"\"\"\n",
    "    def __init__(self, timesteps=1000, beta_schedule='cosine'):\n",
    "        self.timesteps = timesteps\n",
    "        \n",
    "        if beta_schedule == 'cosine':\n",
    "            betas = cosine_beta_schedule(timesteps)\n",
    "        else:\n",
    "            betas = torch.linspace(0.0001, 0.02, timesteps)\n",
    "        \n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        \n",
    "        self.betas = betas\n",
    "        self.alphas = alphas\n",
    "        self.alphas_cumprod = alphas_cumprod\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n",
    "\n",
    "\n",
    "# Visualize schedule\n",
    "schedule = NoiseSchedule(timesteps=1000)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(schedule.betas.numpy())\n",
    "plt.title('Œ≤_t (noise variance)')\n",
    "plt.xlabel('Timestep')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(schedule.alphas_cumprod.numpy())\n",
    "plt.title('·æ±_t (signal retention)')\n",
    "plt.xlabel('Timestep')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(schedule.sqrt_one_minus_alphas_cumprod.numpy(), label='œÉ_t (noise std)')\n",
    "plt.plot(schedule.sqrt_alphas_cumprod.numpy(), label='Œ±_t (signal std)')\n",
    "plt.title('Signal vs Noise')\n",
    "plt.xlabel('Timestep')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Score Network Architecture\n",
    "\n",
    "Perceiver IO backbone + time conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalTimeEmbedding(nn.Module):\n",
    "    \"\"\"Sinusoidal time embedding\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    \n",
    "    def forward(self, t):\n",
    "        \"\"\"t: (B,) continuous timesteps in [0, T]\"\"\"\n",
    "        device = t.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat([emb.sin(), emb.cos()], dim=-1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "class ScoreNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Score network: s_Œ∏(x_t, coords, t | input) ‚Üí score\n",
    "    \n",
    "    Predicts ‚àá_x log p_t(x | input) for output pixel values\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_latents=512,\n",
    "        latent_dim=512,\n",
    "        num_fourier_feats=256,\n",
    "        num_blocks=6,\n",
    "        num_heads=8,\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Perceiver IO backbone\n",
    "        self.perceiver = PerceiverIO(\n",
    "            input_channels=3,\n",
    "            output_channels=3,\n",
    "            num_latents=num_latents,\n",
    "            latent_dim=latent_dim,\n",
    "            num_fourier_feats=num_fourier_feats,\n",
    "            num_blocks=num_blocks,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Time embedding\n",
    "        self.time_embed = SinusoidalTimeEmbedding(latent_dim)\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(latent_dim, latent_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(latent_dim, latent_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, noisy_values, output_coords, t, input_coords, input_values):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            noisy_values: (B, N_out, 3) noisy output pixel values\n",
    "            output_coords: (B, N_out, 2) output pixel coordinates\n",
    "            t: (B,) continuous timesteps\n",
    "            input_coords: (B, N_in, 2) input pixel coordinates\n",
    "            input_values: (B, N_in, 3) input pixel values (clean)\n",
    "        \n",
    "        Returns:\n",
    "            score: (B, N_out, 3) predicted score ‚àá_x log p_t(x | input)\n",
    "        \"\"\"\n",
    "        B = noisy_values.shape[0]\n",
    "        \n",
    "        # Time embedding\n",
    "        t_emb = self.time_mlp(self.time_embed(t))  # (B, latent_dim)\n",
    "        \n",
    "        # Concatenate input (clean) and output (noisy) pixels\n",
    "        all_coords = torch.cat([input_coords, output_coords], dim=1)  # (B, N_in+N_out, 2)\n",
    "        \n",
    "        # Add time embedding to pixel values\n",
    "        input_values_t = input_values + t_emb.unsqueeze(1).expand(-1, input_values.shape[1], -1)[:, :, :3]\n",
    "        noisy_values_t = noisy_values + t_emb.unsqueeze(1).expand(-1, noisy_values.shape[1], -1)[:, :, :3]\n",
    "        \n",
    "        all_values = torch.cat([input_values_t, noisy_values_t], dim=1)  # (B, N_in+N_out, 3)\n",
    "        \n",
    "        # Predict score at output coordinates\n",
    "        score = self.perceiver(all_coords, all_values, output_coords)\n",
    "        \n",
    "        return score\n",
    "\n",
    "\n",
    "# Test score network\n",
    "model = ScoreNetwork().to(device)\n",
    "test_noisy = torch.randn(4, 204, 3).to(device)\n",
    "test_output_coords = torch.rand(4, 204, 2).to(device)\n",
    "test_t = torch.rand(4).to(device) * 1000\n",
    "test_input_coords = torch.rand(4, 204, 2).to(device)\n",
    "test_input_values = torch.rand(4, 204, 3).to(device)\n",
    "\n",
    "test_score = model(test_noisy, test_output_coords, test_t, test_input_coords, test_input_values)\n",
    "print(f\"Score network test: {test_score.shape}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training: Denoising Score Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_score_matching(\n",
    "    model,\n",
    "    train_loader,\n",
    "    schedule,\n",
    "    epochs=100,\n",
    "    lr=1e-4,\n",
    "    device='cuda'\n",
    "):\n",
    "    \"\"\"Train score network with denoising score matching\"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            input_coords = batch['input_coords'].to(device)\n",
    "            input_values = batch['input_values'].to(device)\n",
    "            output_coords = batch['output_coords'].to(device)\n",
    "            output_values = batch['output_values'].to(device)  # x_0 (clean)\n",
    "            \n",
    "            B = input_coords.shape[0]\n",
    "            \n",
    "            # Sample random timestep\n",
    "            t = torch.randint(0, schedule.timesteps, (B,), device=device).float()\n",
    "            \n",
    "            # Add noise: x_t = Œ±_t * x_0 + œÉ_t * Œµ\n",
    "            noise = torch.randn_like(output_values)\n",
    "            sqrt_alpha_t = schedule.sqrt_alphas_cumprod[t.long()].view(B, 1, 1).to(device)\n",
    "            sqrt_sigma_t = schedule.sqrt_one_minus_alphas_cumprod[t.long()].view(B, 1, 1).to(device)\n",
    "            \n",
    "            noisy_values = sqrt_alpha_t * output_values + sqrt_sigma_t * noise\n",
    "            \n",
    "            # Predict score\n",
    "            pred_score = model(noisy_values, output_coords, t, input_coords, input_values)\n",
    "            \n",
    "            # Target score: -Œµ / œÉ_t\n",
    "            target_score = -noise / sqrt_sigma_t\n",
    "            \n",
    "            # Loss: ||predicted_score - target_score||^2\n",
    "            loss = F.mse_loss(pred_score, target_score)\n",
    "            \n",
    "            # Backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        losses.append(avg_loss)\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Loss = {avg_loss:.6f}, LR = {scheduler.get_last_lr()[0]:.6f}\")\n",
    "        \n",
    "        # Save checkpoint every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': avg_loss,\n",
    "            }, f'score_network_epoch_{epoch+1}.pt')\n",
    "    \n",
    "    return losses\n",
    "\n",
    "\n",
    "# Create dataset and dataloader\n",
    "print(\"Loading CIFAR-10...\")\n",
    "train_dataset = SparseCIFAR10Dataset(\n",
    "    root='../data',\n",
    "    train=True,\n",
    "    input_ratio=0.2,\n",
    "    output_ratio=0.2,\n",
    "    download=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "\n",
    "print(f\"Dataset size: {len(train_dataset)}\")\n",
    "print(f\"Batches: {len(train_loader)}\")\n",
    "\n",
    "# Initialize model and schedule\n",
    "model = ScoreNetwork(\n",
    "    num_latents=512,\n",
    "    latent_dim=512,\n",
    "    num_fourier_feats=256,\n",
    "    num_blocks=6,\n",
    "    num_heads=8\n",
    ").to(device)\n",
    "\n",
    "schedule = NoiseSchedule(timesteps=1000, beta_schedule='cosine')\n",
    "\n",
    "# Train\n",
    "print(\"\\nStarting training...\")\n",
    "losses = train_score_matching(model, train_loader, schedule, epochs=100, lr=1e-4, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sampling: Langevin Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def langevin_sample(\n",
    "    model,\n",
    "    schedule,\n",
    "    output_coords,\n",
    "    input_coords,\n",
    "    input_values,\n",
    "    num_steps=1000,\n",
    "    device='cuda'\n",
    "):\n",
    "    \"\"\"\n",
    "    Sample using Langevin dynamics (reverse SDE)\n",
    "    \n",
    "    Args:\n",
    "        model: Trained score network\n",
    "        schedule: Noise schedule\n",
    "        output_coords: (B, N_out, 2) coordinates to predict\n",
    "        input_coords: (B, N_in, 2) input coordinates\n",
    "        input_values: (B, N_in, 3) input values (conditioning)\n",
    "        num_steps: Number of sampling steps\n",
    "    \n",
    "    Returns:\n",
    "        x_0: (B, N_out, 3) predicted clean values\n",
    "    \"\"\"\n",
    "    B = output_coords.shape[0]\n",
    "    N_out = output_coords.shape[1]\n",
    "    \n",
    "    # Start from pure noise\n",
    "    x_t = torch.randn(B, N_out, 3, device=device)\n",
    "    \n",
    "    # Langevin dynamics\n",
    "    timesteps = torch.linspace(schedule.timesteps - 1, 0, num_steps).long()\n",
    "    \n",
    "    for i, t_idx in enumerate(tqdm(timesteps, desc=\"Sampling\")):\n",
    "        t = torch.full((B,), t_idx.item(), device=device, dtype=torch.float)\n",
    "        \n",
    "        # Predict score\n",
    "        score = model(x_t, output_coords, t, input_coords, input_values)\n",
    "        \n",
    "        # Get noise schedule params\n",
    "        beta_t = schedule.betas[t_idx].to(device)\n",
    "        sqrt_beta_t = torch.sqrt(beta_t)\n",
    "        \n",
    "        # Langevin update with score\n",
    "        if i < len(timesteps) - 1:\n",
    "            noise = torch.randn_like(x_t)\n",
    "            x_t = x_t + beta_t * score + sqrt_beta_t * noise\n",
    "        else:\n",
    "            # Last step: no noise\n",
    "            x_t = x_t + beta_t * score\n",
    "    \n",
    "    return torch.clamp(x_t, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation with Unified Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses, linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Score Matching Loss')\n",
    "plt.title('Training Loss: Score-Based Neural Field')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "test_dataset = SparseCIFAR10Dataset(\n",
    "    root='../data',\n",
    "    train=False,\n",
    "    input_ratio=0.2,\n",
    "    output_ratio=0.2,\n",
    "    download=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
    "\n",
    "model.eval()\n",
    "tracker = MetricsTracker()\n",
    "\n",
    "for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "    input_coords = batch['input_coords'].to(device)\n",
    "    input_values = batch['input_values'].to(device)\n",
    "    output_coords = batch['output_coords'].to(device)\n",
    "    output_values = batch['output_values'].to(device)\n",
    "    \n",
    "    # Sample predictions\n",
    "    pred_values = langevin_sample(\n",
    "        model, schedule, output_coords, input_coords, input_values,\n",
    "        num_steps=100, device=device\n",
    "    )\n",
    "    \n",
    "    tracker.update(pred_values, output_values)\n",
    "\n",
    "# Print results\n",
    "results = tracker.compute()\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SCORE-BASED NEURAL FIELD - Test Results\")\n",
    "print(\"=\"*50)\n",
    "print_metrics(results)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "sample_batch = next(iter(test_loader))\n",
    "input_coords = sample_batch['input_coords'].to(device)\n",
    "input_values = sample_batch['input_values'].to(device)\n",
    "output_coords = sample_batch['output_coords'].to(device)\n",
    "output_values = sample_batch['output_values'].to(device)\n",
    "full_images = sample_batch['full_image'].to(device)\n",
    "\n",
    "# Generate predictions\n",
    "pred_values = langevin_sample(\n",
    "    model, schedule, output_coords, input_coords, input_values,\n",
    "    num_steps=100, device=device\n",
    ")\n",
    "\n",
    "# Visualize\n",
    "fig = visualize_predictions(\n",
    "    input_coords, input_values,\n",
    "    output_coords, pred_values, output_values,\n",
    "    full_images, n_samples=4\n",
    ")\n",
    "plt.suptitle('Score-Based Neural Field: Predictions vs Ground Truth', fontsize=14, y=1.02)\n",
    "plt.savefig('score_based_predictions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### ‚úÖ Implemented\n",
    "- Score-based diffusion with Perceiver IO\n",
    "- Fourier feature positional encoding\n",
    "- Denoising score matching training\n",
    "- Langevin dynamics sampling\n",
    "\n",
    "### üìä Results\n",
    "See metrics above for:\n",
    "- MSE/MAE on output pixels\n",
    "- PSNR/SSIM on full images\n",
    "\n",
    "### ‚öñÔ∏è Strengths & Weaknesses\n",
    "\n",
    "**Strengths**:\n",
    "- Principled probabilistic framework\n",
    "- Flexible for uncertainty quantification\n",
    "- Well-studied theory\n",
    "\n",
    "**Weaknesses**:\n",
    "- Langevin sampling slower (100-1000 steps)\n",
    "- Requires careful tuning of step sizes\n",
    "- More complex than direct denoising\n",
    "\n",
    "### üîÑ Next\n",
    "Compare with:\n",
    "- Notebook 4: NF Denoiser (Gaussian RF)\n",
    "- Notebook 5: Flow Matching"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
