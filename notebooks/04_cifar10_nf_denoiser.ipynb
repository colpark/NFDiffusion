{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 Sparse Reconstruction: Neural Field as Denoiser\n",
    "\n",
    "## Overview\n",
    "\n",
    "**Approach**: Gaussian Random Field Denoising (Your Idea!)\n",
    "\n",
    "**Key Concept**: Start with Gaussian noise at output pixel locations, progressively denoise using neural field while keeping input pixels clean and fixed.\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "Step T: Gaussian Noise at Output Locations\n",
    "        + Clean Input Pixels (FIXED)\n",
    "        ‚Üì\n",
    "Neural Field Denoiser f_Œ∏(noisy_out, clean_in, coords, t)\n",
    "        ‚Üì\n",
    "Step T-1: Less Noisy Output\n",
    "          + Clean Input Pixels (FIXED)\n",
    "        ‚Üì\n",
    "        ...\n",
    "        ‚Üì\n",
    "Step 0: Clean Output Predictions!\n",
    "```\n",
    "\n",
    "## Theory: Direct Denoising with Fixed Conditioning\n",
    "\n",
    "### Forward Process (Add Noise to Output Only)\n",
    "$$x_t^{out} = \\sqrt{\\bar{\\alpha}_t} \\cdot x_0^{out} + \\sqrt{1 - \\bar{\\alpha}_t} \\cdot \\epsilon$$\n",
    "\n",
    "where $x_0^{out}$ are clean output pixels, $\\epsilon \\sim \\mathcal{N}(0, I)$\n",
    "\n",
    "**Key**: Input pixels $x^{in}$ remain **clean throughout**!\n",
    "\n",
    "### Reverse Process (Denoise Output)\n",
    "$$x_0^{pred} = f_\\theta(x_t^{out}, x^{in}, \\text{coords}, t)$$\n",
    "\n",
    "Neural field predicts clean output values from:\n",
    "- Noisy output pixels (changing each step)\n",
    "- Clean input pixels (fixed conditioning)\n",
    "- Coordinates (spatial information)\n",
    "- Timestep t (noise level)\n",
    "\n",
    "### Training Objective\n",
    "$$\\mathcal{L} = \\mathbb{E}_{t, x_0, \\epsilon} \\left[\\| f_\\theta(x_t^{out}, x^{in}, \\text{coords}, t) - x_0^{out} \\|^2 \\right]$$\n",
    "\n",
    "**Direct prediction loss** - predict clean from noisy!\n",
    "\n",
    "### Why This Approach?\n",
    "- ‚úÖ **Most intuitive**: Gaussian random field ‚Üí clean field\n",
    "- ‚úÖ **Single objective**: Predict clean values directly\n",
    "- ‚úÖ **Clear separation**: Input clean, output noisy\n",
    "- ‚úÖ **Fast sampling**: DDIM possible (10-50 steps)\n",
    "- ‚úÖ **Flexible**: Can use DDPM or DDIM sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "# Import shared components\n",
    "from core.neural_fields.perceiver import PerceiverIO, FourierFeatures\n",
    "from core.sparse.cifar10_sparse import SparseCIFAR10Dataset\n",
    "from core.sparse.metrics import MetricsTracker, print_metrics, visualize_predictions\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Noise Schedule (Same as DDPM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def cosine_beta_schedule(timesteps, s=0.008):\n    \"\"\"Cosine schedule from Improved DDPM\"\"\"\n    steps = timesteps + 1\n    x = torch.linspace(0, timesteps, steps)\n    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n    return torch.clip(betas, 0.0001, 0.9999)\n\n\nclass DiffusionSchedule:\n    \"\"\"DDPM diffusion schedule\"\"\"\n    def __init__(self, timesteps=1000, beta_schedule='cosine'):\n        self.timesteps = timesteps\n        \n        if beta_schedule == 'cosine':\n            betas = cosine_beta_schedule(timesteps)\n        else:\n            betas = torch.linspace(0.0001, 0.02, timesteps)\n        \n        alphas = 1.0 - betas\n        alphas_cumprod = torch.cumprod(alphas, dim=0)\n        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n        \n        self.betas = betas\n        self.alphas = alphas\n        self.alphas_cumprod = alphas_cumprod\n        self.alphas_cumprod_prev = alphas_cumprod_prev\n        self.sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n        self.sqrt_recip_alphas_cumprod = torch.sqrt(1.0 / alphas_cumprod)\n        self.sqrt_recipm1_alphas_cumprod = torch.sqrt(1.0 / alphas_cumprod - 1)\n        \n        # Posterior variance for DDPM\n        self.posterior_variance = betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n        self.posterior_log_variance = torch.log(torch.clamp(self.posterior_variance, min=1e-20))\n    \n    def to(self, device):\n        \"\"\"Move all tensors to device\"\"\"\n        self.betas = self.betas.to(device)\n        self.alphas = self.alphas.to(device)\n        self.alphas_cumprod = self.alphas_cumprod.to(device)\n        self.alphas_cumprod_prev = self.alphas_cumprod_prev.to(device)\n        self.sqrt_alphas_cumprod = self.sqrt_alphas_cumprod.to(device)\n        self.sqrt_one_minus_alphas_cumprod = self.sqrt_one_minus_alphas_cumprod.to(device)\n        self.sqrt_recip_alphas_cumprod = self.sqrt_recip_alphas_cumprod.to(device)\n        self.sqrt_recipm1_alphas_cumprod = self.sqrt_recipm1_alphas_cumprod.to(device)\n        self.posterior_variance = self.posterior_variance.to(device)\n        self.posterior_log_variance = self.posterior_log_variance.to(device)\n        return self\n\n\n# Visualize\nschedule = DiffusionSchedule(timesteps=1000)\n\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 3, 1)\nplt.plot(schedule.betas.numpy())\nplt.title('Œ≤_t')\nplt.xlabel('Timestep')\nplt.grid(alpha=0.3)\n\nplt.subplot(1, 3, 2)\nplt.plot(schedule.alphas_cumprod.numpy())\nplt.title('·æ±_t (Signal Strength)')\nplt.xlabel('Timestep')\nplt.grid(alpha=0.3)\n\nplt.subplot(1, 3, 3)\nplt.plot(schedule.sqrt_alphas_cumprod.numpy(), label='‚àö·æ±_t')\nplt.plot(schedule.sqrt_one_minus_alphas_cumprod.numpy(), label='‚àö(1-·æ±_t)')\nplt.title('Noise Mixing')\nplt.xlabel('Timestep')\nplt.legend()\nplt.grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural Field Denoiser Architecture\n",
    "\n",
    "Key idea: Input pixels stay clean, only output pixels are noisy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalTimeEmbedding(nn.Module):\n",
    "    \"\"\"Sinusoidal time embedding for noise level\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    \n",
    "    def forward(self, t):\n",
    "        device = t.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat([emb.sin(), emb.cos()], dim=-1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "class NFDenoiser(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural Field Denoiser: Gaussian Random Field ‚Üí Clean Field\n",
    "    \n",
    "    f_Œ∏(noisy_output, clean_input, coords, t) ‚Üí clean_output\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_latents=512,\n",
    "        latent_dim=512,\n",
    "        num_fourier_feats=256,\n",
    "        num_blocks=6,\n",
    "        num_heads=8,\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Perceiver IO backbone\n",
    "        self.perceiver = PerceiverIO(\n",
    "            input_channels=3,\n",
    "            output_channels=3,\n",
    "            num_latents=num_latents,\n",
    "            latent_dim=latent_dim,\n",
    "            num_fourier_feats=num_fourier_feats,\n",
    "            num_blocks=num_blocks,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Time embedding\n",
    "        self.time_embed = SinusoidalTimeEmbedding(latent_dim)\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(latent_dim, latent_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(latent_dim, latent_dim)\n",
    "        )\n",
    "        \n",
    "        # Marker for input vs output pixels\n",
    "        self.marker_embed = nn.Embedding(2, 3)  # 0=input, 1=output\n",
    "    \n",
    "    def forward(self, noisy_output_values, output_coords, t, input_coords, input_values):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            noisy_output_values: (B, N_out, 3) NOISY output pixel values\n",
    "            output_coords: (B, N_out, 2) output pixel coordinates\n",
    "            t: (B,) continuous timesteps\n",
    "            input_coords: (B, N_in, 2) input pixel coordinates\n",
    "            input_values: (B, N_in, 3) CLEAN input pixel values\n",
    "        \n",
    "        Returns:\n",
    "            clean_pred: (B, N_out, 3) predicted CLEAN output values\n",
    "        \"\"\"\n",
    "        B, N_in = input_coords.shape[0], input_coords.shape[1]\n",
    "        N_out = output_coords.shape[1]\n",
    "        \n",
    "        # Time embedding\n",
    "        t_emb = self.time_mlp(self.time_embed(t))  # (B, latent_dim)\n",
    "        \n",
    "        # Add time information to pixel values (broadcast to RGB channels)\n",
    "        time_signal = t_emb[:, :3].unsqueeze(1)  # (B, 1, 3)\n",
    "        \n",
    "        # IMPORTANT: Input pixels are CLEAN (no noise added)\n",
    "        #            Output pixels are NOISY (progressively denoised)\n",
    "        input_values_t = input_values + time_signal  # Add time to clean input\n",
    "        noisy_output_values_t = noisy_output_values + time_signal  # Add time to noisy output\n",
    "        \n",
    "        # Concatenate all pixels (input=clean, output=noisy)\n",
    "        all_coords = torch.cat([input_coords, output_coords], dim=1)  # (B, N_in+N_out, 2)\n",
    "        all_values = torch.cat([input_values_t, noisy_output_values_t], dim=1)  # (B, N_in+N_out, 3)\n",
    "        \n",
    "        # Predict CLEAN values at output coordinates\n",
    "        clean_pred = self.perceiver(all_coords, all_values, output_coords)\n",
    "        \n",
    "        return clean_pred\n",
    "\n",
    "\n",
    "# Test denoiser\n",
    "model = NFDenoiser().to(device)\n",
    "test_noisy = torch.randn(4, 204, 3).to(device)\n",
    "test_output_coords = torch.rand(4, 204, 2).to(device)\n",
    "test_t = torch.rand(4).to(device) * 1000\n",
    "test_input_coords = torch.rand(4, 204, 2).to(device)\n",
    "test_input_values = torch.rand(4, 204, 3).to(device)\n",
    "\n",
    "test_pred = model(test_noisy, test_output_coords, test_t, test_input_coords, test_input_values)\n",
    "print(f\"Denoiser test: {test_pred.shape}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Sampling: DDIM\n\nDefine sampling function before training"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@torch.no_grad()\ndef ddim_sample(\n    model,\n    schedule,\n    output_coords,\n    input_coords,\n    input_values,\n    num_steps=50,\n    eta=0.0,\n    device='cuda'\n):\n    \"\"\"DDIM sampling (faster than DDPM)\"\"\"\n    B = output_coords.shape[0]\n    N_out = output_coords.shape[1]\n    \n    # Start from Gaussian random field\n    x_t = torch.randn(B, N_out, 3, device=device)\n    \n    # Uniform timestep schedule\n    timesteps = torch.linspace(schedule.timesteps - 1, 0, num_steps).long()\n    \n    for i, t_idx in enumerate(tqdm(timesteps, desc=\"Sampling\", leave=False)):\n        t = torch.full((B,), t_idx.item(), device=device, dtype=torch.float)\n        \n        # Predict x_0 from x_t\n        x_0_pred = model(x_t, output_coords, t, input_coords, input_values)\n        \n        if i < len(timesteps) - 1:\n            t_next = timesteps[i + 1]\n            \n            alpha_t = schedule.alphas_cumprod[t_idx]\n            alpha_t_next = schedule.alphas_cumprod[t_next]\n            \n            # Predicted noise\n            eps_pred = (x_t - torch.sqrt(alpha_t) * x_0_pred) / torch.sqrt(1 - alpha_t)\n            \n            # DDIM step\n            x_t = (\n                torch.sqrt(alpha_t_next) * x_0_pred +\n                torch.sqrt(1 - alpha_t_next) * eps_pred\n            )\n            \n            if eta > 0:\n                sigma_t = eta * torch.sqrt(\n                    (1 - alpha_t_next) / (1 - alpha_t) * (1 - alpha_t / alpha_t_next)\n                )\n                noise = torch.randn_like(x_t)\n                x_t = x_t + sigma_t * noise\n        else:\n            x_t = x_0_pred\n    \n    return torch.clamp(x_t, 0, 1)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Training: Direct Clean Prediction with Evaluation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_denoiser(\n    model,\n    train_loader,\n    test_loader,\n    schedule,\n    epochs=100,\n    lr=1e-4,\n    device='cuda',\n    visualize_every=5,\n    eval_every=2\n):\n    \"\"\"Train neural field denoiser\"\"\"\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n    \n    losses = []\n    \n    # Get a fixed batch for visualization\n    viz_batch = next(iter(train_loader))\n    viz_input_coords = viz_batch['input_coords'][:4].to(device)\n    viz_input_values = viz_batch['input_values'][:4].to(device)\n    viz_output_coords = viz_batch['output_coords'][:4].to(device)\n    viz_output_values = viz_batch['output_values'][:4].to(device)\n    viz_full_images = viz_batch['full_image'][:4].to(device)\n    \n    model.train()\n    for epoch in range(epochs):\n        epoch_loss = 0\n        \n        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n            input_coords = batch['input_coords'].to(device)\n            input_values = batch['input_values'].to(device)\n            output_coords = batch['output_coords'].to(device)\n            output_values = batch['output_values'].to(device)\n            \n            B = input_coords.shape[0]\n            \n            # Sample random timestep\n            t = torch.randint(0, schedule.timesteps, (B,), device=device).float()\n            \n            # Add noise to OUTPUT ONLY\n            noise = torch.randn_like(output_values)\n            sqrt_alpha_t = schedule.sqrt_alphas_cumprod[t.long()].view(B, 1, 1)\n            sqrt_one_minus_alpha_t = schedule.sqrt_one_minus_alphas_cumprod[t.long()].view(B, 1, 1)\n            \n            noisy_output = sqrt_alpha_t * output_values + sqrt_one_minus_alpha_t * noise\n            \n            # Predict CLEAN output\n            pred_clean = model(noisy_output, output_coords, t, input_coords, input_values)\n            \n            # Loss\n            loss = F.mse_loss(pred_clean, output_values)\n            \n            # Backward\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            \n            epoch_loss += loss.item()\n        \n        avg_loss = epoch_loss / len(train_loader)\n        losses.append(avg_loss)\n        scheduler.step()\n        \n        print(f\"Epoch {epoch+1}: Loss = {avg_loss:.6f}, LR = {scheduler.get_last_lr()[0]:.6f}\")\n        \n        # Evaluate every N epochs\n        if (epoch + 1) % eval_every == 0 or epoch == 0:\n            model.eval()\n            with torch.no_grad():\n                tracker = MetricsTracker()\n                \n                for i, batch in enumerate(test_loader):\n                    if i >= 10:\n                        break\n                    \n                    input_coords = batch['input_coords'].to(device)\n                    input_values = batch['input_values'].to(device)\n                    output_coords = batch['output_coords'].to(device)\n                    output_values = batch['output_values'].to(device)\n                    \n                    pred_values = ddim_sample(\n                        model, schedule, output_coords, input_coords, input_values,\n                        num_steps=50, eta=0.0, device=device\n                    )\n                    \n                    tracker.update(pred_values, output_values)\n                \n                results = tracker.compute()\n                print(f\"  Eval - MSE: {results['mse']:.6f}, MAE: {results['mae']:.6f}\")\n            \n            model.train()\n        \n        # Visualize every N epochs\n        if (epoch + 1) % visualize_every == 0 or epoch == 0:\n            model.eval()\n            with torch.no_grad():\n                pred_values = ddim_sample(\n                    model, schedule, viz_output_coords, \n                    viz_input_coords, viz_input_values,\n                    num_steps=50, eta=0.0, device=device\n                )\n                \n                fig = visualize_predictions(\n                    viz_input_coords, viz_input_values,\n                    viz_output_coords, pred_values, viz_output_values,\n                    viz_full_images, n_samples=4\n                )\n                plt.suptitle(f'NF Denoiser - Epoch {epoch+1}/{epochs}', fontsize=14, y=1.02)\n                plt.savefig(f'nf_denoiser_epoch_{epoch+1:03d}.png', dpi=150, bbox_inches='tight')\n                plt.show()\n                plt.close()\n            \n            model.train()\n        \n        # Save checkpoint\n        if (epoch + 1) % 10 == 0:\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'loss': avg_loss,\n            }, f'nf_denoiser_epoch_{epoch+1}.pt')\n    \n    return losses\n\n\n# Create dataset\nprint(\"Loading CIFAR-10...\")\ntrain_dataset = SparseCIFAR10Dataset(\n    root='../data',\n    train=True,\n    input_ratio=0.2,\n    output_ratio=0.2,\n    download=True,\n    seed=42\n)\n\ntest_dataset = SparseCIFAR10Dataset(\n    root='../data',\n    train=False,\n    input_ratio=0.2,\n    output_ratio=0.2,\n    download=True,\n    seed=42\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)\n\nprint(f\"Train dataset size: {len(train_dataset)}\")\nprint(f\"Test dataset size: {len(test_dataset)}\")\n\n# Initialize\nmodel = NFDenoiser(\n    num_latents=512,\n    latent_dim=512,\n    num_fourier_feats=256,\n    num_blocks=6,\n    num_heads=8\n).to(device)\n\nschedule = DiffusionSchedule(timesteps=1000, beta_schedule='cosine').to(device)\n\n# Train\nprint(\"\\nStarting training...\")\nlosses = train_denoiser(model, train_loader, test_loader, schedule, \n                        epochs=100, lr=1e-4, device=device, \n                        visualize_every=5, eval_every=2)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Final Evaluation: Full Image Reconstruction"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot training loss\nplt.figure(figsize=(10, 4))\nplt.plot(losses, linewidth=2)\nplt.xlabel('Epoch')\nplt.ylabel('MSE Loss')\nplt.title('Training Loss: Neural Field Denoiser')\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# Final Evaluation: Reconstruct FULL images (all 1024 pixels)\nprint(\"\\n\" + \"=\"*70)\nprint(\"FINAL EVALUATION: Full Image Reconstruction (1024 pixels)\")\nprint(\"=\"*70)\n\nmodel.eval()\n\n# Create full grid of coordinates for 32x32 image\ndef create_full_grid(image_size=32, device='cuda'):\n    \"\"\"Create coordinate grid for full image\"\"\"\n    y, x = torch.meshgrid(\n        torch.linspace(0, 1, image_size),\n        torch.linspace(0, 1, image_size),\n        indexing='ij'\n    )\n    coords = torch.stack([x.flatten(), y.flatten()], dim=-1)  # (1024, 2)\n    return coords.to(device)\n\nfull_coords = create_full_grid(32, device)  # (1024, 2)\n\n# Evaluate on test set - reconstruct FULL images\ntracker_full = MetricsTracker()\n\nfor i, batch in enumerate(tqdm(test_loader, desc=\"Full Image Reconstruction\")):\n    if i >= 50:  # Evaluate on 50 batches = 800 images\n        break\n    \n    input_coords = batch['input_coords'].to(device)\n    input_values = batch['input_values'].to(device)\n    full_images = batch['full_image'].to(device)\n    \n    B = input_coords.shape[0]\n    \n    # Predict ALL pixels (1024) conditioned on sparse input (204)\n    full_coords_batch = full_coords.unsqueeze(0).expand(B, -1, -1)  # (B, 1024, 2)\n    \n    pred_values = ddim_sample(\n        model, schedule, full_coords_batch, input_coords, input_values,\n        num_steps=100, eta=0.0, device=device\n    )\n    \n    # Reshape predictions to image format\n    pred_images = pred_values.view(B, 32, 32, 3).permute(0, 3, 1, 2)  # (B, 3, 32, 32)\n    \n    # Compute metrics on full images\n    tracker_full.update(None, None, pred_images, full_images)\n\n# Print results\nresults_full = tracker_full.compute()\nprint(\"\\nFull Image Reconstruction Results:\")\nprint(f\"  PSNR: {results_full['psnr']:.2f} dB\")\nprint(f\"  SSIM: {results_full['ssim']:.4f}\")\nprint(\"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Visualize Full Image Reconstructions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize full image reconstructions\nsample_batch = next(iter(test_loader))\ninput_coords = sample_batch['input_coords'][:4].to(device)\ninput_values = sample_batch['input_values'][:4].to(device)\nfull_images = sample_batch['full_image'][:4].to(device)\n\nB = input_coords.shape[0]\nfull_coords_batch = full_coords.unsqueeze(0).expand(B, -1, -1)\n\n# Generate FULL image predictions\npred_values = ddim_sample(\n    model, schedule, full_coords_batch, input_coords, input_values,\n    num_steps=100, eta=0.0, device=device\n)\n\npred_images = pred_values.view(B, 32, 32, 3).permute(0, 3, 1, 2)\n\n# Visualize\nfig, axes = plt.subplots(4, 3, figsize=(12, 16))\n\nfor i in range(4):\n    # Ground truth\n    gt_img = full_images[i].permute(1, 2, 0).cpu().numpy()\n    axes[i, 0].imshow(gt_img)\n    axes[i, 0].set_title('Ground Truth')\n    axes[i, 0].axis('off')\n    \n    # Sparse input (visualize the 20% input pixels)\n    input_img = torch.zeros(3, 32, 32, device=device)\n    input_idx = sample_batch['input_indices'][i].to(device)\n    input_img.view(3, -1)[:, input_idx] = input_values[i].T\n    axes[i, 1].imshow(input_img.permute(1, 2, 0).cpu().numpy())\n    axes[i, 1].set_title(f'Input (20% = {len(input_idx)} pixels)')\n    axes[i, 1].axis('off')\n    \n    # Full reconstruction\n    pred_img = pred_images[i].permute(1, 2, 0).cpu().numpy()\n    axes[i, 2].imshow(np.clip(pred_img, 0, 1))\n    axes[i, 2].set_title('Reconstructed (100%)')\n    axes[i, 2].axis('off')\n\nplt.suptitle('NF Denoiser: Full Image Reconstruction from 20% Sparse Input', fontsize=14, y=0.995)\nplt.tight_layout()\nplt.savefig('nf_denoiser_full_reconstruction.png', dpi=150, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### ‚úÖ Implemented\n",
    "- Gaussian Random Field denoising (your idea!)\n",
    "- Direct clean prediction objective\n",
    "- DDIM sampling (50 steps)\n",
    "- Fixed clean input conditioning\n",
    "\n",
    "### üìä Results\n",
    "See metrics above for:\n",
    "- MSE/MAE on output pixels\n",
    "- PSNR/SSIM on full images\n",
    "\n",
    "### ‚öñÔ∏è Strengths & Weaknesses\n",
    "\n",
    "**Strengths**:\n",
    "- ‚úÖ **Most intuitive approach**: Gaussian noise ‚Üí clean field\n",
    "- ‚úÖ **Simple training**: Single MSE loss on clean prediction\n",
    "- ‚úÖ **Fast sampling**: DDIM with 50 steps (vs 1000 for score-based)\n",
    "- ‚úÖ **Clear separation**: Input always clean, output denoised\n",
    "- ‚úÖ **Flexible**: Works with DDPM or DDIM\n",
    "\n",
    "**Potential Weaknesses**:\n",
    "- ‚ö†Ô∏è May need more steps for very high quality\n",
    "- ‚ö†Ô∏è Less theoretically principled than score-based\n",
    "\n",
    "### üîÑ Next\n",
    "Compare with:\n",
    "- Notebook 3: Score-Based (already done)\n",
    "- Notebook 5: Flow Matching (next)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}