{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Directional MAMBA Diffusion (v2)\n",
    "\n",
    "## Key Innovation: Multi-Directional Spatial Scanning\n",
    "\n",
    "**Problem Solved**: Original MAMBA processes coordinates as 1D sequence, losing 2D spatial structure\n",
    "\n",
    "**Solution**: Process in 4 directions (horizontal, vertical, diagonal, anti-diagonal), then fuse\n",
    "\n",
    "**Expected**: Smoother reconstructions with 2-4 dB PSNR improvement\n",
    "\n",
    "```\n",
    "Single Direction (v1):        Multi-Direction (v2):\n",
    "A→B→C→D→E→F→...              Horizontal: A→B→C→D\n",
    "                              Vertical:   A→E→I→M\n",
    "Missing spatial neighbors     Diagonal:   A→B→E→C→F\n",
    "→ Noise and wiggliness        Anti-diag:  D→C→H→G\n",
    "                              → Full 2D awareness!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add parent directory to path\n",
    "notebook_dir = os.path.abspath('')\n",
    "parent_dir = os.path.dirname(notebook_dir)\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "from core.neural_fields.perceiver import FourierFeatures\n",
    "from core.sparse.cifar10_sparse import SparseCIFAR10Dataset\n",
    "from core.sparse.metrics import MetricsTracker, print_metrics, visualize_predictions\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"Parent directory added to path: {parent_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Core SSM Components (Fast Implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSMBlockFast(nn.Module):\n",
    "    \"\"\"\n",
    "    Ultra-fast SSM using cumulative scan\n",
    "    Optimized version from MAMBA v1\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_state=16, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        \n",
    "        # State space parameters\n",
    "        self.A_log = nn.Parameter(torch.randn(d_state) * 0.1 - 1.0)\n",
    "        self.B = nn.Linear(d_model, d_state, bias=False)\n",
    "        self.C = nn.Linear(d_state, d_model, bias=False)\n",
    "        self.D = nn.Parameter(torch.randn(d_model) * 0.01)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.B.weight, gain=0.5)\n",
    "        nn.init.xavier_uniform_(self.C.weight, gain=0.5)\n",
    "        \n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.eps = 1e-8\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, N, D = x.shape\n",
    "        \n",
    "        # Discretization\n",
    "        A = -torch.exp(self.A_log).clamp(min=self.eps, max=10.0)\n",
    "        dt = 1.0 / N\n",
    "        A_bar = torch.exp(dt * A)\n",
    "        B_bar = torch.where(\n",
    "            torch.abs(A) > self.eps,\n",
    "            (A_bar - 1.0) / (A + self.eps),\n",
    "            torch.ones_like(A) * dt\n",
    "        )\n",
    "        \n",
    "        Bu = self.B(x) * B_bar\n",
    "        \n",
    "        # Efficient state computation\n",
    "        indices = torch.arange(N, device=x.device)\n",
    "        decay = A_bar.unsqueeze(0).pow(\n",
    "            (indices.unsqueeze(0) - indices.unsqueeze(1)).clamp(min=0).unsqueeze(-1)\n",
    "        )\n",
    "        mask = indices.unsqueeze(0) >= indices.unsqueeze(1)\n",
    "        decay = decay * mask.unsqueeze(-1).float()\n",
    "        \n",
    "        h = torch.einsum('nmd,bnd->bmd', decay, Bu)\n",
    "        h = torch.clamp(h, min=-10.0, max=10.0)\n",
    "        \n",
    "        # Output\n",
    "        y = self.C(h) + self.D * x\n",
    "        \n",
    "        # Gating\n",
    "        gate = self.gate(x)\n",
    "        y = gate * y + (1 - gate) * x\n",
    "        \n",
    "        return self.dropout(self.norm(y))\n",
    "\n",
    "\n",
    "class MambaBlock(nn.Module):\n",
    "    \"\"\"Standard Mamba block (for comparison)\"\"\"\n",
    "    def __init__(self, d_model, d_state=16, expand_factor=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.proj_in = nn.Linear(d_model, d_model * expand_factor)\n",
    "        self.ssm = SSMBlockFast(d_model * expand_factor, d_state, dropout)\n",
    "        self.proj_out = nn.Linear(d_model * expand_factor, d_model)\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.proj_in(x)\n",
    "        x = self.ssm(x)\n",
    "        x = self.proj_out(x)\n",
    "        x = x + residual\n",
    "        x = x + self.mlp(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "print(\"✓ Core SSM components loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi-Directional MAMBA Components\n",
    "\n",
    "**Innovation**: Process coordinates in 4 directions to capture full 2D spatial structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Coordinate Ordering Functions\n",
    "# ============================================\n",
    "\n",
    "def order_by_row(coords):\n",
    "    \"\"\"Row-major ordering (horizontal scan)\"\"\"\n",
    "    B, N, _ = coords.shape\n",
    "    indices_list = []\n",
    "    for b in range(B):\n",
    "        y_vals = coords[b, :, 1]\n",
    "        x_vals = coords[b, :, 0]\n",
    "        sort_keys = y_vals * 1000 + x_vals\n",
    "        indices = torch.argsort(sort_keys)\n",
    "        indices_list.append(indices)\n",
    "    return torch.stack(indices_list, dim=0)\n",
    "\n",
    "def order_by_column(coords):\n",
    "    \"\"\"Column-major ordering (vertical scan)\"\"\"\n",
    "    B, N, _ = coords.shape\n",
    "    indices_list = []\n",
    "    for b in range(B):\n",
    "        y_vals = coords[b, :, 1]\n",
    "        x_vals = coords[b, :, 0]\n",
    "        sort_keys = x_vals * 1000 + y_vals\n",
    "        indices = torch.argsort(sort_keys)\n",
    "        indices_list.append(indices)\n",
    "    return torch.stack(indices_list, dim=0)\n",
    "\n",
    "def order_by_diagonal(coords):\n",
    "    \"\"\"Diagonal ordering (top-left to bottom-right)\"\"\"\n",
    "    B, N, _ = coords.shape\n",
    "    indices_list = []\n",
    "    for b in range(B):\n",
    "        y_vals = coords[b, :, 1]\n",
    "        x_vals = coords[b, :, 0]\n",
    "        diag_vals = x_vals + y_vals\n",
    "        sort_keys = diag_vals * 1000 + x_vals\n",
    "        indices = torch.argsort(sort_keys)\n",
    "        indices_list.append(indices)\n",
    "    return torch.stack(indices_list, dim=0)\n",
    "\n",
    "def order_by_antidiagonal(coords):\n",
    "    \"\"\"Anti-diagonal ordering (top-right to bottom-left)\"\"\"\n",
    "    B, N, _ = coords.shape\n",
    "    indices_list = []\n",
    "    for b in range(B):\n",
    "        y_vals = coords[b, :, 1]\n",
    "        x_vals = coords[b, :, 0]\n",
    "        antidiag_vals = x_vals - y_vals\n",
    "        sort_keys = antidiag_vals * 1000 + x_vals\n",
    "        indices = torch.argsort(sort_keys)\n",
    "        indices_list.append(indices)\n",
    "    return torch.stack(indices_list, dim=0)\n",
    "\n",
    "def reorder_sequence(x, indices):\n",
    "    \"\"\"Apply ordering to sequence\"\"\"\n",
    "    B, N, D = x.shape\n",
    "    indices_expanded = indices.unsqueeze(-1).expand(B, N, D)\n",
    "    return torch.gather(x, dim=1, index=indices_expanded)\n",
    "\n",
    "def inverse_reorder(x, indices):\n",
    "    \"\"\"Reverse ordering back to original positions\"\"\"\n",
    "    B, N, D = x.shape\n",
    "    inverse_indices = torch.zeros_like(indices)\n",
    "    for b in range(B):\n",
    "        inverse_indices[b, indices[b]] = torch.arange(N, device=indices.device)\n",
    "    indices_expanded = inverse_indices.unsqueeze(-1).expand(B, N, D)\n",
    "    return torch.gather(x, dim=1, index=indices_expanded)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Multi-Directional SSM\n",
    "# ============================================\n",
    "\n",
    "class MultiDirectionalSSM(nn.Module):\n",
    "    \"\"\"Process sequence in 4 directions, fuse results\"\"\"\n",
    "    def __init__(self, d_model, d_state=16, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # 4 separate SSM blocks for each direction\n",
    "        self.ssm_horizontal = SSMBlockFast(d_model, d_state, dropout)\n",
    "        self.ssm_vertical = SSMBlockFast(d_model, d_state, dropout)\n",
    "        self.ssm_diagonal = SSMBlockFast(d_model, d_state, dropout)\n",
    "        self.ssm_antidiagonal = SSMBlockFast(d_model, d_state, dropout)\n",
    "        \n",
    "        # Fusion layer\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(4 * d_model, d_model * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 2, d_model)\n",
    "        )\n",
    "        \n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x, coords):\n",
    "        # Get orderings\n",
    "        indices_h = order_by_row(coords)\n",
    "        indices_v = order_by_column(coords)\n",
    "        indices_d = order_by_diagonal(coords)\n",
    "        indices_a = order_by_antidiagonal(coords)\n",
    "        \n",
    "        # Horizontal\n",
    "        x_h = reorder_sequence(x, indices_h)\n",
    "        y_h = self.ssm_horizontal(x_h)\n",
    "        y_h = inverse_reorder(y_h, indices_h)\n",
    "        \n",
    "        # Vertical\n",
    "        x_v = reorder_sequence(x, indices_v)\n",
    "        y_v = self.ssm_vertical(x_v)\n",
    "        y_v = inverse_reorder(y_v, indices_v)\n",
    "        \n",
    "        # Diagonal\n",
    "        x_d = reorder_sequence(x, indices_d)\n",
    "        y_d = self.ssm_diagonal(x_d)\n",
    "        y_d = inverse_reorder(y_d, indices_d)\n",
    "        \n",
    "        # Anti-diagonal\n",
    "        x_a = reorder_sequence(x, indices_a)\n",
    "        y_a = self.ssm_antidiagonal(x_a)\n",
    "        y_a = inverse_reorder(y_a, indices_a)\n",
    "        \n",
    "        # Fuse all 4 directions\n",
    "        y_concat = torch.cat([y_h, y_v, y_d, y_a], dim=-1)\n",
    "        y_fused = self.fusion(y_concat)\n",
    "        \n",
    "        # Residual\n",
    "        y = x + y_fused\n",
    "        y = self.norm(y)\n",
    "        \n",
    "        return y\n",
    "\n",
    "\n",
    "class MultiDirectionalMambaBlock(nn.Module):\n",
    "    \"\"\"Complete multi-directional Mamba block\"\"\"\n",
    "    def __init__(self, d_model, d_state=16, expand_factor=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.proj_in = nn.Linear(d_model, d_model * expand_factor)\n",
    "        \n",
    "        self.multi_ssm = MultiDirectionalSSM(\n",
    "            d_model * expand_factor,\n",
    "            d_state,\n",
    "            dropout\n",
    "        )\n",
    "        \n",
    "        self.proj_out = nn.Linear(d_model * expand_factor, d_model)\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, coords):\n",
    "        # SSM branch with multi-directional processing\n",
    "        residual = x\n",
    "        x = self.proj_in(x)\n",
    "        x = self.multi_ssm(x, coords)\n",
    "        x = self.proj_out(x)\n",
    "        x = x + residual\n",
    "        \n",
    "        # MLP branch\n",
    "        x = x + self.mlp(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "print(\"✓ Multi-directional MAMBA components loaded\")\n",
    "print(\"  - 4 scanning directions: horizontal, vertical, diagonal, anti-diagonal\")\n",
    "print(\"  - Fusion mechanism to combine all directions\")\n",
    "print(\"  - Full 2D spatial awareness!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Time Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalTimeEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    \n",
    "    def forward(self, t):\n",
    "        device = t.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat([emb.sin(), emb.cos()], dim=-1)\n",
    "        return emb\n",
    "\n",
    "print(\"✓ Time embedding loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Directional MAMBA Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAMBADiffusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Directional State Space Model for Sparse Field Diffusion\n",
    "    \n",
    "    Key Innovation:\n",
    "    - Processes coordinates in 4 directions (horizontal, vertical, diagonal, anti-diagonal)\n",
    "    - Fuses directional outputs to capture full 2D spatial structure\n",
    "    - Addresses MAMBA's limitation of 1D sequential processing\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_fourier_feats=256,\n",
    "        d_model=512,\n",
    "        num_layers=6,\n",
    "        d_state=16,\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Fourier features\n",
    "        self.fourier = FourierFeatures(coord_dim=2, num_freqs=num_fourier_feats, scale=10.0)\n",
    "        feat_dim = num_fourier_feats * 2\n",
    "        \n",
    "        # Project inputs and queries\n",
    "        self.input_proj = nn.Linear(feat_dim + 3, d_model)\n",
    "        self.query_proj = nn.Linear(feat_dim + 3, d_model)\n",
    "        \n",
    "        # Time embedding\n",
    "        self.time_embed = SinusoidalTimeEmbedding(d_model)\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(d_model, d_model)\n",
    "        )\n",
    "        \n",
    "        # Multi-directional MAMBA blocks\n",
    "        self.mamba_blocks = nn.ModuleList([\n",
    "            MultiDirectionalMambaBlock(\n",
    "                d_model,\n",
    "                d_state=d_state,\n",
    "                expand_factor=2,\n",
    "                dropout=dropout\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Cross-attention\n",
    "        self.query_cross_attn = nn.MultiheadAttention(\n",
    "            d_model, num_heads=8, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Output decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_model * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 2, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, 3)\n",
    "        )\n",
    "    \n",
    "    def forward(self, noisy_values, query_coords, t, input_coords, input_values):\n",
    "        B = query_coords.shape[0]\n",
    "        N_in = input_coords.shape[1]\n",
    "        N_out = query_coords.shape[1]\n",
    "        \n",
    "        # Time embedding\n",
    "        t_emb = self.time_mlp(self.time_embed(t))\n",
    "        \n",
    "        # Fourier features\n",
    "        input_feats = self.fourier(input_coords)\n",
    "        query_feats = self.fourier(query_coords)\n",
    "        \n",
    "        # Encode\n",
    "        input_tokens = self.input_proj(\n",
    "            torch.cat([input_feats, input_values], dim=-1)\n",
    "        )\n",
    "        query_tokens = self.query_proj(\n",
    "            torch.cat([query_feats, noisy_values], dim=-1)\n",
    "        )\n",
    "        \n",
    "        # Add time\n",
    "        input_tokens = input_tokens + t_emb.unsqueeze(1)\n",
    "        query_tokens = query_tokens + t_emb.unsqueeze(1)\n",
    "        \n",
    "        # Concatenate coordinates and sequences\n",
    "        all_coords = torch.cat([input_coords, query_coords], dim=1)\n",
    "        seq = torch.cat([input_tokens, query_tokens], dim=1)\n",
    "        \n",
    "        # Process through multi-directional MAMBA blocks\n",
    "        for mamba_block in self.mamba_blocks:\n",
    "            seq = mamba_block(seq, all_coords)  # Pass coords for directional scanning\n",
    "        \n",
    "        # Split back\n",
    "        input_seq = seq[:, :N_in, :]\n",
    "        query_seq = seq[:, N_in:, :]\n",
    "        \n",
    "        # Cross-attention\n",
    "        output, _ = self.query_cross_attn(query_seq, input_seq, input_seq)\n",
    "        \n",
    "        # Decode\n",
    "        return self.decoder(output)\n",
    "\n",
    "\n",
    "# Test model\n",
    "model = MAMBADiffusion(\n",
    "    num_fourier_feats=256,\n",
    "    d_model=512,\n",
    "    num_layers=6,\n",
    "    d_state=16\n",
    ").to(device)\n",
    "\n",
    "test_noisy = torch.rand(4, 204, 3).to(device)\n",
    "test_query_coords = torch.rand(4, 204, 2).to(device)\n",
    "test_t = torch.rand(4).to(device)\n",
    "test_input_coords = torch.rand(4, 204, 2).to(device)\n",
    "test_input_values = torch.rand(4, 204, 3).to(device)\n",
    "\n",
    "test_out = model(test_noisy, test_query_coords, test_t, test_input_coords, test_input_values)\n",
    "print(f\"✓ Model test passed: {test_out.shape}\")\n",
    "print(f\"✓ Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  (Expected: ~4× more than v1 due to 4 directional SSMs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Flow Matching Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_flow(x_0, x_1, t):\n",
    "    \"\"\"Linear interpolation\"\"\"\n",
    "    return (1 - t) * x_0 + t * x_1\n",
    "\n",
    "def target_velocity(x_0, x_1):\n",
    "    \"\"\"Target velocity\"\"\"\n",
    "    return x_1 - x_0\n",
    "\n",
    "@torch.no_grad()\n",
    "def heun_sample(model, output_coords, input_coords, input_values, num_steps=50, device='cuda'):\n",
    "    \"\"\"Heun ODE solver\"\"\"\n",
    "    B, N_out = output_coords.shape[0], output_coords.shape[1]\n",
    "    x_t = torch.randn(B, N_out, 3, device=device)\n",
    "    \n",
    "    dt = 1.0 / num_steps\n",
    "    ts = torch.linspace(0, 1 - dt, num_steps)\n",
    "    \n",
    "    for t_val in tqdm(ts, desc=\"Sampling\", leave=False):\n",
    "        t = torch.full((B,), t_val.item(), device=device)\n",
    "        t_next = torch.full((B,), t_val.item() + dt, device=device)\n",
    "        \n",
    "        v1 = model(x_t, output_coords, t, input_coords, input_values)\n",
    "        x_next_pred = x_t + dt * v1\n",
    "        \n",
    "        v2 = model(x_next_pred, output_coords, t_next, input_coords, input_values)\n",
    "        x_t = x_t + dt * 0.5 * (v1 + v2)\n",
    "    \n",
    "    return torch.clamp(x_t, 0, 1)\n",
    "\n",
    "def train_flow_matching(\n",
    "    model, train_loader, test_loader, epochs=100, lr=1e-4, device='cuda',\n",
    "    visualize_every=5, eval_every=2, save_dir='checkpoints_multidir'\n",
    "):\n",
    "    \"\"\"Train with flow matching\"\"\"\n",
    "    import os\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    # Full grid for visualization\n",
    "    y, x = torch.meshgrid(\n",
    "        torch.linspace(0, 1, 32),\n",
    "        torch.linspace(0, 1, 32),\n",
    "        indexing='ij'\n",
    "    )\n",
    "    full_coords = torch.stack([x.flatten(), y.flatten()], dim=-1).to(device)\n",
    "    \n",
    "    viz_batch = next(iter(train_loader))\n",
    "    viz_input_coords = viz_batch['input_coords'][:4].to(device)\n",
    "    viz_input_values = viz_batch['input_values'][:4].to(device)\n",
    "    viz_output_coords = viz_batch['output_coords'][:4].to(device)\n",
    "    viz_output_values = viz_batch['output_values'][:4].to(device)\n",
    "    viz_full_images = viz_batch['full_image'][:4].to(device)\n",
    "    viz_input_indices = viz_batch['input_indices'][:4]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            input_coords = batch['input_coords'].to(device)\n",
    "            input_values = batch['input_values'].to(device)\n",
    "            output_coords = batch['output_coords'].to(device)\n",
    "            output_values = batch['output_values'].to(device)\n",
    "            \n",
    "            B = input_coords.shape[0]\n",
    "            t = torch.rand(B, device=device)\n",
    "            \n",
    "            x_0 = torch.randn_like(output_values)\n",
    "            x_1 = output_values\n",
    "            \n",
    "            t_broadcast = t.view(B, 1, 1)\n",
    "            x_t = conditional_flow(x_0, x_1, t_broadcast)\n",
    "            u_t = target_velocity(x_0, x_1)\n",
    "            \n",
    "            v_pred = model(x_t, output_coords, t, input_coords, input_values)\n",
    "            loss = F.mse_loss(v_pred, u_t)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        losses.append(avg_loss)\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Loss = {avg_loss:.6f}, LR = {scheduler.get_last_lr()[0]:.6f}\")\n",
    "        \n",
    "        # Evaluation\n",
    "        val_loss = None\n",
    "        if (epoch + 1) % eval_every == 0 or epoch == 0:\n",
    "            model.eval()\n",
    "            tracker = MetricsTracker()\n",
    "            val_loss_accum = 0\n",
    "            val_batches = 0\n",
    "            with torch.no_grad():\n",
    "                for i, batch in enumerate(test_loader):\n",
    "                    if i >= 10:\n",
    "                        break\n",
    "                    pred_values = heun_sample(\n",
    "                        model, batch['output_coords'].to(device),\n",
    "                        batch['input_coords'].to(device), batch['input_values'].to(device),\n",
    "                        num_steps=50, device=device\n",
    "                    )\n",
    "                    tracker.update(pred_values, batch['output_values'].to(device))\n",
    "                    val_loss_accum += F.mse_loss(pred_values, batch['output_values'].to(device)).item()\n",
    "                    val_batches += 1\n",
    "                    \n",
    "                results = tracker.compute()\n",
    "                val_loss = val_loss_accum / val_batches\n",
    "                print(f\"  Eval - MSE: {results['mse']:.6f}, MAE: {results['mae']:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "                \n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    torch.save({\n",
    "                        'epoch': epoch + 1,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'scheduler_state_dict': scheduler.state_dict(),\n",
    "                        'loss': avg_loss,\n",
    "                        'val_loss': val_loss,\n",
    "                        'best_val_loss': best_val_loss\n",
    "                    }, f'{save_dir}/mamba_multidir_best.pth')\n",
    "                    print(f\"  ✓ Saved best model (val_loss: {val_loss:.6f})\")\n",
    "        \n",
    "        # Save latest\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "            'val_loss': val_loss if val_loss is not None else avg_loss,\n",
    "            'best_val_loss': best_val_loss\n",
    "        }, f'{save_dir}/mamba_multidir_latest.pth')\n",
    "        \n",
    "        # Visualization\n",
    "        if (epoch + 1) % visualize_every == 0 or epoch == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                pred_values = heun_sample(\n",
    "                    model, viz_output_coords, viz_input_coords, viz_input_values,\n",
    "                    num_steps=50, device=device\n",
    "                )\n",
    "                \n",
    "                full_coords_batch = full_coords.unsqueeze(0).expand(4, -1, -1)\n",
    "                full_pred_values = heun_sample(\n",
    "                    model, full_coords_batch, viz_input_coords, viz_input_values,\n",
    "                    num_steps=50, device=device\n",
    "                )\n",
    "                full_pred_images = full_pred_values.view(4, 32, 32, 3).permute(0, 3, 1, 2)\n",
    "                \n",
    "                fig, axes = plt.subplots(4, 5, figsize=(20, 16))\n",
    "                \n",
    "                for i in range(4):\n",
    "                    # Ground truth\n",
    "                    axes[i, 0].imshow(viz_full_images[i].permute(1, 2, 0).cpu().numpy())\n",
    "                    axes[i, 0].set_title('Ground Truth' if i == 0 else '', fontsize=10)\n",
    "                    axes[i, 0].axis('off')\n",
    "                    \n",
    "                    # Sparse input\n",
    "                    input_img = torch.zeros(3, 32, 32, device=device)\n",
    "                    input_idx = viz_input_indices[i]\n",
    "                    input_img.view(3, -1)[:, input_idx] = viz_input_values[i].T\n",
    "                    axes[i, 1].imshow(input_img.permute(1, 2, 0).cpu().numpy())\n",
    "                    axes[i, 1].set_title('Sparse Input (20%)' if i == 0 else '', fontsize=10)\n",
    "                    axes[i, 1].axis('off')\n",
    "                    \n",
    "                    # Sparse target\n",
    "                    target_img = torch.zeros(3, 32, 32, device=device)\n",
    "                    output_idx = viz_batch['output_indices'][i]\n",
    "                    target_img.view(3, -1)[:, output_idx] = viz_output_values[i].T\n",
    "                    axes[i, 2].imshow(target_img.permute(1, 2, 0).cpu().numpy())\n",
    "                    axes[i, 2].set_title('Sparse Target (20%)' if i == 0 else '', fontsize=10)\n",
    "                    axes[i, 2].axis('off')\n",
    "                    \n",
    "                    # Sparse prediction\n",
    "                    pred_img = torch.zeros(3, 32, 32, device=device)\n",
    "                    pred_img.view(3, -1)[:, output_idx] = pred_values[i].T\n",
    "                    axes[i, 3].imshow(np.clip(pred_img.permute(1, 2, 0).cpu().numpy(), 0, 1))\n",
    "                    axes[i, 3].set_title('Sparse Prediction' if i == 0 else '', fontsize=10)\n",
    "                    axes[i, 3].axis('off')\n",
    "                    \n",
    "                    # Full field\n",
    "                    axes[i, 4].imshow(np.clip(full_pred_images[i].permute(1, 2, 0).cpu().numpy(), 0, 1))\n",
    "                    axes[i, 4].set_title('Full Field' if i == 0 else '', fontsize=10)\n",
    "                    axes[i, 4].axis('off')\n",
    "                \n",
    "                plt.suptitle(f'Multi-Dir MAMBA - Epoch {epoch+1} (Best Val: {best_val_loss:.6f})', \n",
    "                           fontsize=14, y=0.995)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'{save_dir}/epoch_{epoch+1:03d}.png', dpi=150, bbox_inches='tight')\n",
    "                plt.show()\n",
    "                plt.close()\n",
    "    \n",
    "    print(f\"\\n✓ Training complete! Best validation loss: {best_val_loss:.6f}\")\n",
    "    return losses\n",
    "\n",
    "print(\"✓ Training functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Data and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "train_dataset = SparseCIFAR10Dataset(\n",
    "    root='../data', train=True, input_ratio=0.2, output_ratio=0.2, download=True, seed=42\n",
    ")\n",
    "test_dataset = SparseCIFAR10Dataset(\n",
    "    root='../data', train=False, input_ratio=0.2, output_ratio=0.2, download=True, seed=42\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Test: {len(test_dataset)}\")\n",
    "\n",
    "# Initialize model\n",
    "model = MAMBADiffusion(\n",
    "    num_fourier_feats=256,\n",
    "    d_model=512,\n",
    "    num_layers=6,\n",
    "    d_state=16\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Train\n",
    "losses = train_flow_matching(model, train_loader, test_loader, epochs=100, lr=1e-4, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Evaluation & Loss Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses, linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss: Multi-Directional MAMBA')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.savefig('checkpoints_multidir/training_loss.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Multi-Scale Evaluation\n",
    "\n",
    "Test scale-invariant continuous representations at 32×32, 64×64, 96×96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_scale_grids(device='cuda'):\n",
    "    \"\"\"Create coordinate grids at different resolutions\"\"\"\n",
    "    grids = {}\n",
    "    for size in [32, 64, 96]:\n",
    "        y, x = torch.meshgrid(\n",
    "            torch.linspace(0, 1, size),\n",
    "            torch.linspace(0, 1, size),\n",
    "            indexing='ij'\n",
    "        )\n",
    "        grids[size] = torch.stack([x.flatten(), y.flatten()], dim=-1).to(device)\n",
    "    return grids\n",
    "\n",
    "@torch.no_grad()\n",
    "def multi_scale_reconstruction(model, input_coords, input_values, grids, num_steps=100, device='cuda'):\n",
    "    \"\"\"Reconstruct at multiple scales\"\"\"\n",
    "    model.eval()\n",
    "    B = input_coords.shape[0]\n",
    "    reconstructions = {}\n",
    "    \n",
    "    for size, coords in grids.items():\n",
    "        print(f\"Reconstructing at {size}×{size}...\")\n",
    "        coords_batch = coords.unsqueeze(0).expand(B, -1, -1)\n",
    "        pred_values = heun_sample(\n",
    "            model, coords_batch, input_coords, input_values,\n",
    "            num_steps=num_steps, device=device\n",
    "        )\n",
    "        pred_images = pred_values.view(B, size, size, 3).permute(0, 3, 1, 2)\n",
    "        reconstructions[size] = pred_images\n",
    "    \n",
    "    return reconstructions\n",
    "\n",
    "# Create grids\n",
    "multi_scale_grids = create_multi_scale_grids(device)\n",
    "\n",
    "# Test on batch\n",
    "test_batch = next(iter(test_loader))\n",
    "B_test = 4\n",
    "\n",
    "multi_scale_results = multi_scale_reconstruction(\n",
    "    model,\n",
    "    test_batch['input_coords'][:B_test].to(device),\n",
    "    test_batch['input_values'][:B_test].to(device),\n",
    "    multi_scale_grids,\n",
    "    num_steps=100,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\nMulti-scale reconstruction complete!\")\n",
    "for size, imgs in multi_scale_results.items():\n",
    "    print(f\"  {size}×{size}: {imgs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Multi-Scale Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_multi_scale(ground_truth, sparse_input_img, multi_scale_results, sample_idx=0):\n",
    "    \"\"\"Visualize multi-scale reconstructions\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    # Row 1\n",
    "    axes[0, 0].imshow(ground_truth.permute(1, 2, 0).cpu().numpy())\n",
    "    axes[0, 0].set_title('Ground Truth (32×32)', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    axes[0, 1].imshow(sparse_input_img.permute(1, 2, 0).cpu().numpy())\n",
    "    axes[0, 1].set_title('Sparse Input (20%)', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    img_32 = multi_scale_results[32][sample_idx].permute(1, 2, 0).cpu().numpy()\n",
    "    axes[0, 2].imshow(np.clip(img_32, 0, 1))\n",
    "    axes[0, 2].set_title('Reconstructed 32×32\\n(Native)', fontsize=12, fontweight='bold')\n",
    "    axes[0, 2].axis('off')\n",
    "    \n",
    "    # Row 2\n",
    "    img_64 = multi_scale_results[64][sample_idx].permute(1, 2, 0).cpu().numpy()\n",
    "    axes[1, 0].imshow(np.clip(img_64, 0, 1))\n",
    "    axes[1, 0].set_title('Reconstructed 64×64\\n(2× Upsampling)', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    img_96 = multi_scale_results[96][sample_idx].permute(1, 2, 0).cpu().numpy()\n",
    "    axes[1, 1].imshow(np.clip(img_96, 0, 1))\n",
    "    axes[1, 1].set_title('Reconstructed 96×96\\n(3× Upsampling)', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    # Comparison\n",
    "    img_32_up = torch.nn.functional.interpolate(\n",
    "        multi_scale_results[32][sample_idx:sample_idx+1],\n",
    "        size=64, mode='nearest'\n",
    "    )[0].permute(1, 2, 0).cpu().numpy()\n",
    "    axes[1, 2].imshow(np.clip(img_32_up, 0, 1))\n",
    "    axes[1, 2].set_title('32×32 Upsampled\\n(Nearest)', fontsize=12, fontweight='bold')\n",
    "    axes[1, 2].axis('off')\n",
    "    \n",
    "    plt.suptitle('Multi-Directional MAMBA: Scale-Invariant Reconstruction', fontsize=16, fontweight='bold', y=0.98)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Visualize samples\n",
    "for i in range(min(B_test, 4)):\n",
    "    sparse_img = torch.zeros(3, 32, 32)\n",
    "    input_idx = test_batch['input_indices'][i]\n",
    "    sparse_img.view(3, -1)[:, input_idx] = test_batch['input_values'][i].T\n",
    "    \n",
    "    fig = visualize_multi_scale(\n",
    "        test_batch['full_image'][i],\n",
    "        sparse_img,\n",
    "        multi_scale_results,\n",
    "        sample_idx=i\n",
    "    )\n",
    "    plt.savefig(f'checkpoints_multidir/multiscale_sample_{i}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Quantitative Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"QUANTITATIVE EVALUATION: Full Field Reconstruction at 32×32\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model.eval()\n",
    "tracker_full_field = MetricsTracker()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(tqdm(test_loader, desc=\"Full field evaluation\")):\n",
    "        if i >= 100:\n",
    "            break\n",
    "        \n",
    "        B = batch['input_coords'].shape[0]\n",
    "        full_coords_batch = multi_scale_grids[32].unsqueeze(0).expand(B, -1, -1)\n",
    "        \n",
    "        pred_values = heun_sample(\n",
    "            model, full_coords_batch,\n",
    "            batch['input_coords'].to(device),\n",
    "            batch['input_values'].to(device),\n",
    "            num_steps=100, device=device\n",
    "        )\n",
    "        \n",
    "        pred_images = pred_values.view(B, 32, 32, 3).permute(0, 3, 1, 2)\n",
    "        tracker_full_field.update(None, None, pred_images, batch['full_image'].to(device))\n",
    "\n",
    "results = tracker_full_field.compute()\n",
    "results_std = tracker_full_field.compute_std()\n",
    "\n",
    "print(f\"\\nMulti-Directional MAMBA Results (32×32):\")\n",
    "print(f\"  PSNR: {results['psnr']:.2f} ± {results_std['psnr_std']:.2f} dB\")\n",
    "print(f\"  SSIM: {results['ssim']:.4f} ± {results_std['ssim_std']:.4f}\")\n",
    "print(f\"  MSE:  {results['mse']:.6f} ± {results_std['mse_std']:.6f}\")\n",
    "print(f\"  MAE:  {results['mae']:.6f} ± {results_std['mae_std']:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Expected Improvement over v1 (single-direction):\")\n",
    "print(\"  PSNR: +2-4 dB (if spatial locality was the issue)\")\n",
    "print(\"  SSIM: +0.05-0.07\")\n",
    "print(\"  Visual: Significantly smoother textures\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Multi-Directional MAMBA Advantages\n",
    "\n",
    "**vs v1 (Single-Direction)**:\n",
    "- ✅ **Full 2D spatial awareness**: 4 scanning directions capture all spatial relationships\n",
    "- ✅ **Better texture quality**: Horizontal, vertical, and diagonal context preserved\n",
    "- ✅ **Smoother reconstructions**: Reduced noise from proper spatial structure\n",
    "- ✅ **Improved multi-scale**: Better generalization to 64×64 and 96×96\n",
    "\n",
    "**Trade-offs**:\n",
    "- ⚠️ **4× more parameters**: 4 SSM blocks per direction\n",
    "- ⚠️ **3-4× slower training**: 4 directional passes per layer\n",
    "- ⚠️ **More memory**: ~4× VRAM for SSM layers\n",
    "\n",
    "**Expected Performance**:\n",
    "- **PSNR**: 26-28 dB (vs v1: ~24 dB) → +2-4 dB improvement\n",
    "- **SSIM**: 0.90-0.92 (vs v1: ~0.85) → +0.05-0.07 improvement\n",
    "- **Visual**: Smooth textures, clear edges, natural multi-scale\n",
    "\n",
    "### If Multi-Directional Works:\n",
    "→ Confirms **spatial locality** was the main issue causing noise\n",
    "\n",
    "### If No Improvement:\n",
    "→ Investigate other causes:\n",
    "1. Training duration (200 epochs insufficient)\n",
    "2. Fourier scale (scale=10 too high)\n",
    "3. ODE steps (50 too few)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
