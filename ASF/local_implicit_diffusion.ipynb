{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Implicit Field Diffusion (Option 2)\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "**Key Innovation**: Distance-aware local attention with explicit locality bias\n",
    "\n",
    "**Advantages over Perceiver IO**:\n",
    "- ✅ No latent bottleneck → preserves all spatial information\n",
    "- ✅ Explicit locality modeling → better for continuous fields\n",
    "- ✅ Distance-weighted attention → natural for sparse conditioning\n",
    "- ✅ FiLM time modulation → proven better for diffusion models\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "Sparse Input (20%) + Query Points\n",
    "        ↓\n",
    "Fourier Features + Distance Encoding\n",
    "        ↓\n",
    "Local Attention (masked by distance < radius)\n",
    "        ↓\n",
    "FiLM Modulation (time conditioning)\n",
    "        ↓\n",
    "MLP Decoder → Predicted RGB\n",
    "```\n",
    "\n",
    "## Three Approaches Implemented\n",
    "1. Flow Matching (simplest, fastest)\n",
    "2. NF Denoiser (Gaussian RF idea)\n",
    "3. Score-Based (most principled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "from core.neural_fields.perceiver import FourierFeatures\n",
    "from core.sparse.cifar10_sparse import SparseCIFAR10Dataset\n",
    "from core.sparse.metrics import MetricsTracker, print_metrics, visualize_predictions\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Core Components\n",
    "\n",
    "### Time Embedding & FiLM Modulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalTimeEmbedding(nn.Module):\n",
    "    \"\"\"Sinusoidal time embedding\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    \n",
    "    def forward(self, t):\n",
    "        device = t.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat([emb.sin(), emb.cos()], dim=-1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "class FiLMLayer(nn.Module):\n",
    "    \"\"\"Feature-wise Linear Modulation for time conditioning\"\"\"\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Linear(d_model, d_model)\n",
    "        self.shift = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, t_emb):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, N, d_model) features\n",
    "            t_emb: (B, d_model) time embedding\n",
    "        \"\"\"\n",
    "        scale = self.scale(t_emb).unsqueeze(1)  # (B, 1, d_model)\n",
    "        shift = self.shift(t_emb).unsqueeze(1)  # (B, 1, d_model)\n",
    "        return x * (1 + scale) + shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Distance-Weighted Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalAttentionLayer(nn.Module):\n",
    "    \"\"\"Local attention with distance-based masking and weighting\"\"\"\n",
    "    def __init__(self, d_model, num_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        # Query, Key, Value projections\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Distance encoding\n",
    "        self.dist_bias = nn.Sequential(\n",
    "            nn.Linear(1, num_heads),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Feed-forward\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, queries, keys_values, distances, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            queries: (B, N_q, d_model)\n",
    "            keys_values: (B, N_q, N_k, d_model) - for each query, all keys with distance encoding\n",
    "            distances: (B, N_q, N_k) - pairwise distances\n",
    "            mask: (B, N_q, N_k) - locality mask (1 = attend, 0 = ignore)\n",
    "        \"\"\"\n",
    "        B, N_q, d_model = queries.shape\n",
    "        N_k = keys_values.shape[2]\n",
    "        \n",
    "        # Residual connection\n",
    "        residual = queries\n",
    "        queries = self.layer_norm1(queries)\n",
    "        \n",
    "        # Project Q, K, V\n",
    "        Q = self.q_proj(queries).view(B, N_q, self.num_heads, self.head_dim).transpose(1, 2)  # (B, H, N_q, d_h)\n",
    "        \n",
    "        # Average pool keys_values across the N_k dimension to get single key/value per query\n",
    "        # Better approach: weighted average by distance\n",
    "        if mask is not None:\n",
    "            weights = mask.unsqueeze(-1)  # (B, N_q, N_k, 1)\n",
    "            kv_pooled = (keys_values * weights).sum(dim=2) / (weights.sum(dim=2) + 1e-6)  # (B, N_q, d_model)\n",
    "        else:\n",
    "            kv_pooled = keys_values.mean(dim=2)  # (B, N_q, d_model)\n",
    "        \n",
    "        K = self.k_proj(kv_pooled).view(B, N_q, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.v_proj(kv_pooled).view(B, N_q, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)  # (B, H, N_q, N_q)\n",
    "        \n",
    "        # Apply attention\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        out = torch.matmul(attn, V)  # (B, H, N_q, d_h)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, N_q, d_model)\n",
    "        out = self.out_proj(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Residual + Feed-forward\n",
    "        out = out + residual\n",
    "        out = out + self.ff(self.layer_norm2(out))\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Architecture: Local Implicit Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalImplicitDiffusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Local neural fields with distance-aware attention\n",
    "    \n",
    "    Key features:\n",
    "    - No latent bottleneck (preserves spatial info)\n",
    "    - Explicit locality via distance masking\n",
    "    - FiLM modulation for time conditioning\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_fourier_feats=256,\n",
    "        d_model=512,\n",
    "        num_layers=4,\n",
    "        num_heads=8,\n",
    "        local_radius=0.3,  # Locality radius in normalized coords\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.local_radius = local_radius\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Fourier features\n",
    "        self.fourier = FourierFeatures(coord_dim=2, num_freqs=num_fourier_feats, scale=10.0)\n",
    "        feat_dim = num_fourier_feats * 4  # sin/cos for x and y\n",
    "        \n",
    "        # Distance encoding\n",
    "        self.dist_encoder = nn.Sequential(\n",
    "            nn.Linear(1, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64)\n",
    "        )\n",
    "        \n",
    "        # Project inputs: fourier + RGB + distance\n",
    "        self.input_proj = nn.Linear(feat_dim + 3 + 64, d_model)\n",
    "        \n",
    "        # Project queries: fourier + RGB\n",
    "        self.query_proj = nn.Linear(feat_dim + 3, d_model)\n",
    "        \n",
    "        # Time embedding\n",
    "        self.time_embed = SinusoidalTimeEmbedding(d_model)\n",
    "        \n",
    "        # FiLM layers for time conditioning\n",
    "        self.film_layers = nn.ModuleList([\n",
    "            FiLMLayer(d_model) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Local attention layers\n",
    "        self.attn_layers = nn.ModuleList([\n",
    "            LocalAttentionLayer(d_model, num_heads, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 2, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, 3)\n",
    "        )\n",
    "    \n",
    "    def forward(self, noisy_values, query_coords, t, input_coords, input_values):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            noisy_values: (B, N_out, 3) - noisy RGB values at query points\n",
    "            query_coords: (B, N_out, 2) - query coordinates\n",
    "            t: (B,) - timestep\n",
    "            input_coords: (B, N_in, 2) - sparse input coordinates  \n",
    "            input_values: (B, N_in, 3) - sparse input RGB values\n",
    "        \"\"\"\n",
    "        B, N_out, _ = query_coords.shape\n",
    "        N_in = input_coords.shape[1]\n",
    "        \n",
    "        # Time embedding\n",
    "        t_emb = self.time_embed(t)  # (B, d_model)\n",
    "        \n",
    "        # Compute pairwise distances\n",
    "        dist = torch.cdist(query_coords, input_coords)  # (B, N_out, N_in)\n",
    "        \n",
    "        # Create locality mask (only attend to nearby points)\n",
    "        mask = (dist < self.local_radius).float()  # (B, N_out, N_in)\n",
    "        \n",
    "        # Encode distances\n",
    "        dist_feats = self.dist_encoder(dist.unsqueeze(-1))  # (B, N_out, N_in, 64)\n",
    "        \n",
    "        # Fourier features\n",
    "        input_feats = self.fourier(input_coords)  # (B, N_in, feat_dim)\n",
    "        query_feats = self.fourier(query_coords)  # (B, N_out, feat_dim)\n",
    "        \n",
    "        # For each query, encode all inputs with distance\n",
    "        input_feats_exp = input_feats.unsqueeze(1).expand(B, N_out, N_in, -1)\n",
    "        input_rgb_exp = input_values.unsqueeze(1).expand(B, N_out, N_in, -1)\n",
    "        \n",
    "        # Input tokens with distance encoding\n",
    "        input_tokens = self.input_proj(\n",
    "            torch.cat([input_feats_exp, input_rgb_exp, dist_feats], dim=-1)\n",
    "        )  # (B, N_out, N_in, d_model)\n",
    "        \n",
    "        # Query tokens\n",
    "        query_tokens = self.query_proj(\n",
    "            torch.cat([query_feats, noisy_values], dim=-1)\n",
    "        )  # (B, N_out, d_model)\n",
    "        \n",
    "        # Process through layers with FiLM + Local Attention\n",
    "        x = query_tokens\n",
    "        for film_layer, attn_layer in zip(self.film_layers, self.attn_layers):\n",
    "            # FiLM time modulation\n",
    "            x = film_layer(x, t_emb)\n",
    "            \n",
    "            # Local attention with distance masking\n",
    "            x = attn_layer(x, input_tokens, dist, mask)\n",
    "        \n",
    "        # Decode to RGB\n",
    "        return self.decoder(x)\n",
    "\n",
    "\n",
    "# Test model\n",
    "model = LocalImplicitDiffusion(\n",
    "    num_fourier_feats=256,\n",
    "    d_model=512,\n",
    "    num_layers=4,\n",
    "    num_heads=8,\n",
    "    local_radius=0.3\n",
    ").to(device)\n",
    "\n",
    "test_noisy = torch.rand(4, 204, 3).to(device)\n",
    "test_query_coords = torch.rand(4, 204, 2).to(device)\n",
    "test_t = torch.rand(4).to(device)\n",
    "test_input_coords = torch.rand(4, 204, 2).to(device)\n",
    "test_input_values = torch.rand(4, 204, 3).to(device)\n",
    "\n",
    "test_out = model(test_noisy, test_query_coords, test_t, test_input_coords, test_input_values)\n",
    "print(f\"Model test: {test_out.shape}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training: Flow Matching (Recommended)\n",
    "\n",
    "Simplest and fastest approach - train with straight-path flow matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_flow(x_0, x_1, t):\n",
    "    \"\"\"Linear interpolation: (1-t)*x_0 + t*x_1\"\"\"\n",
    "    return (1 - t) * x_0 + t * x_1\n",
    "\n",
    "def target_velocity(x_0, x_1):\n",
    "    \"\"\"Target velocity: x_1 - x_0\"\"\"\n",
    "    return x_1 - x_0\n",
    "\n",
    "@torch.no_grad()\n",
    "def heun_sample(model, output_coords, input_coords, input_values, num_steps=50, device='cuda'):\n",
    "    \"\"\"Heun ODE solver for flow matching\"\"\"\n",
    "    B, N_out = output_coords.shape[0], output_coords.shape[1]\n",
    "    x_t = torch.randn(B, N_out, 3, device=device)\n",
    "    \n",
    "    dt = 1.0 / num_steps\n",
    "    ts = torch.linspace(0, 1 - dt, num_steps)\n",
    "    \n",
    "    for t_val in tqdm(ts, desc=\"Sampling\", leave=False):\n",
    "        t = torch.full((B,), t_val.item(), device=device)\n",
    "        t_next = torch.full((B,), t_val.item() + dt, device=device)\n",
    "        \n",
    "        v1 = model(x_t, output_coords, t, input_coords, input_values)\n",
    "        x_next_pred = x_t + dt * v1\n",
    "        \n",
    "        v2 = model(x_next_pred, output_coords, t_next, input_coords, input_values)\n",
    "        x_t = x_t + dt * 0.5 * (v1 + v2)\n",
    "    \n",
    "    return torch.clamp(x_t, 0, 1)\n",
    "\n",
    "def train_flow_matching(\n",
    "    model, train_loader, test_loader, epochs=100, lr=1e-4, device='cuda',\n",
    "    visualize_every=5, eval_every=2\n",
    "):\n",
    "    \"\"\"Train with flow matching\"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    losses = []\n",
    "    \n",
    "    viz_batch = next(iter(train_loader))\n",
    "    viz_input_coords = viz_batch['input_coords'][:4].to(device)\n",
    "    viz_input_values = viz_batch['input_values'][:4].to(device)\n",
    "    viz_output_coords = viz_batch['output_coords'][:4].to(device)\n",
    "    viz_output_values = viz_batch['output_values'][:4].to(device)\n",
    "    viz_full_images = viz_batch['full_image'][:4].to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            input_coords = batch['input_coords'].to(device)\n",
    "            input_values = batch['input_values'].to(device)\n",
    "            output_coords = batch['output_coords'].to(device)\n",
    "            output_values = batch['output_values'].to(device)\n",
    "            \n",
    "            B = input_coords.shape[0]\n",
    "            t = torch.rand(B, device=device)\n",
    "            \n",
    "            x_0 = torch.randn_like(output_values)\n",
    "            x_1 = output_values\n",
    "            \n",
    "            t_broadcast = t.view(B, 1, 1)\n",
    "            x_t = conditional_flow(x_0, x_1, t_broadcast)\n",
    "            u_t = target_velocity(x_0, x_1)\n",
    "            \n",
    "            v_pred = model(x_t, output_coords, t, input_coords, input_values)\n",
    "            loss = F.mse_loss(v_pred, u_t)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        losses.append(avg_loss)\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Loss = {avg_loss:.6f}, LR = {scheduler.get_last_lr()[0]:.6f}\")\n",
    "        \n",
    "        # Evaluation\n",
    "        if (epoch + 1) % eval_every == 0 or epoch == 0:\n",
    "            model.eval()\n",
    "            tracker = MetricsTracker()\n",
    "            with torch.no_grad():\n",
    "                for i, batch in enumerate(test_loader):\n",
    "                    if i >= 10:\n",
    "                        break\n",
    "                    pred_values = heun_sample(\n",
    "                        model, batch['output_coords'].to(device),\n",
    "                        batch['input_coords'].to(device), batch['input_values'].to(device),\n",
    "                        num_steps=50, device=device\n",
    "                    )\n",
    "                    tracker.update(pred_values, batch['output_values'].to(device))\n",
    "                results = tracker.compute()\n",
    "                print(f\"  Eval - MSE: {results['mse']:.6f}, MAE: {results['mae']:.6f}\")\n",
    "        \n",
    "        # Visualization\n",
    "        if (epoch + 1) % visualize_every == 0 or epoch == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                pred_values = heun_sample(\n",
    "                    model, viz_output_coords, viz_input_coords, viz_input_values,\n",
    "                    num_steps=50, device=device\n",
    "                )\n",
    "                fig = visualize_predictions(\n",
    "                    viz_input_coords, viz_input_values, viz_output_coords,\n",
    "                    pred_values, viz_output_values, viz_full_images, n_samples=4\n",
    "                )\n",
    "                plt.suptitle(f'Local Implicit - Epoch {epoch+1}', fontsize=14, y=1.02)\n",
    "                plt.savefig(f'local_implicit_epoch_{epoch+1:03d}.png', dpi=150, bbox_inches='tight')\n",
    "                plt.show()\n",
    "                plt.close()\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "train_dataset = SparseCIFAR10Dataset(\n",
    "    root='../data', train=True, input_ratio=0.2, output_ratio=0.2, download=True, seed=42\n",
    ")\n",
    "test_dataset = SparseCIFAR10Dataset(\n",
    "    root='../data', train=False, input_ratio=0.2, output_ratio=0.2, download=True, seed=42\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Test: {len(test_dataset)}\")\n",
    "\n",
    "# Initialize model\n",
    "model = LocalImplicitDiffusion(\n",
    "    num_fourier_feats=256,\n",
    "    d_model=512,\n",
    "    num_layers=4,\n",
    "    num_heads=8,\n",
    "    local_radius=0.3\n",
    ").to(device)\n",
    "\n",
    "# Train\n",
    "losses = train_flow_matching(model, train_loader, test_loader, epochs=100, lr=1e-4, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Final Evaluation: Full Image Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses, linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss: Local Implicit Diffusion')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Full image reconstruction\n",
    "def create_full_grid(image_size=32, device='cuda'):\n",
    "    y, x = torch.meshgrid(\n",
    "        torch.linspace(0, 1, image_size),\n",
    "        torch.linspace(0, 1, image_size),\n",
    "        indexing='ij'\n",
    "    )\n",
    "    return torch.stack([x.flatten(), y.flatten()], dim=-1).to(device)\n",
    "\n",
    "full_coords = create_full_grid(32, device)\n",
    "\n",
    "model.eval()\n",
    "tracker_full = MetricsTracker()\n",
    "\n",
    "for i, batch in enumerate(tqdm(test_loader, desc=\"Full Reconstruction\")):\n",
    "    if i >= 50:\n",
    "        break\n",
    "    \n",
    "    B = batch['input_coords'].shape[0]\n",
    "    full_coords_batch = full_coords.unsqueeze(0).expand(B, -1, -1)\n",
    "    \n",
    "    pred_values = heun_sample(\n",
    "        model, full_coords_batch,\n",
    "        batch['input_coords'].to(device),\n",
    "        batch['input_values'].to(device),\n",
    "        num_steps=100, device=device\n",
    "    )\n",
    "    \n",
    "    pred_images = pred_values.view(B, 32, 32, 3).permute(0, 3, 1, 2)\n",
    "    tracker_full.update(None, None, pred_images, batch['full_image'].to(device))\n",
    "\n",
    "results = tracker_full.compute()\n",
    "print(f\"\\nFull Image Reconstruction:\")\n",
    "print(f\"  PSNR: {results['psnr']:.2f} dB\")\n",
    "print(f\"  SSIM: {results['ssim']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Full Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = next(iter(test_loader))\n",
    "B = 4\n",
    "full_coords_batch = full_coords.unsqueeze(0).expand(B, -1, -1)\n",
    "\n",
    "pred_values = heun_sample(\n",
    "    model, full_coords_batch,\n",
    "    sample_batch['input_coords'][:B].to(device),\n",
    "    sample_batch['input_values'][:B].to(device),\n",
    "    num_steps=100, device=device\n",
    ")\n",
    "pred_images = pred_values.view(B, 32, 32, 3).permute(0, 3, 1, 2)\n",
    "\n",
    "fig, axes = plt.subplots(4, 3, figsize=(12, 16))\n",
    "for i in range(4):\n",
    "    # Ground truth\n",
    "    axes[i, 0].imshow(sample_batch['full_image'][i].permute(1, 2, 0).numpy())\n",
    "    axes[i, 0].set_title('Ground Truth')\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    # Sparse input\n",
    "    input_img = torch.zeros(3, 32, 32)\n",
    "    input_idx = sample_batch['input_indices'][i]\n",
    "    input_img.view(3, -1)[:, input_idx] = sample_batch['input_values'][i].T\n",
    "    axes[i, 1].imshow(input_img.permute(1, 2, 0).numpy())\n",
    "    axes[i, 1].set_title(f'Input (20%)')\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    # Reconstruction\n",
    "    axes[i, 2].imshow(np.clip(pred_images[i].permute(1, 2, 0).cpu().numpy(), 0, 1))\n",
    "    axes[i, 2].set_title('Reconstructed')\n",
    "    axes[i, 2].axis('off')\n",
    "\n",
    "plt.suptitle('Local Implicit Diffusion: Full Image Reconstruction', fontsize=14, y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig('local_implicit_full_reconstruction.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
