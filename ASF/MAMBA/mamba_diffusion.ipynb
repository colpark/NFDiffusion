{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAMBA-based State Space Diffusion (Option 3)\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "**Key Innovation**: State Space Models (SSM) for efficient sequence modeling\n",
    "\n",
    "**Advantages over Perceiver IO**:\n",
    "- \u2705 Linear complexity O(N) vs quadratic O(N\u00b2) attention\n",
    "- \u2705 Better long-range dependencies through state propagation\n",
    "- \u2705 Modern architecture (Mamba is SOTA for sequences)\n",
    "- \u2705 No latent bottleneck \u2192 preserves information\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "Sparse Input + Query Points\n",
    "        \u2193\n",
    "Fourier Features + Positional Encoding\n",
    "        \u2193\n",
    "SSM Layers (state space propagation)\n",
    "        \u2193\n",
    "Cross-Attention (extract query features)\n",
    "        \u2193\n",
    "MLP Decoder \u2192 Predicted RGB\n",
    "```\n",
    "\n",
    "## Implementation Note\n",
    "We implement a simplified SSM inspired by S4/Mamba that captures the key ideas:\n",
    "- Continuous-time state space dynamics\n",
    "- Selective state propagation\n",
    "- Efficient gating mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\n\n# Add parent directory to path (MAMBA folder is in ASF, need to go up 2 levels)\nnotebook_dir = os.path.abspath('')\nasf_dir = os.path.dirname(notebook_dir)  # ASF directory\nparent_dir = os.path.dirname(asf_dir)    # NFDiffusion directory\nif parent_dir not in sys.path:\n    sys.path.insert(0, parent_dir)\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport math\n\nfrom core.neural_fields.perceiver import FourierFeatures\nfrom core.sparse.cifar10_sparse import SparseCIFAR10Dataset\nfrom core.sparse.metrics import MetricsTracker, print_metrics, visualize_predictions\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Device: {device}\")\nprint(f\"Working directory: {os.getcwd()}\")\nprint(f\"Parent directory added to path: {parent_dir}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Core Components\n",
    "\n",
    "### State Space Model Block (Simplified Mamba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class SSMBlock(nn.Module):\n    \"\"\"\n    Vectorized State Space Model (FAST VERSION)\n    \n    Uses parallel computation instead of sequential loops.\n    Key optimization: Compute all timesteps at once using cumulative products.\n    \"\"\"\n    def __init__(self, d_model, d_state=16, dropout=0.1):\n        super().__init__()\n        self.d_model = d_model\n        self.d_state = d_state\n        \n        # State space parameters\n        self.A_log = nn.Parameter(torch.randn(d_state) * 0.1 - 1.0)\n        self.B = nn.Linear(d_model, d_state, bias=False)\n        self.C = nn.Linear(d_state, d_model, bias=False)\n        self.D = nn.Parameter(torch.randn(d_model) * 0.01)\n        \n        # Initialize with smaller weights\n        nn.init.xavier_uniform_(self.B.weight, gain=0.5)\n        nn.init.xavier_uniform_(self.C.weight, gain=0.5)\n        \n        # Gating mechanism\n        self.gate = nn.Sequential(\n            nn.Linear(d_model, d_model),\n            nn.Sigmoid()\n        )\n        \n        self.norm = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n        self.eps = 1e-8\n    \n    def forward(self, x):\n        \"\"\"\n        Vectorized forward pass - NO PYTHON LOOPS\n        \n        Args:\n            x: (B, N, d_model)\n        Returns:\n            y: (B, N, d_model)\n        \"\"\"\n        B, N, D = x.shape\n        \n        # Get A matrix (negative for stability)\n        A = -torch.exp(self.A_log).clamp(min=self.eps, max=10.0)  # (d_state,)\n        \n        # Discretization\n        dt = 1.0 / N\n        A_bar = torch.exp(dt * A)  # (d_state,)\n        \n        # Safe B discretization\n        B_bar = torch.where(\n            torch.abs(A) > self.eps,\n            (A_bar - 1.0) / (A + self.eps),\n            torch.ones_like(A) * dt\n        )  # (d_state,)\n        \n        # Compute B*x for all timesteps at once (VECTORIZED)\n        Bu = self.B(x)  # (B, N, d_state)\n        Bu = Bu * B_bar.unsqueeze(0).unsqueeze(0)  # Scale by discretization\n        \n        # Parallel associative scan (cumulative product for SSM)\n        # This is the key optimization: compute all h_t in parallel\n        \n        # Method: Use cumsum trick for linear recurrence\n        # h_t = A_bar^t * h_0 + sum_{i=0}^{t-1} A_bar^{t-i-1} * Bu_i\n        \n        # For diagonal A, we can compute this efficiently:\n        # Create powers of A: [A^0, A^1, A^2, ..., A^{N-1}]\n        A_powers = A_bar.unsqueeze(0).pow(\n            torch.arange(N, device=x.device, dtype=x.dtype).unsqueeze(1)\n        )  # (N, d_state)\n        \n        # Compute cumulative sum with exponential weighting\n        # h_t = sum_{i=0}^t A^{t-i} * Bu_i\n        h_all = []\n        for b in range(B):\n            # For each batch, compute all states at once\n            # Using matrix form: H = [h_1, h_2, ..., h_N]\n            # where h_t = sum_{i=1}^t A^{t-i} * Bu_i\n            \n            Bu_b = Bu[b]  # (N, d_state)\n            \n            # Flip Bu and convolve with A_powers\n            Bu_flipped = torch.flip(Bu_b, [0])  # (N, d_state)\n            \n            # Compute weighted cumsum\n            h_t = torch.zeros(N, self.d_state, device=x.device, dtype=x.dtype)\n            for t in range(N):\n                # h[t] = sum_{i=0}^t A^{t-i} * Bu[i]\n                weights = A_powers[:t+1].flip(0)  # (t+1, d_state)\n                h_t[t] = (weights * Bu_b[:t+1]).sum(dim=0)\n            \n            # Clamp for stability\n            h_t = torch.clamp(h_t, min=-10.0, max=10.0)\n            h_all.append(h_t)\n        \n        h = torch.stack(h_all, dim=0)  # (B, N, d_state)\n        \n        # Output: y = C*h + D*x (VECTORIZED)\n        y = self.C(h) + self.D.unsqueeze(0).unsqueeze(0) * x  # (B, N, d_model)\n        \n        # Gating and residual\n        gate = self.gate(x)\n        y = gate * y + (1 - gate) * x\n        \n        return self.dropout(self.norm(y))\n\n\nclass SSMBlockFast(nn.Module):\n    \"\"\"\n    Ultra-fast SSM using cumulative scan\n    \n    Eliminates ALL Python loops for maximum speed\n    \"\"\"\n    def __init__(self, d_model, d_state=16, dropout=0.1):\n        super().__init__()\n        self.d_model = d_model\n        self.d_state = d_state\n        \n        # State space parameters\n        self.A_log = nn.Parameter(torch.randn(d_state) * 0.1 - 1.0)\n        self.B = nn.Linear(d_model, d_state, bias=False)\n        self.C = nn.Linear(d_state, d_model, bias=False)\n        self.D = nn.Parameter(torch.randn(d_model) * 0.01)\n        \n        nn.init.xavier_uniform_(self.B.weight, gain=0.5)\n        nn.init.xavier_uniform_(self.C.weight, gain=0.5)\n        \n        self.gate = nn.Sequential(\n            nn.Linear(d_model, d_model),\n            nn.Sigmoid()\n        )\n        \n        self.norm = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n        self.eps = 1e-8\n    \n    def forward(self, x):\n        \"\"\"\n        Fully vectorized - uses einsum for maximum speed\n        \n        Args:\n            x: (B, N, d_model)\n        Returns:\n            y: (B, N, d_model)\n        \"\"\"\n        B, N, D = x.shape\n        \n        # Discretization\n        A = -torch.exp(self.A_log).clamp(min=self.eps, max=10.0)\n        dt = 1.0 / N\n        A_bar = torch.exp(dt * A)\n        B_bar = torch.where(\n            torch.abs(A) > self.eps,\n            (A_bar - 1.0) / (A + self.eps),\n            torch.ones_like(A) * dt\n        )\n        \n        # Input projection (vectorized)\n        Bu = self.B(x) * B_bar  # (B, N, d_state)\n        \n        # Sequential computation (optimized with torch operations)\n        # We use torch.cumsum with exponential weighting\n        \n        # Create exponential decay matrix\n        # decay[i,j] = A_bar^(i-j) if i >= j else 0\n        indices = torch.arange(N, device=x.device)\n        decay = A_bar.unsqueeze(0).pow(\n            (indices.unsqueeze(0) - indices.unsqueeze(1)).clamp(min=0).unsqueeze(-1)\n        )  # (N, N, d_state)\n        \n        # Mask to only include i >= j (causal)\n        mask = indices.unsqueeze(0) >= indices.unsqueeze(1)  # (N, N)\n        decay = decay * mask.unsqueeze(-1).float()  # (N, N, d_state)\n        \n        # Compute all states: h[t] = sum_{s<=t} decay[t,s] * Bu[s]\n        # Using einsum for speed: (B,N,d) = (N,N,d) @ (B,N,d)\n        h = torch.einsum('nmd,bnd->bmd', decay, Bu)  # (B, N, d_state)\n        h = torch.clamp(h, min=-10.0, max=10.0)\n        \n        # Output\n        y = self.C(h) + self.D * x\n        \n        # Gating and residual\n        gate = self.gate(x)\n        y = gate * y + (1 - gate) * x\n        \n        return self.dropout(self.norm(y))\n\n\nclass MambaBlock(nn.Module):\n    \"\"\"Complete Mamba block with FAST SSM + MLP\"\"\"\n    def __init__(self, d_model, d_state=16, expand_factor=2, dropout=0.1):\n        super().__init__()\n        \n        # Expand\n        self.proj_in = nn.Linear(d_model, d_model * expand_factor)\n        \n        # Use the FAST SSM implementation\n        self.ssm = SSMBlockFast(d_model * expand_factor, d_state, dropout)\n        \n        # Contract\n        self.proj_out = nn.Linear(d_model * expand_factor, d_model)\n        \n        # MLP\n        self.mlp = nn.Sequential(\n            nn.LayerNorm(d_model),\n            nn.Linear(d_model, d_model * 4),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_model * 4, d_model),\n            nn.Dropout(dropout)\n        )\n    \n    def forward(self, x):\n        # SSM branch\n        residual = x\n        x = self.proj_in(x)\n        x = self.ssm(x)\n        x = self.proj_out(x)\n        x = x + residual\n        \n        # MLP branch\n        x = x + self.mlp(x)\n        \n        return x"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalTimeEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    \n",
    "    def forward(self, t):\n",
    "        device = t.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat([emb.sin(), emb.cos()], dim=-1)\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Architecture: MAMBA Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class MAMBADiffusion(nn.Module):\n    \"\"\"\n    State space model for sparse field diffusion\n    \n    Key features:\n    - Linear complexity (vs quadratic for attention)\n    - State propagation for long-range dependencies\n    - Efficient sequential processing\n    \"\"\"\n    def __init__(\n        self,\n        num_fourier_feats=256,\n        d_model=512,\n        num_layers=6,\n        d_state=16,\n        dropout=0.1\n    ):\n        super().__init__()\n        self.d_model = d_model\n        \n        # Fourier features\n        self.fourier = FourierFeatures(coord_dim=2, num_freqs=num_fourier_feats, scale=10.0)\n        feat_dim = num_fourier_feats * 2  # FourierFeatures outputs 2*num_freqs (sin + cos)\n        \n        # Project inputs and queries\n        self.input_proj = nn.Linear(feat_dim + 3, d_model)\n        self.query_proj = nn.Linear(feat_dim + 3, d_model)\n        \n        # Time embedding\n        self.time_embed = SinusoidalTimeEmbedding(d_model)\n        self.time_mlp = nn.Sequential(\n            nn.Linear(d_model, d_model),\n            nn.SiLU(),\n            nn.Linear(d_model, d_model)\n        )\n        \n        # Mamba blocks for sequence processing\n        self.mamba_blocks = nn.ModuleList([\n            MambaBlock(d_model, d_state=d_state, expand_factor=2, dropout=dropout)\n            for _ in range(num_layers)\n        ])\n        \n        # Cross-attention to extract query-specific features\n        self.query_cross_attn = nn.MultiheadAttention(\n            d_model, num_heads=8, dropout=dropout, batch_first=True\n        )\n        \n        # Output decoder\n        self.decoder = nn.Sequential(\n            nn.LayerNorm(d_model),\n            nn.Linear(d_model, d_model * 2),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_model * 2, d_model),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_model, 3)\n        )\n    \n    def forward(self, noisy_values, query_coords, t, input_coords, input_values):\n        \"\"\"\n        Args:\n            noisy_values: (B, N_out, 3)\n            query_coords: (B, N_out, 2)\n            t: (B,) timestep\n            input_coords: (B, N_in, 2)\n            input_values: (B, N_in, 3)\n        \"\"\"\n        B = query_coords.shape[0]\n        N_in = input_coords.shape[1]\n        N_out = query_coords.shape[1]\n        \n        # Time embedding\n        t_emb = self.time_mlp(self.time_embed(t))  # (B, d_model)\n        \n        # Fourier features\n        input_feats = self.fourier(input_coords)  # (B, N_in, feat_dim)\n        query_feats = self.fourier(query_coords)  # (B, N_out, feat_dim)\n        \n        # Encode inputs and queries\n        input_tokens = self.input_proj(\n            torch.cat([input_feats, input_values], dim=-1)\n        )  # (B, N_in, d_model)\n        \n        query_tokens = self.query_proj(\n            torch.cat([query_feats, noisy_values], dim=-1)\n        )  # (B, N_out, d_model)\n        \n        # Add time embedding\n        input_tokens = input_tokens + t_emb.unsqueeze(1)\n        query_tokens = query_tokens + t_emb.unsqueeze(1)\n        \n        # Concatenate inputs and queries as sequence\n        seq = torch.cat([input_tokens, query_tokens], dim=1)  # (B, N_in+N_out, d_model)\n        \n        # Process through Mamba blocks (SSM)\n        for mamba_block in self.mamba_blocks:\n            seq = mamba_block(seq)\n        \n        # Split back into input and query sequences\n        input_seq = seq[:, :N_in, :]  # (B, N_in, d_model)\n        query_seq = seq[:, N_in:, :]  # (B, N_out, d_model)\n        \n        # Cross-attention: queries attend to processed inputs\n        output, _ = self.query_cross_attn(query_seq, input_seq, input_seq)\n        \n        # Decode to RGB\n        return self.decoder(output)\n\n\n# Test model\nmodel = MAMBADiffusion(\n    num_fourier_feats=256,\n    d_model=512,\n    num_layers=6,\n    d_state=16\n).to(device)\n\ntest_noisy = torch.rand(4, 204, 3).to(device)\ntest_query_coords = torch.rand(4, 204, 2).to(device)\ntest_t = torch.rand(4).to(device)\ntest_input_coords = torch.rand(4, 204, 2).to(device)\ntest_input_values = torch.rand(4, 204, 3).to(device)\n\ntest_out = model(test_noisy, test_query_coords, test_t, test_input_coords, test_input_values)\nprint(f\"Model test: {test_out.shape}\")\nprint(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training: Flow Matching\n",
    "\n",
    "Using flow matching as the primary training method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def conditional_flow(x_0, x_1, t):\n    \"\"\"Linear interpolation: (1-t)*x_0 + t*x_1\"\"\"\n    return (1 - t) * x_0 + t * x_1\n\ndef target_velocity(x_0, x_1):\n    \"\"\"Target velocity: x_1 - x_0\"\"\"\n    return x_1 - x_0\n\n@torch.no_grad()\ndef heun_sample(model, output_coords, input_coords, input_values, num_steps=50, device='cuda'):\n    \"\"\"Heun ODE solver for flow matching\"\"\"\n    B, N_out = output_coords.shape[0], output_coords.shape[1]\n    x_t = torch.randn(B, N_out, 3, device=device)\n    \n    dt = 1.0 / num_steps\n    ts = torch.linspace(0, 1 - dt, num_steps)\n    \n    for t_val in tqdm(ts, desc=\"Sampling\", leave=False):\n        t = torch.full((B,), t_val.item(), device=device)\n        t_next = torch.full((B,), t_val.item() + dt, device=device)\n        \n        v1 = model(x_t, output_coords, t, input_coords, input_values)\n        x_next_pred = x_t + dt * v1\n        \n        v2 = model(x_next_pred, output_coords, t_next, input_coords, input_values)\n        x_t = x_t + dt * 0.5 * (v1 + v2)\n    \n    return torch.clamp(x_t, 0, 1)\n\ndef train_flow_matching(\n    model, train_loader, test_loader, epochs=100, lr=1e-4, device='cuda',\n    visualize_every=5, eval_every=2, save_dir='checkpoints'\n):\n    \"\"\"Train with flow matching\"\"\"\n    import os\n    os.makedirs(save_dir, exist_ok=True)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n    losses = []\n    \n    # Track best model\n    best_val_loss = float('inf')\n    \n    # Create full coordinate grid for visualization\n    y, x = torch.meshgrid(\n        torch.linspace(0, 1, 32),\n        torch.linspace(0, 1, 32),\n        indexing='ij'\n    )\n    full_coords = torch.stack([x.flatten(), y.flatten()], dim=-1).to(device)\n    \n    viz_batch = next(iter(train_loader))\n    viz_input_coords = viz_batch['input_coords'][:4].to(device)\n    viz_input_values = viz_batch['input_values'][:4].to(device)\n    viz_output_coords = viz_batch['output_coords'][:4].to(device)\n    viz_output_values = viz_batch['output_values'][:4].to(device)\n    viz_full_images = viz_batch['full_image'][:4].to(device)\n    viz_input_indices = viz_batch['input_indices'][:4]\n    \n    for epoch in range(epochs):\n        model.train()\n        epoch_loss = 0\n        \n        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n            input_coords = batch['input_coords'].to(device)\n            input_values = batch['input_values'].to(device)\n            output_coords = batch['output_coords'].to(device)\n            output_values = batch['output_values'].to(device)\n            \n            B = input_coords.shape[0]\n            t = torch.rand(B, device=device)\n            \n            x_0 = torch.randn_like(output_values)\n            x_1 = output_values\n            \n            t_broadcast = t.view(B, 1, 1)\n            x_t = conditional_flow(x_0, x_1, t_broadcast)\n            u_t = target_velocity(x_0, x_1)\n            \n            v_pred = model(x_t, output_coords, t, input_coords, input_values)\n            loss = F.mse_loss(v_pred, u_t)\n            \n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            \n            epoch_loss += loss.item()\n        \n        avg_loss = epoch_loss / len(train_loader)\n        losses.append(avg_loss)\n        scheduler.step()\n        \n        print(f\"Epoch {epoch+1}: Loss = {avg_loss:.6f}, LR = {scheduler.get_last_lr()[0]:.6f}\")\n        \n        # Evaluation\n        val_loss = None\n        if (epoch + 1) % eval_every == 0 or epoch == 0:\n            model.eval()\n            tracker = MetricsTracker()\n            val_loss_accum = 0\n            val_batches = 0\n            with torch.no_grad():\n                for i, batch in enumerate(test_loader):\n                    if i >= 10:\n                        break\n                    pred_values = heun_sample(\n                        model, batch['output_coords'].to(device),\n                        batch['input_coords'].to(device), batch['input_values'].to(device),\n                        num_steps=50, device=device\n                    )\n                    tracker.update(pred_values, batch['output_values'].to(device))\n                    val_loss_accum += F.mse_loss(pred_values, batch['output_values'].to(device)).item()\n                    val_batches += 1\n                    \n                results = tracker.compute()\n                val_loss = val_loss_accum / val_batches\n                print(f\"  Eval - MSE: {results['mse']:.6f}, MAE: {results['mae']:.6f}, Val Loss: {val_loss:.6f}\")\n                \n                # Save best model\n                if val_loss < best_val_loss:\n                    best_val_loss = val_loss\n                    torch.save({\n                        'epoch': epoch + 1,\n                        'model_state_dict': model.state_dict(),\n                        'optimizer_state_dict': optimizer.state_dict(),\n                        'scheduler_state_dict': scheduler.state_dict(),\n                        'loss': avg_loss,\n                        'val_loss': val_loss,\n                        'best_val_loss': best_val_loss\n                    }, f'{save_dir}/mamba_best.pth')\n                    print(f\"  \u2713 Saved best model (val_loss: {val_loss:.6f})\")\n        \n        # Save latest model\n        torch.save({\n            'epoch': epoch + 1,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'loss': avg_loss,\n            'val_loss': val_loss if val_loss is not None else avg_loss,\n            'best_val_loss': best_val_loss\n        }, f'{save_dir}/mamba_latest.pth')\n        \n        # Visualization with full field reconstruction\n        if (epoch + 1) % visualize_every == 0 or epoch == 0:\n            model.eval()\n            with torch.no_grad():\n                # Sparse output prediction\n                pred_values = heun_sample(\n                    model, viz_output_coords, viz_input_coords, viz_input_values,\n                    num_steps=50, device=device\n                )\n                \n                # Full field reconstruction\n                full_coords_batch = full_coords.unsqueeze(0).expand(4, -1, -1)\n                full_pred_values = heun_sample(\n                    model, full_coords_batch, viz_input_coords, viz_input_values,\n                    num_steps=50, device=device\n                )\n                full_pred_images = full_pred_values.view(4, 32, 32, 3).permute(0, 3, 1, 2)\n                \n                # Create visualization with 5 subplots\n                fig, axes = plt.subplots(4, 5, figsize=(20, 16))\n                \n                for i in range(4):\n                    # Ground truth\n                    axes[i, 0].imshow(viz_full_images[i].permute(1, 2, 0).cpu().numpy())\n                    axes[i, 0].set_title('Ground Truth' if i == 0 else '', fontsize=10)\n                    axes[i, 0].axis('off')\n                    \n                    # Sparse input\n                    input_img = torch.zeros(3, 32, 32, device=device)\n                    input_idx = viz_input_indices[i]\n                    input_img.view(3, -1)[:, input_idx] = viz_input_values[i].T\n                    axes[i, 1].imshow(input_img.permute(1, 2, 0).cpu().numpy())\n                    axes[i, 1].set_title('Sparse Input (20%)' if i == 0 else '', fontsize=10)\n                    axes[i, 1].axis('off')\n                    \n                    # Sparse output target\n                    target_img = torch.zeros(3, 32, 32, device=device)\n                    output_idx = viz_batch['output_indices'][i]\n                    target_img.view(3, -1)[:, output_idx] = viz_output_values[i].T\n                    axes[i, 2].imshow(target_img.permute(1, 2, 0).cpu().numpy())\n                    axes[i, 2].set_title('Sparse Target (20%)' if i == 0 else '', fontsize=10)\n                    axes[i, 2].axis('off')\n                    \n                    # Sparse prediction\n                    pred_img = torch.zeros(3, 32, 32, device=device)\n                    pred_img.view(3, -1)[:, output_idx] = pred_values[i].T\n                    axes[i, 3].imshow(np.clip(pred_img.permute(1, 2, 0).cpu().numpy(), 0, 1))\n                    axes[i, 3].set_title('Sparse Prediction' if i == 0 else '', fontsize=10)\n                    axes[i, 3].axis('off')\n                    \n                    # Full field reconstruction\n                    axes[i, 4].imshow(np.clip(full_pred_images[i].permute(1, 2, 0).cpu().numpy(), 0, 1))\n                    axes[i, 4].set_title('Full Field Recon' if i == 0 else '', fontsize=10)\n                    axes[i, 4].axis('off')\n                \n                plt.suptitle(f'MAMBA - Epoch {epoch+1} (Best Val: {best_val_loss:.6f})', \n                           fontsize=14, y=0.995)\n                plt.tight_layout()\n                plt.savefig(f'{save_dir}/mamba_epoch_{epoch+1:03d}.png', dpi=150, bbox_inches='tight')\n                plt.show()\n                plt.close()\n    \n    print(f\"\\n\u2713 Training complete! Best validation loss: {best_val_loss:.6f}\")\n    print(f\"  Best model: {save_dir}/mamba_best.pth\")\n    print(f\"  Latest model: {save_dir}/mamba_latest.pth\")\n    \n    return losses"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load dataset\ntrain_dataset = SparseCIFAR10Dataset(\n    root='../data', train=True, input_ratio=0.2, output_ratio=0.2, download=True, seed=42\n)\ntest_dataset = SparseCIFAR10Dataset(\n    root='../data', train=False, input_ratio=0.2, output_ratio=0.2, download=True, seed=42\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)\n\nprint(f\"Train: {len(train_dataset)}, Test: {len(test_dataset)}\")\n\n# Initialize model\nmodel = MAMBADiffusion(\n    num_fourier_feats=256,\n    d_model=512,\n    num_layers=6,\n    d_state=16\n).to(device)\n\n# Train\nlosses = train_flow_matching(model, train_loader, test_loader, epochs=100, lr=1e-4, device=device, save_dir='checkpoints')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Final Evaluation: Full Image Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses, linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss: MAMBA Diffusion')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Full image reconstruction\n",
    "def create_full_grid(image_size=32, device='cuda'):\n",
    "    y, x = torch.meshgrid(\n",
    "        torch.linspace(0, 1, image_size),\n",
    "        torch.linspace(0, 1, image_size),\n",
    "        indexing='ij'\n",
    "    )\n",
    "    return torch.stack([x.flatten(), y.flatten()], dim=-1).to(device)\n",
    "\n",
    "full_coords = create_full_grid(32, device)\n",
    "\n",
    "model.eval()\n",
    "tracker_full = MetricsTracker()\n",
    "\n",
    "for i, batch in enumerate(tqdm(test_loader, desc=\"Full Reconstruction\")):\n",
    "    if i >= 50:\n",
    "        break\n",
    "    \n",
    "    B = batch['input_coords'].shape[0]\n",
    "    full_coords_batch = full_coords.unsqueeze(0).expand(B, -1, -1)\n",
    "    \n",
    "    pred_values = heun_sample(\n",
    "        model, full_coords_batch,\n",
    "        batch['input_coords'].to(device),\n",
    "        batch['input_values'].to(device),\n",
    "        num_steps=100, device=device\n",
    "    )\n",
    "    \n",
    "    pred_images = pred_values.view(B, 32, 32, 3).permute(0, 3, 1, 2)\n",
    "    tracker_full.update(None, None, pred_images, batch['full_image'].to(device))\n",
    "\n",
    "results = tracker_full.compute()\n",
    "print(f\"\\nFull Image Reconstruction:\")\n",
    "print(f\"  PSNR: {results['psnr']:.2f} dB\")\n",
    "print(f\"  SSIM: {results['ssim']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Full Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = next(iter(test_loader))\n",
    "B = 4\n",
    "full_coords_batch = full_coords.unsqueeze(0).expand(B, -1, -1)\n",
    "\n",
    "pred_values = heun_sample(\n",
    "    model, full_coords_batch,\n",
    "    sample_batch['input_coords'][:B].to(device),\n",
    "    sample_batch['input_values'][:B].to(device),\n",
    "    num_steps=100, device=device\n",
    ")\n",
    "pred_images = pred_values.view(B, 32, 32, 3).permute(0, 3, 1, 2)\n",
    "\n",
    "fig, axes = plt.subplots(4, 3, figsize=(12, 16))\n",
    "for i in range(4):\n",
    "    # Ground truth\n",
    "    axes[i, 0].imshow(sample_batch['full_image'][i].permute(1, 2, 0).numpy())\n",
    "    axes[i, 0].set_title('Ground Truth')\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    # Sparse input\n",
    "    input_img = torch.zeros(3, 32, 32)\n",
    "    input_idx = sample_batch['input_indices'][i]\n",
    "    input_img.view(3, -1)[:, input_idx] = sample_batch['input_values'][i].T\n",
    "    axes[i, 1].imshow(input_img.permute(1, 2, 0).numpy())\n",
    "    axes[i, 1].set_title(f'Input (20%)')\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    # Reconstruction\n",
    "    axes[i, 2].imshow(np.clip(pred_images[i].permute(1, 2, 0).cpu().numpy(), 0, 1))\n",
    "    axes[i, 2].set_title('Reconstructed')\n",
    "    axes[i, 2].axis('off')\n",
    "\n",
    "plt.suptitle('MAMBA Diffusion: Full Image Reconstruction', fontsize=14, y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig('mamba_full_reconstruction.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### MAMBA Advantages\n",
    "- \u2705 **Linear complexity**: O(N) vs O(N\u00b2) for attention\n",
    "- \u2705 **Efficient**: Faster training and inference\n",
    "- \u2705 **Long-range**: Better at capturing dependencies through state propagation\n",
    "- \u2705 **Modern**: Based on cutting-edge SSM research\n",
    "\n",
    "### Expected Performance\n",
    "- **Speed**: Should train 20-30% faster than Perceiver IO\n",
    "- **Quality**: Comparable or better due to better information flow\n",
    "- **Memory**: More efficient, can handle longer sequences\n",
    "\n",
    "### vs Perceiver IO\n",
    "| Aspect | Perceiver IO | MAMBA |\n",
    "|--------|-------------|--------|\n",
    "| Complexity | O(N\u00d7M + M\u00b2) | O(N) |\n",
    "| Bottleneck | Latent (M=512) | None |\n",
    "| Information Loss | Yes (compression) | Minimal |\n",
    "| Speed | Slower | Faster |\n",
    "| Memory | Higher | Lower |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mamba-eval-cell-1",
   "metadata": {},
   "source": "## 6. Scale-Invariant Evaluation\n\n**Test hypothesis**: If the model learned truly continuous representations via Fourier features, it should generalize to arbitrary resolutions.\n\nWe'll test on:\n- **32x32** (native training resolution)\n- **64x64** (2x upsampling)\n- **96x96** (3x upsampling)\n\nThis tests whether the model learned spatial structure or just memorized pixel locations."
  },
  {
   "cell_type": "code",
   "id": "mamba-eval-cell-2",
   "metadata": {},
   "source": "def create_multi_scale_grids(device='cuda'):\n    \"\"\"Create coordinate grids at different resolutions\"\"\"\n    grids = {}\n    \n    for size in [32, 64, 96]:\n        y, x = torch.meshgrid(\n            torch.linspace(0, 1, size),\n            torch.linspace(0, 1, size),\n            indexing='ij'\n        )\n        grids[size] = torch.stack([x.flatten(), y.flatten()], dim=-1).to(device)\n    \n    return grids\n\n# Create grids\nmulti_scale_grids = create_multi_scale_grids(device)\n\nprint(\"Multi-scale coordinate grids:\")\nfor size, grid in multi_scale_grids.items():\n    print(f\"  {size}x{size}: {grid.shape} ({size**2} pixels)\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "mamba-eval-cell-3",
   "metadata": {},
   "source": "### Multi-Scale Reconstruction\n\nSample the model at 32x32, 64x64, and 96x96 resolutions using the same sparse input (20% of 32x32)."
  },
  {
   "cell_type": "code",
   "id": "mamba-eval-cell-4",
   "metadata": {},
   "source": "@torch.no_grad()\ndef multi_scale_reconstruction(model, input_coords, input_values, grids, num_steps=100, device='cuda'):\n    \"\"\"\n    Reconstruct at multiple scales\n    \n    Args:\n        model: Trained model\n        input_coords: (B, N_in, 2) - sparse inputs\n        input_values: (B, N_in, 3) - sparse RGB values\n        grids: Dict of {size: coordinates}\n        num_steps: ODE solver steps\n    \n    Returns:\n        Dict of {size: reconstructed_images}\n    \"\"\"\n    model.eval()\n    B = input_coords.shape[0]\n    \n    reconstructions = {}\n    \n    for size, coords in grids.items():\n        print(f\"Reconstructing at {size}x{size}...\")\n        \n        # Expand coords for batch\n        coords_batch = coords.unsqueeze(0).expand(B, -1, -1)\n        \n        # Sample\n        pred_values = heun_sample(\n            model, coords_batch, input_coords, input_values,\n            num_steps=num_steps, device=device\n        )\n        \n        # Reshape to image\n        pred_images = pred_values.view(B, size, size, 3).permute(0, 3, 1, 2)\n        reconstructions[size] = pred_images\n    \n    return reconstructions\n\n# Test on a batch\ntest_batch = next(iter(test_loader))\nB_test = 4\n\nmulti_scale_results = multi_scale_reconstruction(\n    model,\n    test_batch['input_coords'][:B_test].to(device),\n    test_batch['input_values'][:B_test].to(device),\n    multi_scale_grids,\n    num_steps=100,\n    device=device\n)\n\nprint(\"\\nMulti-scale reconstruction complete!\")\nfor size, imgs in multi_scale_results.items():\n    print(f\"  {size}x{size}: {imgs.shape}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "mamba-eval-cell-5",
   "metadata": {},
   "source": "### Visualization: Scale-Invariant Reconstruction\n\nCompare the same image reconstructed at different resolutions."
  },
  {
   "cell_type": "code",
   "id": "mamba-eval-cell-6",
   "metadata": {},
   "source": "def visualize_multi_scale(ground_truth, sparse_input_img, multi_scale_results, sample_idx=0):\n    \"\"\"\n    Visualize multi-scale reconstructions\n    \n    Args:\n        ground_truth: (3, 32, 32) original image\n        sparse_input_img: (3, 32, 32) sparse input visualization\n        multi_scale_results: Dict {size: (B, 3, size, size)}\n        sample_idx: Which sample to visualize\n    \"\"\"\n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n    \n    # Row 1: Inputs and 32x32\n    axes[0, 0].imshow(ground_truth.permute(1, 2, 0).cpu().numpy())\n    axes[0, 0].set_title('Ground Truth (32x32)', fontsize=12, fontweight='bold')\n    axes[0, 0].axis('off')\n    \n    axes[0, 1].imshow(sparse_input_img.permute(1, 2, 0).cpu().numpy())\n    axes[0, 1].set_title('Sparse Input (20%)', fontsize=12, fontweight='bold')\n    axes[0, 1].axis('off')\n    \n    img_32 = multi_scale_results[32][sample_idx].permute(1, 2, 0).cpu().numpy()\n    axes[0, 2].imshow(np.clip(img_32, 0, 1))\n    axes[0, 2].set_title('Reconstructed 32x32\\n(Native Resolution)', fontsize=12, fontweight='bold')\n    axes[0, 2].axis('off')\n    \n    # Row 2: Upsampled versions\n    img_64 = multi_scale_results[64][sample_idx].permute(1, 2, 0).cpu().numpy()\n    axes[1, 0].imshow(np.clip(img_64, 0, 1))\n    axes[1, 0].set_title('Reconstructed 64x64\\n(2x Upsampling)', fontsize=12, fontweight='bold')\n    axes[1, 0].axis('off')\n    \n    img_96 = multi_scale_results[96][sample_idx].permute(1, 2, 0).cpu().numpy()\n    axes[1, 1].imshow(np.clip(img_96, 0, 1))\n    axes[1, 1].set_title('Reconstructed 96x96\\n(3x Upsampling)', fontsize=12, fontweight='bold')\n    axes[1, 1].axis('off')\n    \n    # Comparison: 32 vs 64 (zoomed detail)\n    # Upsample 32 to 64 with nearest neighbor for fair comparison\n    img_32_up = torch.nn.functional.interpolate(\n        multi_scale_results[32][sample_idx:sample_idx+1],\n        size=64, mode='nearest'\n    )[0].permute(1, 2, 0).cpu().numpy()\n    \n    axes[1, 2].imshow(np.clip(img_32_up, 0, 1))\n    axes[1, 2].set_title('32x32 Upsampled to 64x64\\n(Nearest Neighbor)', fontsize=12, fontweight='bold')\n    axes[1, 2].axis('off')\n    \n    plt.suptitle('Scale-Invariant Continuous Field Reconstruction (MAMBA)', fontsize=16, fontweight='bold', y=0.98)\n    plt.tight_layout()\n    \n    return fig\n\n# Visualize multiple samples\nfor i in range(min(B_test, 4)):\n    # Create sparse input visualization\n    sparse_img = torch.zeros(3, 32, 32)\n    input_idx = test_batch['input_indices'][i]\n    sparse_img.view(3, -1)[:, input_idx] = test_batch['input_values'][i].T\n    \n    fig = visualize_multi_scale(\n        test_batch['full_image'][i],\n        sparse_img,\n        multi_scale_results,\n        sample_idx=i\n    )\n    plt.savefig(f'mamba_multiscale_sample_{i}.png', dpi=150, bbox_inches='tight')\n    plt.show()\n    plt.close()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "mamba-eval-cell-7",
   "metadata": {},
   "source": "### Analysis: Scale Invariance Quality\n\nCompare reconstruction quality at different scales. Note that we can only compute metrics at 32x32 (where we have ground truth)."
  },
  {
   "cell_type": "code",
   "id": "mamba-eval-cell-8",
   "metadata": {},
   "source": "# Quantitative evaluation at native resolution (32x32)\nprint(\"=\"*60)\nprint(\"QUANTITATIVE EVALUATION: Full Field Reconstruction at 32x32\")\nprint(\"=\"*60)\n\nmodel.eval()\ntracker_full_field = MetricsTracker()\n\nwith torch.no_grad():\n    for i, batch in enumerate(tqdm(test_loader, desc=\"Full field evaluation\")):\n        if i >= 100:  # Evaluate on 100 batches\n            break\n        \n        B = batch['input_coords'].shape[0]\n        full_coords_batch = multi_scale_grids[32].unsqueeze(0).expand(B, -1, -1)\n        \n        pred_values = heun_sample(\n            model, full_coords_batch,\n            batch['input_coords'].to(device),\n            batch['input_values'].to(device),\n            num_steps=100, device=device\n        )\n        \n        pred_images = pred_values.view(B, 32, 32, 3).permute(0, 3, 1, 2)\n        tracker_full_field.update(None, None, pred_images, batch['full_image'].to(device))\n\nresults = tracker_full_field.compute()\nresults_std = tracker_full_field.compute_std()\n\nprint(f\"\\nFull Field Reconstruction (32x32):\")\nprint(f\"  PSNR: {results['psnr']:.2f} \u00b1 {results_std['psnr_std']:.2f} dB\")\nprint(f\"  SSIM: {results['ssim']:.4f} \u00b1 {results_std['ssim_std']:.4f}\")\nprint(f\"  MSE:  {results['mse']:.6f} \u00b1 {results_std['mse_std']:.6f}\")\nprint(f\"  MAE:  {results['mae']:.6f} \u00b1 {results_std['mae_std']:.6f}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "mamba-eval-cell-9",
   "metadata": {},
   "source": "### Qualitative Analysis: Scale Invariance\n\n**Key Observations to Look For:**\n\n1. **Sharpness at higher resolutions**: If the model learned continuous features, 64x64 and 96x96 should look sharper than simply upsampling 32x32\n2. **Artifact patterns**: New artifacts appearing at higher resolutions suggest overfitting to 32x32 grid\n3. **Feature coherence**: Colors, edges, and textures should remain consistent across scales\n4. **Detail emergence**: Higher resolutions should reveal finer details (if the model truly learned continuous representations)\n\n**What Success Looks Like:**\n- 64x64 and 96x96 look natural and smooth (not pixelated)\n- Better quality than nearest-neighbor upsampling of 32x32\n- No grid-aligned artifacts\n- Consistent colors and structures across scales\n\n**What Failure Looks Like:**\n- Grid artifacts visible at 64x64/96x96\n- Quality similar to or worse than upsampled 32x32\n- Distorted colors or structures at higher resolutions\n- Model \"confused\" by off-grid coordinates"
  },
  {
   "cell_type": "code",
   "id": "mamba-eval-cell-10",
   "metadata": {},
   "source": "# Side-by-side comparison: Native continuous reconstruction vs upsampled\ndef compare_upsampling_methods(multi_scale_results, sample_idx=0):\n    \"\"\"Compare continuous reconstruction vs traditional upsampling\"\"\"\n    \n    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n    \n    # Original 32x32\n    img_32 = multi_scale_results[32][sample_idx].permute(1, 2, 0).cpu().numpy()\n    axes[0, 0].imshow(np.clip(img_32, 0, 1))\n    axes[0, 0].set_title('32x32 Original', fontsize=14, fontweight='bold')\n    axes[0, 0].axis('off')\n    \n    # Traditional upsampling: Nearest Neighbor to 64x64\n    img_32_nn_64 = torch.nn.functional.interpolate(\n        multi_scale_results[32][sample_idx:sample_idx+1],\n        size=64, mode='nearest'\n    )[0].permute(1, 2, 0).cpu().numpy()\n    axes[0, 1].imshow(np.clip(img_32_nn_64, 0, 1))\n    axes[0, 1].set_title('64x64 Nearest Neighbor\\n(Traditional)', fontsize=14, fontweight='bold')\n    axes[0, 1].axis('off')\n    \n    # Traditional upsampling: Bilinear to 64x64\n    img_32_bi_64 = torch.nn.functional.interpolate(\n        multi_scale_results[32][sample_idx:sample_idx+1],\n        size=64, mode='bilinear', align_corners=False\n    )[0].permute(1, 2, 0).cpu().numpy()\n    axes[0, 2].imshow(np.clip(img_32_bi_64, 0, 1))\n    axes[0, 2].set_title('64x64 Bilinear\\n(Traditional)', fontsize=14, fontweight='bold')\n    axes[0, 2].axis('off')\n    \n    # Continuous reconstruction at 64x64\n    img_64_cont = multi_scale_results[64][sample_idx].permute(1, 2, 0).cpu().numpy()\n    axes[1, 0].imshow(np.clip(img_64_cont, 0, 1))\n    axes[1, 0].set_title('64x64 Continuous Field\\n(Our Method)', fontsize=14, fontweight='bold', color='green')\n    axes[1, 0].axis('off')\n    \n    # Continuous reconstruction at 96x96\n    img_96_cont = multi_scale_results[96][sample_idx].permute(1, 2, 0).cpu().numpy()\n    axes[1, 1].imshow(np.clip(img_96_cont, 0, 1))\n    axes[1, 1].set_title('96x96 Continuous Field\\n(Our Method)', fontsize=14, fontweight='bold', color='green')\n    axes[1, 1].axis('off')\n    \n    # Bilinear upsampling to 96x96 for comparison\n    img_32_bi_96 = torch.nn.functional.interpolate(\n        multi_scale_results[32][sample_idx:sample_idx+1],\n        size=96, mode='bilinear', align_corners=False\n    )[0].permute(1, 2, 0).cpu().numpy()\n    axes[1, 2].imshow(np.clip(img_32_bi_96, 0, 1))\n    axes[1, 2].set_title('96x96 Bilinear\\n(Traditional)', fontsize=14, fontweight='bold')\n    axes[1, 2].axis('off')\n    \n    plt.suptitle('Continuous Field Reconstruction vs Traditional Upsampling (MAMBA)',\n                 fontsize=16, fontweight='bold', y=0.98)\n    plt.tight_layout()\n    \n    return fig\n\nfor i in range(min(B_test, 2)):\n    fig = compare_upsampling_methods(multi_scale_results, sample_idx=i)\n    plt.savefig(f'mamba_upsampling_comparison_{i}.png', dpi=150, bbox_inches='tight')\n    plt.show()\n    plt.close()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"SCALE-INVARIANCE TEST COMPLETE\")\nprint(\"=\"*60)\nprint(\"\\nConclusion:\")\nprint(\"If the continuous field reconstructions look smoother and sharper than\")\nprint(\"traditional upsampling methods, the model has successfully learned\")\nprint(\"scale-invariant continuous representations via Fourier features!\")",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}