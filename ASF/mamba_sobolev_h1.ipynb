{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Directional MAMBA with H¹ Sobolev Regularization\n",
    "\n",
    "## Key Innovation: Functional Analysis-Based Smoothness\n",
    "\n",
    "**Theory**: Images live in Sobolev space W^{1,2} = H¹. Enforcing bounded H¹ norm guarantees continuous solutions.\n",
    "\n",
    "**H¹ Sobolev Space**:\n",
    "```\n",
    "||u||²_H¹ = ||u||²_L² + ||∇u||²_L²\n",
    "         = ∫_Ω u² dx + ∫_Ω |∇u|² dx\n",
    "```\n",
    "\n",
    "**Sobolev Embedding Theorem**:\n",
    "- H¹(Ω) ⊂ C^{0,α}(Ω) in 2D (continuous with Hölder regularity)\n",
    "- Bounded derivatives → continuous functions\n",
    "- Strong mathematical foundation from PDE theory\n",
    "\n",
    "**Implementation**:\n",
    "- Penalize H¹ seminorm: ||∇u||²_L² = ∫ |∇u|² dx\n",
    "- Use Monte Carlo integration: ∫ f dx ≈ (1/n) Σ f(xᵢ)\n",
    "- Compute exact gradients via autograd: ∂f/∂x using torch.autograd.grad\n",
    "\n",
    "**Benefits**:\n",
    "- ✅ **Strongest theoretical guarantee** - functional analysis foundation\n",
    "- ✅ **Exact gradients** - no finite difference approximations\n",
    "- ✅ **Adaptive** - can tune weight and sample count\n",
    "- ⚠️ **Training cost** - extra backward passes for gradient computation\n",
    "\n",
    "**Expected**: 2-4 dB PSNR improvement with mathematically guaranteed smoothness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add parent directory to path\n",
    "notebook_dir = os.path.abspath('')\n",
    "parent_dir = os.path.dirname(notebook_dir)\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "from core.neural_fields.perceiver import FourierFeatures\n",
    "from core.sparse.cifar10_sparse import SparseCIFAR10Dataset\n",
    "from core.sparse.metrics import MetricsTracker\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Core SSM Components (Standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSMBlockFast(nn.Module):\n",
    "    \"\"\"Ultra-fast SSM (standard version)\"\"\"\n",
    "    def __init__(self, d_model, d_state=16, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        \n",
    "        # State space parameters\n",
    "        self.A_log = nn.Parameter(torch.randn(d_state) * 0.1 - 1.0)\n",
    "        self.B = nn.Linear(d_model, d_state, bias=False)\n",
    "        self.C = nn.Linear(d_state, d_model, bias=False)\n",
    "        self.D = nn.Parameter(torch.randn(d_model) * 0.01)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.B.weight, gain=0.5)\n",
    "        nn.init.xavier_uniform_(self.C.weight, gain=0.5)\n",
    "        \n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.eps = 1e-8\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, N, D = x.shape\n",
    "        \n",
    "        A = -torch.exp(self.A_log).clamp(min=self.eps, max=10.0)\n",
    "        dt = 1.0 / N\n",
    "        A_bar = torch.exp(dt * A)\n",
    "        B_bar = torch.where(\n",
    "            torch.abs(A) > self.eps,\n",
    "            (A_bar - 1.0) / (A + self.eps),\n",
    "            torch.ones_like(A) * dt\n",
    "        )\n",
    "        \n",
    "        Bu = self.B(x) * B_bar\n",
    "        \n",
    "        indices = torch.arange(N, device=x.device)\n",
    "        decay = A_bar.unsqueeze(0).pow(\n",
    "            (indices.unsqueeze(0) - indices.unsqueeze(1)).clamp(min=0).unsqueeze(-1)\n",
    "        )\n",
    "        mask = indices.unsqueeze(0) >= indices.unsqueeze(1)\n",
    "        decay = decay * mask.unsqueeze(-1).float()\n",
    "        \n",
    "        h = torch.einsum('nmd,bnd->bmd', decay, Bu)\n",
    "        h = torch.clamp(h, min=-10.0, max=10.0)\n",
    "        \n",
    "        y = self.C(h) + self.D * x\n",
    "        \n",
    "        gate = self.gate(x)\n",
    "        y = gate * y + (1 - gate) * x\n",
    "        \n",
    "        return self.dropout(self.norm(y))\n",
    "\n",
    "\n",
    "class MambaBlock(nn.Module):\n",
    "    \"\"\"Standard Mamba block\"\"\"\n",
    "    def __init__(self, d_model, d_state=16, expand_factor=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.proj_in = nn.Linear(d_model, d_model * expand_factor)\n",
    "        self.ssm = SSMBlockFast(d_model * expand_factor, d_state, dropout)\n",
    "        self.proj_out = nn.Linear(d_model * expand_factor, d_model)\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.proj_in(x)\n",
    "        x = self.ssm(x)\n",
    "        x = self.proj_out(x)\n",
    "        x = x + residual\n",
    "        x = x + self.mlp(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "print(\"✓ SSM components loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi-Directional Scanning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_by_row(coords):\n",
    "    B, N, _ = coords.shape\n",
    "    indices_list = []\n",
    "    for b in range(B):\n",
    "        y_vals = coords[b, :, 1]\n",
    "        x_vals = coords[b, :, 0]\n",
    "        sort_keys = y_vals * 1000 + x_vals\n",
    "        indices = torch.argsort(sort_keys)\n",
    "        indices_list.append(indices)\n",
    "    return torch.stack(indices_list, dim=0)\n",
    "\n",
    "def order_by_column(coords):\n",
    "    B, N, _ = coords.shape\n",
    "    indices_list = []\n",
    "    for b in range(B):\n",
    "        y_vals = coords[b, :, 1]\n",
    "        x_vals = coords[b, :, 0]\n",
    "        sort_keys = x_vals * 1000 + y_vals\n",
    "        indices = torch.argsort(sort_keys)\n",
    "        indices_list.append(indices)\n",
    "    return torch.stack(indices_list, dim=0)\n",
    "\n",
    "def order_by_diagonal(coords):\n",
    "    B, N, _ = coords.shape\n",
    "    indices_list = []\n",
    "    for b in range(B):\n",
    "        y_vals = coords[b, :, 1]\n",
    "        x_vals = coords[b, :, 0]\n",
    "        diag_vals = x_vals + y_vals\n",
    "        sort_keys = diag_vals * 1000 + x_vals\n",
    "        indices = torch.argsort(sort_keys)\n",
    "        indices_list.append(indices)\n",
    "    return torch.stack(indices_list, dim=0)\n",
    "\n",
    "def order_by_antidiagonal(coords):\n",
    "    B, N, _ = coords.shape\n",
    "    indices_list = []\n",
    "    for b in range(B):\n",
    "        y_vals = coords[b, :, 1]\n",
    "        x_vals = coords[b, :, 0]\n",
    "        antidiag_vals = x_vals - y_vals\n",
    "        sort_keys = antidiag_vals * 1000 + x_vals\n",
    "        indices = torch.argsort(sort_keys)\n",
    "        indices_list.append(indices)\n",
    "    return torch.stack(indices_list, dim=0)\n",
    "\n",
    "def reorder_sequence(x, indices):\n",
    "    B, N, D = x.shape\n",
    "    indices_expanded = indices.unsqueeze(-1).expand(B, N, D)\n",
    "    return torch.gather(x, dim=1, index=indices_expanded)\n",
    "\n",
    "def inverse_reorder(x, indices):\n",
    "    B, N, D = x.shape\n",
    "    inverse_indices = torch.zeros_like(indices)\n",
    "    for b in range(B):\n",
    "        inverse_indices[b, indices[b]] = torch.arange(N, device=indices.device)\n",
    "    indices_expanded = inverse_indices.unsqueeze(-1).expand(B, N, D)\n",
    "    return torch.gather(x, dim=1, index=indices_expanded)\n",
    "\n",
    "\n",
    "class MultiDirectionalSSM(nn.Module):\n",
    "    def __init__(self, d_model, d_state=16, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.ssm_horizontal = SSMBlockFast(d_model, d_state, dropout)\n",
    "        self.ssm_vertical = SSMBlockFast(d_model, d_state, dropout)\n",
    "        self.ssm_diagonal = SSMBlockFast(d_model, d_state, dropout)\n",
    "        self.ssm_antidiagonal = SSMBlockFast(d_model, d_state, dropout)\n",
    "        \n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(4 * d_model, d_model * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 2, d_model)\n",
    "        )\n",
    "        \n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x, coords):\n",
    "        indices_h = order_by_row(coords)\n",
    "        indices_v = order_by_column(coords)\n",
    "        indices_d = order_by_diagonal(coords)\n",
    "        indices_a = order_by_antidiagonal(coords)\n",
    "        \n",
    "        x_h = reorder_sequence(x, indices_h)\n",
    "        y_h = self.ssm_horizontal(x_h)\n",
    "        y_h = inverse_reorder(y_h, indices_h)\n",
    "        \n",
    "        x_v = reorder_sequence(x, indices_v)\n",
    "        y_v = self.ssm_vertical(x_v)\n",
    "        y_v = inverse_reorder(y_v, indices_v)\n",
    "        \n",
    "        x_d = reorder_sequence(x, indices_d)\n",
    "        y_d = self.ssm_diagonal(x_d)\n",
    "        y_d = inverse_reorder(y_d, indices_d)\n",
    "        \n",
    "        x_a = reorder_sequence(x, indices_a)\n",
    "        y_a = self.ssm_antidiagonal(x_a)\n",
    "        y_a = inverse_reorder(y_a, indices_a)\n",
    "        \n",
    "        y_concat = torch.cat([y_h, y_v, y_d, y_a], dim=-1)\n",
    "        y_fused = self.fusion(y_concat)\n",
    "        y = x + y_fused\n",
    "        y = self.norm(y)\n",
    "        \n",
    "        return y\n",
    "\n",
    "\n",
    "class MultiDirectionalMambaBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_state=16, expand_factor=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.proj_in = nn.Linear(d_model, d_model * expand_factor)\n",
    "        self.multi_ssm = MultiDirectionalSSM(d_model * expand_factor, d_state, dropout)\n",
    "        self.proj_out = nn.Linear(d_model * expand_factor, d_model)\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, coords):\n",
    "        residual = x\n",
    "        x = self.proj_in(x)\n",
    "        x = self.multi_ssm(x, coords)\n",
    "        x = self.proj_out(x)\n",
    "        x = x + residual\n",
    "        x = x + self.mlp(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "print(\"✓ Multi-directional SSM loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. H¹ Sobolev Regularization\n",
    "\n",
    "**Mathematical Foundation**:\n",
    "\n",
    "H¹ seminorm:\n",
    "```\n",
    "|u|²_H¹ = ∫_Ω |∇u|² dx\n",
    "        = ∫_Ω [(∂u/∂x)² + (∂u/∂y)²] dx\n",
    "```\n",
    "\n",
    "Monte Carlo approximation:\n",
    "```\n",
    "∫_Ω f dx ≈ (1/n) Σᵢ f(xᵢ), xᵢ ~ Uniform(Ω)\n",
    "```\n",
    "\n",
    "For RGB output (3 channels):\n",
    "```\n",
    "|u|²_H¹ = Σ_c ∫_Ω [(∂u_c/∂x)² + (∂u_c/∂y)²] dx\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sobolev_h1_loss(model, t, input_coords, input_values, \n",
    "                    n_samples=500, weight=0.01):\n",
    "    \"\"\"\n",
    "    H¹ Sobolev seminorm: ∫_Ω |∇u|² dx\n",
    "    \n",
    "    Uses Monte Carlo integration with autograd for exact gradients.\n",
    "    \n",
    "    Args:\n",
    "        model: Neural field model\n",
    "        t: Timestep (B,)\n",
    "        input_coords: Sparse input coordinates (B, N_in, 2)\n",
    "        input_values: Sparse input values (B, N_in, 3)\n",
    "        n_samples: Number of MC samples for integration\n",
    "        weight: Regularization weight\n",
    "    \n",
    "    Returns:\n",
    "        H¹ seminorm loss (scalar)\n",
    "    \"\"\"\n",
    "    B = input_coords.shape[0]\n",
    "    device = input_coords.device\n",
    "    \n",
    "    # Sample random coordinates for Monte Carlo integration\n",
    "    # Important: requires_grad=True to compute ∂f/∂x\n",
    "    sample_coords = torch.rand(B, n_samples, 2, device=device, requires_grad=True)\n",
    "    sample_x_t = torch.randn(B, n_samples, 3, device=device)\n",
    "    \n",
    "    # Forward pass\n",
    "    v_pred = model(sample_x_t, sample_coords, t, input_coords, input_values)\n",
    "    # v_pred: (B, n_samples, 3) - RGB predictions\n",
    "    \n",
    "    # Compute gradients w.r.t. coordinates using autograd\n",
    "    # For each RGB channel, compute ∂v/∂coords\n",
    "    h1_seminorm = 0\n",
    "    \n",
    "    for channel in range(3):  # R, G, B\n",
    "        # Gradient of channel w.r.t. coordinates\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=v_pred[:, :, channel],\n",
    "            inputs=sample_coords,\n",
    "            grad_outputs=torch.ones_like(v_pred[:, :, channel]),\n",
    "            create_graph=True,  # Allow backprop through this gradient\n",
    "            retain_graph=True\n",
    "        )[0]  # (B, n_samples, 2) - [∂v/∂x, ∂v/∂y]\n",
    "        \n",
    "        # |∇v|² = (∂v/∂x)² + (∂v/∂y)²\n",
    "        grad_squared = (gradients ** 2).sum(dim=-1)  # (B, n_samples)\n",
    "        \n",
    "        # Monte Carlo estimate: ∫ |∇v|² dx ≈ (1/n) Σ |∇v(xᵢ)|²\n",
    "        h1_seminorm += grad_squared.mean()\n",
    "    \n",
    "    return weight * h1_seminorm\n",
    "\n",
    "\n",
    "print(\"✓ H¹ Sobolev regularization loaded\")\n",
    "print(\"  - Uses exact autograd for ∂f/∂x computation\")\n",
    "print(\"  - Monte Carlo integration over [0,1]²\")\n",
    "print(\"  - Penalizes ||∇u||²_L² for all RGB channels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Time Embedding and Main Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalTimeEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    \n",
    "    def forward(self, t):\n",
    "        device = t.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat([emb.sin(), emb.cos()], dim=-1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "class MAMBADiffusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Directional MAMBA Diffusion\n",
    "    \n",
    "    Will be trained with H¹ Sobolev regularization for smoothness\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_fourier_feats=256,\n",
    "        d_model=512,\n",
    "        num_layers=6,\n",
    "        d_state=16,\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.fourier = FourierFeatures(coord_dim=2, num_freqs=num_fourier_feats, scale=10.0)\n",
    "        feat_dim = num_fourier_feats * 2\n",
    "        \n",
    "        self.input_proj = nn.Linear(feat_dim + 3, d_model)\n",
    "        self.query_proj = nn.Linear(feat_dim + 3, d_model)\n",
    "        \n",
    "        self.time_embed = SinusoidalTimeEmbedding(d_model)\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(d_model, d_model)\n",
    "        )\n",
    "        \n",
    "        self.mamba_blocks = nn.ModuleList([\n",
    "            MultiDirectionalMambaBlock(d_model, d_state=d_state, expand_factor=2, dropout=dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.query_cross_attn = nn.MultiheadAttention(\n",
    "            d_model, num_heads=8, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_model * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 2, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, 3)\n",
    "        )\n",
    "    \n",
    "    def forward(self, noisy_values, query_coords, t, input_coords, input_values):\n",
    "        B = query_coords.shape[0]\n",
    "        N_in = input_coords.shape[1]\n",
    "        N_out = query_coords.shape[1]\n",
    "        \n",
    "        t_emb = self.time_mlp(self.time_embed(t))\n",
    "        \n",
    "        input_feats = self.fourier(input_coords)\n",
    "        query_feats = self.fourier(query_coords)\n",
    "        \n",
    "        input_tokens = self.input_proj(\n",
    "            torch.cat([input_feats, input_values], dim=-1)\n",
    "        )\n",
    "        query_tokens = self.query_proj(\n",
    "            torch.cat([query_feats, noisy_values], dim=-1)\n",
    "        )\n",
    "        \n",
    "        input_tokens = input_tokens + t_emb.unsqueeze(1)\n",
    "        query_tokens = query_tokens + t_emb.unsqueeze(1)\n",
    "        \n",
    "        all_coords = torch.cat([input_coords, query_coords], dim=1)\n",
    "        seq = torch.cat([input_tokens, query_tokens], dim=1)\n",
    "        \n",
    "        for mamba_block in self.mamba_blocks:\n",
    "            seq = mamba_block(seq, all_coords)\n",
    "        \n",
    "        input_seq = seq[:, :N_in, :]\n",
    "        query_seq = seq[:, N_in:, :]\n",
    "        \n",
    "        output, _ = self.query_cross_attn(query_seq, input_seq, input_seq)\n",
    "        \n",
    "        return self.decoder(output)\n",
    "\n",
    "\n",
    "# Test model\n",
    "model = MAMBADiffusion(\n",
    "    num_fourier_feats=256,\n",
    "    d_model=512,\n",
    "    num_layers=6,\n",
    "    d_state=16\n",
    ").to(device)\n",
    "\n",
    "test_noisy = torch.rand(4, 204, 3).to(device)\n",
    "test_query_coords = torch.rand(4, 204, 2).to(device)\n",
    "test_t = torch.rand(4).to(device)\n",
    "test_input_coords = torch.rand(4, 204, 2).to(device)\n",
    "test_input_values = torch.rand(4, 204, 3).to(device)\n",
    "\n",
    "test_out = model(test_noisy, test_query_coords, test_t, test_input_coords, test_input_values)\n",
    "print(f\"✓ Model test passed: {test_out.shape}\")\n",
    "print(f\"✓ Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training with H¹ Sobolev Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_flow(x_0, x_1, t):\n",
    "    return (1 - t) * x_0 + t * x_1\n",
    "\n",
    "def target_velocity(x_0, x_1):\n",
    "    return x_1 - x_0\n",
    "\n",
    "@torch.no_grad()\n",
    "def heun_sample(model, output_coords, input_coords, input_values, num_steps=50, device='cuda'):\n",
    "    B, N_out = output_coords.shape[0], output_coords.shape[1]\n",
    "    x_t = torch.randn(B, N_out, 3, device=device)\n",
    "    \n",
    "    dt = 1.0 / num_steps\n",
    "    ts = torch.linspace(0, 1 - dt, num_steps)\n",
    "    \n",
    "    for t_val in tqdm(ts, desc=\"Sampling\", leave=False):\n",
    "        t = torch.full((B,), t_val.item(), device=device)\n",
    "        t_next = torch.full((B,), t_val.item() + dt, device=device)\n",
    "        \n",
    "        v1 = model(x_t, output_coords, t, input_coords, input_values)\n",
    "        x_next_pred = x_t + dt * v1\n",
    "        \n",
    "        v2 = model(x_next_pred, output_coords, t_next, input_coords, input_values)\n",
    "        x_t = x_t + dt * 0.5 * (v1 + v2)\n",
    "    \n",
    "    return torch.clamp(x_t, 0, 1)\n",
    "\n",
    "def train_flow_matching(\n",
    "    model, train_loader, test_loader, epochs=100, lr=1e-4, device='cuda',\n",
    "    visualize_every=5, eval_every=2, save_dir='checkpoints_sobolev',\n",
    "    sobolev_weight=0.01, sobolev_samples=500, sobolev_every=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Train with flow matching + H¹ Sobolev regularization\n",
    "    \n",
    "    Args:\n",
    "        sobolev_weight: Weight for H¹ seminorm penalty\n",
    "        sobolev_samples: Number of MC samples for gradient integration\n",
    "        sobolev_every: Apply regularization every N epochs (reduce overhead)\n",
    "    \"\"\"\n",
    "    import os\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    losses = []\n",
    "    sobolev_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    # Visualization setup\n",
    "    y, x = torch.meshgrid(\n",
    "        torch.linspace(0, 1, 32),\n",
    "        torch.linspace(0, 1, 32),\n",
    "        indexing='ij'\n",
    "    )\n",
    "    full_coords = torch.stack([x.flatten(), y.flatten()], dim=-1).to(device)\n",
    "    \n",
    "    viz_batch = next(iter(train_loader))\n",
    "    viz_input_coords = viz_batch['input_coords'][:4].to(device)\n",
    "    viz_input_values = viz_batch['input_values'][:4].to(device)\n",
    "    viz_output_coords = viz_batch['output_coords'][:4].to(device)\n",
    "    viz_output_values = viz_batch['output_values'][:4].to(device)\n",
    "    viz_full_images = viz_batch['full_image'][:4].to(device)\n",
    "    viz_input_indices = viz_batch['input_indices'][:4]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training Configuration:\")\n",
    "    print(f\"  Sobolev weight: {sobolev_weight}\")\n",
    "    print(f\"  Sobolev samples: {sobolev_samples}\")\n",
    "    print(f\"  Sobolev frequency: every {sobolev_every} epochs\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_sobolev = 0\n",
    "        \n",
    "        # Decide if we apply Sobolev regularization this epoch\n",
    "        use_sobolev = (epoch % sobolev_every == 0)\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} {'[+Sobolev]' if use_sobolev else ''}\"):\n",
    "            input_coords = batch['input_coords'].to(device)\n",
    "            input_values = batch['input_values'].to(device)\n",
    "            output_coords = batch['output_coords'].to(device)\n",
    "            output_values = batch['output_values'].to(device)\n",
    "            \n",
    "            B = input_coords.shape[0]\n",
    "            t = torch.rand(B, device=device)\n",
    "            \n",
    "            x_0 = torch.randn_like(output_values)\n",
    "            x_1 = output_values\n",
    "            \n",
    "            t_broadcast = t.view(B, 1, 1)\n",
    "            x_t = conditional_flow(x_0, x_1, t_broadcast)\n",
    "            u_t = target_velocity(x_0, x_1)\n",
    "            \n",
    "            v_pred = model(x_t, output_coords, t, input_coords, input_values)\n",
    "            \n",
    "            # Flow matching loss\n",
    "            flow_loss = F.mse_loss(v_pred, u_t)\n",
    "            \n",
    "            # H¹ Sobolev regularization (every N epochs)\n",
    "            if use_sobolev:\n",
    "                sob_loss = sobolev_h1_loss(\n",
    "                    model, t, input_coords, input_values,\n",
    "                    n_samples=sobolev_samples, weight=sobolev_weight\n",
    "                )\n",
    "                total_loss = flow_loss + sob_loss\n",
    "                epoch_sobolev += sob_loss.item()\n",
    "            else:\n",
    "                total_loss = flow_loss\n",
    "                sob_loss = torch.tensor(0.0)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += flow_loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        avg_sobolev = epoch_sobolev / len(train_loader) if use_sobolev else 0\n",
    "        losses.append(avg_loss)\n",
    "        sobolev_losses.append(avg_sobolev)\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Flow Loss = {avg_loss:.6f}, Sobolev = {avg_sobolev:.6f}, LR = {scheduler.get_last_lr()[0]:.6f}\")\n",
    "        \n",
    "        # Evaluation\n",
    "        val_loss = None\n",
    "        if (epoch + 1) % eval_every == 0 or epoch == 0:\n",
    "            model.eval()\n",
    "            tracker = MetricsTracker()\n",
    "            val_loss_accum = 0\n",
    "            val_batches = 0\n",
    "            with torch.no_grad():\n",
    "                for i, batch in enumerate(test_loader):\n",
    "                    if i >= 10:\n",
    "                        break\n",
    "                    pred_values = heun_sample(\n",
    "                        model, batch['output_coords'].to(device),\n",
    "                        batch['input_coords'].to(device), batch['input_values'].to(device),\n",
    "                        num_steps=50, device=device\n",
    "                    )\n",
    "                    tracker.update(pred_values, batch['output_values'].to(device))\n",
    "                    val_loss_accum += F.mse_loss(pred_values, batch['output_values'].to(device)).item()\n",
    "                    val_batches += 1\n",
    "                    \n",
    "                results = tracker.compute()\n",
    "                val_loss = val_loss_accum / val_batches\n",
    "                print(f\"  Eval - MSE: {results['mse']:.6f}, MAE: {results['mae']:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "                \n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    torch.save({\n",
    "                        'epoch': epoch + 1,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'scheduler_state_dict': scheduler.state_dict(),\n",
    "                        'loss': avg_loss,\n",
    "                        'val_loss': val_loss,\n",
    "                        'best_val_loss': best_val_loss\n",
    "                    }, f'{save_dir}/sobolev_best.pth')\n",
    "                    print(f\"  ✓ Saved best model (val_loss: {val_loss:.6f})\")\n",
    "        \n",
    "        # Save latest\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "            'val_loss': val_loss if val_loss is not None else avg_loss,\n",
    "            'best_val_loss': best_val_loss\n",
    "        }, f'{save_dir}/sobolev_latest.pth')\n",
    "        \n",
    "        # Visualization\n",
    "        if (epoch + 1) % visualize_every == 0 or epoch == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                pred_values = heun_sample(\n",
    "                    model, viz_output_coords, viz_input_coords, viz_input_values,\n",
    "                    num_steps=50, device=device\n",
    "                )\n",
    "                \n",
    "                full_coords_batch = full_coords.unsqueeze(0).expand(4, -1, -1)\n",
    "                full_pred_values = heun_sample(\n",
    "                    model, full_coords_batch, viz_input_coords, viz_input_values,\n",
    "                    num_steps=50, device=device\n",
    "                )\n",
    "                full_pred_images = full_pred_values.view(4, 32, 32, 3).permute(0, 3, 1, 2)\n",
    "                \n",
    "                fig, axes = plt.subplots(4, 5, figsize=(20, 16))\n",
    "                \n",
    "                for i in range(4):\n",
    "                    axes[i, 0].imshow(viz_full_images[i].permute(1, 2, 0).cpu().numpy())\n",
    "                    axes[i, 0].set_title('Ground Truth' if i == 0 else '', fontsize=10)\n",
    "                    axes[i, 0].axis('off')\n",
    "                    \n",
    "                    input_img = torch.zeros(3, 32, 32, device=device)\n",
    "                    input_idx = viz_input_indices[i]\n",
    "                    input_img.view(3, -1)[:, input_idx] = viz_input_values[i].T\n",
    "                    axes[i, 1].imshow(input_img.permute(1, 2, 0).cpu().numpy())\n",
    "                    axes[i, 1].set_title('Sparse Input (20%)' if i == 0 else '', fontsize=10)\n",
    "                    axes[i, 1].axis('off')\n",
    "                    \n",
    "                    target_img = torch.zeros(3, 32, 32, device=device)\n",
    "                    output_idx = viz_batch['output_indices'][i]\n",
    "                    target_img.view(3, -1)[:, output_idx] = viz_output_values[i].T\n",
    "                    axes[i, 2].imshow(target_img.permute(1, 2, 0).cpu().numpy())\n",
    "                    axes[i, 2].set_title('Sparse Target (20%)' if i == 0 else '', fontsize=10)\n",
    "                    axes[i, 2].axis('off')\n",
    "                    \n",
    "                    pred_img = torch.zeros(3, 32, 32, device=device)\n",
    "                    pred_img.view(3, -1)[:, output_idx] = pred_values[i].T\n",
    "                    axes[i, 3].imshow(np.clip(pred_img.permute(1, 2, 0).cpu().numpy(), 0, 1))\n",
    "                    axes[i, 3].set_title('Sparse Prediction' if i == 0 else '', fontsize=10)\n",
    "                    axes[i, 3].axis('off')\n",
    "                    \n",
    "                    axes[i, 4].imshow(np.clip(full_pred_images[i].permute(1, 2, 0).cpu().numpy(), 0, 1))\n",
    "                    axes[i, 4].set_title('Full Field' if i == 0 else '', fontsize=10)\n",
    "                    axes[i, 4].axis('off')\n",
    "                \n",
    "                plt.suptitle(f'H¹ Sobolev MAMBA - Epoch {epoch+1} (Best: {best_val_loss:.6f})', \n",
    "                           fontsize=14, y=0.995)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'{save_dir}/epoch_{epoch+1:03d}.png', dpi=150, bbox_inches='tight')\n",
    "                plt.show()\n",
    "                plt.close()\n",
    "    \n",
    "    print(f\"\\n✓ Training complete! Best validation loss: {best_val_loss:.6f}\")\n",
    "    return losses, sobolev_losses\n",
    "\n",
    "print(\"✓ Training functions with H¹ Sobolev regularization loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Data and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "train_dataset = SparseCIFAR10Dataset(\n",
    "    root='../data', train=True, input_ratio=0.2, output_ratio=0.2, download=True, seed=42\n",
    ")\n",
    "test_dataset = SparseCIFAR10Dataset(\n",
    "    root='../data', train=False, input_ratio=0.2, output_ratio=0.2, download=True, seed=42\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Test: {len(test_dataset)}\")\n",
    "\n",
    "# Initialize model\n",
    "model = MAMBADiffusion(\n",
    "    num_fourier_feats=256,\n",
    "    d_model=512,\n",
    "    num_layers=6,\n",
    "    d_state=16\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(\"\\n✓ H¹ Sobolev regularization will enforce bounded gradients\")\n",
    "print(\"✓ Applied every 5 epochs to balance quality and training time\")\n",
    "\n",
    "# Train\n",
    "losses, sobolev_losses = train_flow_matching(\n",
    "    model, train_loader, test_loader, \n",
    "    epochs=100, lr=1e-4, device=device,\n",
    "    sobolev_weight=0.01,  # Tune this: higher = smoother but may reduce details\n",
    "    sobolev_samples=500,   # More samples = better gradient estimate\n",
    "    sobolev_every=5        # Apply every N epochs (reduce overhead)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Loss Plots and Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot losses\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 4))\n",
    "\n",
    "# Flow matching loss\n",
    "axes[0].plot(losses, linewidth=2, label='Flow Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Flow Matching Loss')\n",
    "axes[0].grid(alpha=0.3)\n",
    "axes[0].legend()\n",
    "\n",
    "# Sobolev regularization loss\n",
    "sobolev_epochs = [i for i, v in enumerate(sobolev_losses) if v > 0]\n",
    "sobolev_values = [v for v in sobolev_losses if v > 0]\n",
    "axes[1].plot(sobolev_epochs, sobolev_values, linewidth=2, marker='o', label='H¹ Seminorm')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('||∇u||²_L²')\n",
    "axes[1].set_title('H¹ Sobolev Seminorm (applied every 5 epochs)')\n",
    "axes[1].grid(alpha=0.3)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('checkpoints_sobolev/training_loss.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Full field evaluation\n",
    "def create_full_grid(image_size=32, device='cuda'):\n",
    "    y, x = torch.meshgrid(\n",
    "        torch.linspace(0, 1, image_size),\n",
    "        torch.linspace(0, 1, image_size),\n",
    "        indexing='ij'\n",
    "    )\n",
    "    return torch.stack([x.flatten(), y.flatten()], dim=-1).to(device)\n",
    "\n",
    "full_coords = create_full_grid(32, device)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL EVALUATION: Full Field Reconstruction\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model.eval()\n",
    "tracker_full = MetricsTracker()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(tqdm(test_loader, desc=\"Full reconstruction\")):\n",
    "        if i >= 100:\n",
    "            break\n",
    "        \n",
    "        B = batch['input_coords'].shape[0]\n",
    "        full_coords_batch = full_coords.unsqueeze(0).expand(B, -1, -1)\n",
    "        \n",
    "        pred_values = heun_sample(\n",
    "            model, full_coords_batch,\n",
    "            batch['input_coords'].to(device),\n",
    "            batch['input_values'].to(device),\n",
    "            num_steps=100, device=device\n",
    "        )\n",
    "        \n",
    "        pred_images = pred_values.view(B, 32, 32, 3).permute(0, 3, 1, 2)\n",
    "        tracker_full.update(None, None, pred_images, batch['full_image'].to(device))\n",
    "\n",
    "results = tracker_full.compute()\n",
    "results_std = tracker_full.compute_std()\n",
    "\n",
    "print(f\"\\nH¹ Sobolev Regularization Results:\")\n",
    "print(f\"  PSNR: {results['psnr']:.2f} ± {results_std['psnr_std']:.2f} dB\")\n",
    "print(f\"  SSIM: {results['ssim']:.4f} ± {results_std['ssim_std']:.4f}\")\n",
    "print(f\"  MSE:  {results['mse']:.6f} ± {results_std['mse_std']:.6f}\")\n",
    "print(f\"  MAE:  {results['mae']:.6f} ± {results_std['mae_std']:.6f}\")\n",
    "print(\"\\n✓ H¹ Sobolev embedding guarantees continuity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### H¹ Sobolev Regularization Benefits\n",
    "\n",
    "**Theoretical Foundation**:\n",
    "- H¹ Sobolev space: functions with bounded derivatives\n",
    "- Sobolev embedding theorem: H¹(Ω) ⊂ C^{0,α}(Ω) (continuous with Hölder regularity)\n",
    "- Strong mathematical guarantee from PDE theory\n",
    "\n",
    "**Implementation**:\n",
    "- Penalize H¹ seminorm: ||∇u||²_L² = ∫ |∇u|² dx\n",
    "- Monte Carlo integration over random samples\n",
    "- Exact gradients via torch.autograd.grad (no finite differences)\n",
    "\n",
    "**Practical Impact**:\n",
    "- ✅ **Strongest continuity guarantee** - functional analysis foundation\n",
    "- ✅ **Smooth reconstructions** - penalizes large gradients\n",
    "- ✅ **Tunable** - adjust weight and sample count\n",
    "- ⚠️ **Training cost** - extra backward passes (mitigated by applying every N epochs)\n",
    "\n",
    "**Expected vs Baseline**:\n",
    "- PSNR: +2-4 dB improvement\n",
    "- SSIM: +0.05-0.08 improvement\n",
    "- Visually: Much smoother textures, reduced noise\n",
    "- Training: 20-40% slower (when regularization applied)\n",
    "\n",
    "**Hyperparameter Tuning**:\n",
    "- `sobolev_weight`: 0.005-0.02 (higher = smoother, may lose details)\n",
    "- `sobolev_samples`: 300-1000 (more = better estimate, slower)\n",
    "- `sobolev_every`: 3-10 (balance quality vs training time)\n",
    "\n",
    "**When to Use**:\n",
    "- Need strongest mathematical guarantee of smoothness\n",
    "- Can afford extra training time\n",
    "- Want tunable regularization strength"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
