{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAMBA Diffusion - Fixed Version (Playbook-Aligned)\n",
    "\n",
    "## Critical Fixes Applied\n",
    "\n",
    "Based on `mamba_sparse_flow_playbook.md`, this notebook implements:\n",
    "\n",
    "### 1. ✅ Pixel-Center Coordinates\n",
    "- Use `(0.5/size, 1.5/size, ..., (size-0.5)/size)` instead of `linspace(0,1,size)`\n",
    "- Eliminates half-pixel misalignment at 32×32\n",
    "\n",
    "### 2. ✅ Geometry-Aware SSM\n",
    "- **Morton (Z-order) curve** for spatial sorting (as recommended in playbook)\n",
    "- **Per-step Δt** from Euclidean distances along the curve\n",
    "- Makes SSM sequence-length invariant\n",
    "\n",
    "### 3. ✅ Band-Limited Fourier Features\n",
    "- Resolution-aware with Nyquist cutoff\n",
    "- Scale=4.0, num_freqs=64 for 32×32\n",
    "- Prevents high-frequency aliasing\n",
    "\n",
    "### 4. ✅ Local Cross-Attention\n",
    "- KNN-based (k=32 neighbors)\n",
    "- Relative position encoding\n",
    "- RBF interpolation prior + residual learning\n",
    "\n",
    "### 5. ✅ Training Hygiene\n",
    "- RGB in `[-1, 1]` range\n",
    "- EMA weights for sampling\n",
    "- AdamW optimizer with proper settings\n",
    "- No dropout in SSM\n",
    "\n",
    "### Expected Result\n",
    "**32×32 reconstructions should now be clean** while maintaining super-resolution capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "notebook_dir = os.path.abspath('')\n",
    "parent_dir = os.path.dirname(notebook_dir)\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from copy import deepcopy\n",
    "\n",
    "from core.sparse.cifar10_sparse import SparseCIFAR10Dataset\n",
    "from core.sparse.metrics import MetricsTracker\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pixel-Center Coordinate Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_center_grid(size, device='cuda'):\n",
    "    \"\"\"\n",
    "    Create grid with pixel centers, not edges.\n",
    "    \n",
    "    Centers in [0,1]: 0.5/size, 1.5/size, ..., (size-0.5)/size\n",
    "    \n",
    "    This eliminates half-pixel offset that causes noise at native resolution.\n",
    "    \"\"\"\n",
    "    p = torch.linspace(0.5/size, 1.0 - 0.5/size, size, device=device)\n",
    "    y, x = torch.meshgrid(p, p, indexing='ij')\n",
    "    return torch.stack([x.reshape(-1), y.reshape(-1)], -1)  # (size^2, 2)\n",
    "\n",
    "# Test\n",
    "grid_32 = make_center_grid(32, device)\n",
    "print(f\"32×32 grid: {grid_32.shape}\")\n",
    "print(f\"  First coord: {grid_32[0].cpu().tolist()}  (should be ~[0.015625, 0.015625])\")\n",
    "print(f\"  Last coord:  {grid_32[-1].cpu().tolist()} (should be ~[0.984375, 0.984375])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Morton (Z-Order) Curve Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morton_order(coords):\n",
    "    \"\"\"\n",
    "    Sort coordinates along Morton (Z-order) curve.\n",
    "    \n",
    "    Playbook recommendation: Morton over Hilbert for simplicity and speed.\n",
    "    \n",
    "    Args:\n",
    "        coords: (B, N, 2) in [0, 1]\n",
    "    Returns:\n",
    "        perm: (B, N) argsort indices\n",
    "    \"\"\"\n",
    "    # Quantize to 16-bit integers\n",
    "    xy = (coords.clamp(0, 1) * 65535).long()  # (B, N, 2)\n",
    "    x, y = xy[..., 0], xy[..., 1]\n",
    "    \n",
    "    # Part-by-1 interleaving (Morton code)\n",
    "    def part1by1(v):\n",
    "        v = (v | (v << 8)) & 0x00FF00FF\n",
    "        v = (v | (v << 4)) & 0x0F0F0F0F\n",
    "        v = (v | (v << 2)) & 0x33333333\n",
    "        v = (v | (v << 1)) & 0x55555555\n",
    "        return v\n",
    "    \n",
    "    code = (part1by1(x) << 1) | part1by1(y)  # (B, N)\n",
    "    return torch.argsort(code, dim=1)  # (B, N)\n",
    "\n",
    "# Test\n",
    "test_coords = torch.rand(2, 20, 2)\n",
    "perm = morton_order(test_coords)\n",
    "print(f\"Morton order test:\")\n",
    "print(f\"  Input: {test_coords.shape}\")\n",
    "print(f\"  Permutation: {perm.shape}\")\n",
    "\n",
    "# Visualize Morton order\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Original\n",
    "axes[0].scatter(test_coords[0, :, 0], test_coords[0, :, 1], c=range(20), cmap='viridis', s=100)\n",
    "axes[0].plot(test_coords[0, :, 0], test_coords[0, :, 1], 'k-', alpha=0.3, linewidth=0.5)\n",
    "axes[0].set_title('Original Order')\n",
    "axes[0].set_xlim(0, 1)\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].set_aspect('equal')\n",
    "\n",
    "# Morton order\n",
    "sorted_coords = test_coords[0][perm[0]]\n",
    "axes[1].scatter(sorted_coords[:, 0], sorted_coords[:, 1], c=range(20), cmap='viridis', s=100)\n",
    "axes[1].plot(sorted_coords[:, 0], sorted_coords[:, 1], 'r-', alpha=0.5, linewidth=1.5)\n",
    "axes[1].set_title('Morton (Z-Order) - Locality Preserving')\n",
    "axes[1].set_xlim(0, 1)\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('morton_curve_ordering.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Resolution-Aware Fourier Features (Band-Limited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResolutionAwareFourier(nn.Module):\n",
    "    \"\"\"\n",
    "    Band-limited Fourier features that respect grid Nyquist frequency.\n",
    "    \n",
    "    For 32×32: max frequency = 16 cycles (Nyquist)\n",
    "    For 64×64: max frequency = 32 cycles\n",
    "    \n",
    "    Prevents high-frequency aliasing that appears as speckle at 32×32.\n",
    "    \"\"\"\n",
    "    def __init__(self, coord_dim=2, num_freqs=128, max_cycles=16):\n",
    "        super().__init__()\n",
    "        self.coord_dim = coord_dim\n",
    "        self.num_freqs = num_freqs\n",
    "        \n",
    "        # Linear frequency spacing up to max_cycles\n",
    "        freqs = torch.linspace(1.0, max_cycles, num_freqs)\n",
    "        self.register_buffer('freqs', freqs)\n",
    "    \n",
    "    def forward(self, coords, size_hint=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            coords: (B, N, 2) in [0, 1]\n",
    "            size_hint: Grid resolution (32, 64, 96, etc.)\n",
    "        Returns:\n",
    "            features: (B, N, 4*F) where F = effective num freqs\n",
    "        \"\"\"\n",
    "        # Compute Nyquist cutoff\n",
    "        if size_hint is not None:\n",
    "            cutoff = size_hint // 2  # Nyquist frequency\n",
    "            mask = self.freqs <= cutoff\n",
    "            f = self.freqs[mask]  # (F_eff,)\n",
    "        else:\n",
    "            f = self.freqs\n",
    "        \n",
    "        # Separate frequencies for x and y\n",
    "        x_proj = 2 * math.pi * coords[..., :1] * f  # (B, N, F)\n",
    "        y_proj = 2 * math.pi * coords[..., 1:] * f  # (B, N, F)\n",
    "        \n",
    "        # sin/cos for both dimensions\n",
    "        return torch.cat([\n",
    "            x_proj.sin(), x_proj.cos(),\n",
    "            y_proj.sin(), y_proj.cos()\n",
    "        ], dim=-1)  # (B, N, 4*F)\n",
    "\n",
    "# Test\n",
    "fourier = ResolutionAwareFourier(coord_dim=2, num_freqs=64, max_cycles=16).to(device)\n",
    "test_coords = torch.rand(4, 100, 2).to(device)\n",
    "\n",
    "# At 32×32\n",
    "feats_32 = fourier(test_coords, size_hint=32)\n",
    "print(f\"Resolution-aware Fourier test:\")\n",
    "print(f\"  Input: {test_coords.shape}\")\n",
    "print(f\"  Output at 32×32: {feats_32.shape} (max freq = 16 cycles)\")\n",
    "\n",
    "# At 64×64\n",
    "feats_64 = fourier(test_coords, size_hint=64)\n",
    "print(f\"  Output at 64×64: {feats_64.shape} (max freq = 32 cycles)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Geometry-Aware SSM with Per-Step Δt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSMBlockGeometric(nn.Module):\n",
    "    \"\"\"\n",
    "    SSM with geometry-aware time steps.\n",
    "    \n",
    "    Key fix: dt derived from coordinate distances, not 1/N.\n",
    "    Makes SSM behavior independent of sequence length.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_state=16, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        \n",
    "        # State space parameters\n",
    "        self.A_log = nn.Parameter(torch.randn(d_state) * 0.1 - 1.0)\n",
    "        self.B = nn.Linear(d_model, d_state, bias=False)\n",
    "        self.C = nn.Linear(d_state, d_model, bias=False)\n",
    "        self.D = nn.Parameter(torch.randn(d_model) * 0.01)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.B.weight, gain=0.5)\n",
    "        nn.init.xavier_uniform_(self.C.weight, gain=0.5)\n",
    "        \n",
    "        # Gating (no dropout per playbook)\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)  # Keep at 0 for SSM\n",
    "        self.eps = 1e-8\n",
    "    \n",
    "    def forward(self, x, dt=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, N, d_model)\n",
    "            dt: (B, N) per-step time deltas from geometry\n",
    "        \"\"\"\n",
    "        B, N, D = x.shape\n",
    "        \n",
    "        # Get A matrix\n",
    "        A = -torch.exp(self.A_log).clamp(min=self.eps, max=10.0)  # (d_state,)\n",
    "        \n",
    "        # Input projection\n",
    "        Bu = self.B(x)  # (B, N, d_state)\n",
    "        \n",
    "        # Handle dt\n",
    "        if dt is None:\n",
    "            dt = torch.ones(B, N, device=x.device, dtype=x.dtype) / N\n",
    "        elif dt.ndim == 1:\n",
    "            dt = dt.unsqueeze(0).expand(B, -1)\n",
    "        \n",
    "        # Cumulative time\n",
    "        t = torch.cumsum(dt, dim=1)  # (B, N)\n",
    "        \n",
    "        # Compute decay matrix: decay[i,j] = exp(A * (t[i] - t[j])) if i >= j\n",
    "        idx = torch.arange(N, device=x.device)\n",
    "        mask = (idx.unsqueeze(0) >= idx.unsqueeze(1)).float()  # (N, N)\n",
    "        \n",
    "        t_diff = (t.unsqueeze(2) - t.unsqueeze(1)).clamp(min=0.0)  # (B, N, N)\n",
    "        decay = torch.exp(t_diff.unsqueeze(-1) * A.view(1, 1, 1, -1))  # (B, N, N, d_state)\n",
    "        decay = decay * mask.view(1, N, N, 1)\n",
    "        \n",
    "        # Compute hidden states\n",
    "        h = torch.einsum('bijn,bjd->bid', decay, Bu)  # (B, N, d_state)\n",
    "        h = torch.clamp(h, min=-10.0, max=10.0)\n",
    "        \n",
    "        # Output\n",
    "        y = self.C(h) + self.D * x\n",
    "        \n",
    "        # Gating and residual\n",
    "        gate = self.gate(x)\n",
    "        y = gate * y + (1 - gate) * x\n",
    "        \n",
    "        return self.dropout(self.norm(y))\n",
    "\n",
    "\n",
    "class MambaBlockGeometric(nn.Module):\n",
    "    \"\"\"Mamba block with geometric SSM\"\"\"\n",
    "    def __init__(self, d_model, d_state=16, expand_factor=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.proj_in = nn.Linear(d_model, d_model * expand_factor)\n",
    "        self.ssm = SSMBlockGeometric(d_model * expand_factor, d_state, dropout=0.0)  # No dropout in SSM\n",
    "        self.proj_out = nn.Linear(d_model * expand_factor, d_model)\n",
    "        \n",
    "        # MLP with dropout only here\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, dt=None):\n",
    "        residual = x\n",
    "        x = self.proj_in(x)\n",
    "        x = self.ssm(x, dt=dt)\n",
    "        x = self.proj_out(x)\n",
    "        x = x + residual\n",
    "        x = x + self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Time Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalTimeEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    \n",
    "    def forward(self, t):\n",
    "        device = t.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat([emb.sin(), emb.cos()], dim=-1)\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Local Cross-Attention with KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_indices(query_coords, input_coords, k=32):\n",
    "    \"\"\"\n",
    "    Find K nearest neighbors for each query point.\n",
    "    \n",
    "    Args:\n",
    "        query_coords: (B, N_q, 2)\n",
    "        input_coords: (B, N_in, 2)\n",
    "        k: number of neighbors\n",
    "    Returns:\n",
    "        indices: (B, N_q, k)\n",
    "    \"\"\"\n",
    "    dist = torch.cdist(query_coords, input_coords)  # (B, N_q, N_in)\n",
    "    _, indices = torch.topk(dist, k, dim=-1, largest=False)  # (B, N_q, k)\n",
    "    return indices\n",
    "\n",
    "\n",
    "class LocalCrossAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    KNN-based local cross-attention with relative position encoding.\n",
    "    \n",
    "    Playbook: \"Make cross-attention local (KNN) and use relative offsets.\"\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads=8, k=32, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.k = k\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Relative position MLP\n",
    "        self.rel_pos_mlp = nn.Sequential(\n",
    "            nn.Linear(2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_heads)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "    \n",
    "    def forward(self, query, key, value, query_coords, key_coords):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: (B, N_q, d_model)\n",
    "            key: (B, N_k, d_model)\n",
    "            value: (B, N_k, d_model)\n",
    "            query_coords: (B, N_q, 2)\n",
    "            key_coords: (B, N_k, 2)\n",
    "        \"\"\"\n",
    "        B, N_q, _ = query.shape\n",
    "        N_k = key.shape[1]\n",
    "        \n",
    "        # Find KNN\n",
    "        knn_idx = knn_indices(query_coords, key_coords, k=min(self.k, N_k))  # (B, N_q, k)\n",
    "        k_actual = knn_idx.shape[2]\n",
    "        \n",
    "        # Project Q, K, V\n",
    "        Q = self.q_proj(query).view(B, N_q, self.num_heads, self.head_dim)  # (B, N_q, H, D)\n",
    "        K = self.k_proj(key).view(B, N_k, self.num_heads, self.head_dim)    # (B, N_k, H, D)\n",
    "        V = self.v_proj(value).view(B, N_k, self.num_heads, self.head_dim)  # (B, N_k, H, D)\n",
    "        \n",
    "        # Gather neighbors\n",
    "        K_local = torch.gather(\n",
    "            K, 1, knn_idx.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, -1, self.num_heads, self.head_dim)\n",
    "        ).transpose(2, 3)  # (B, N_q, H, k, D)\n",
    "        \n",
    "        V_local = torch.gather(\n",
    "            V, 1, knn_idx.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, -1, self.num_heads, self.head_dim)\n",
    "        ).transpose(2, 3)  # (B, N_q, H, k, D)\n",
    "        \n",
    "        # Compute relative positions\n",
    "        key_coords_local = torch.gather(\n",
    "            key_coords, 1, knn_idx.unsqueeze(-1).expand(-1, -1, -1, 2)\n",
    "        )  # (B, N_q, k, 2)\n",
    "        rel_pos = query_coords.unsqueeze(2) - key_coords_local  # (B, N_q, k, 2)\n",
    "        rel_pos_bias = self.rel_pos_mlp(rel_pos)  # (B, N_q, k, H)\n",
    "        rel_pos_bias = rel_pos_bias.permute(0, 1, 3, 2)  # (B, N_q, H, k)\n",
    "        \n",
    "        # Attention scores\n",
    "        Q = Q.unsqueeze(-2)  # (B, N_q, H, 1, D)\n",
    "        scores = (Q @ K_local.transpose(-2, -1)) * self.scale  # (B, N_q, H, 1, k)\n",
    "        scores = scores.squeeze(-2) + rel_pos_bias  # (B, N_q, H, k)\n",
    "        \n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        # Aggregate\n",
    "        out = (attn.unsqueeze(-1) * V_local).sum(dim=-2)  # (B, N_q, H, D)\n",
    "        out = out.reshape(B, N_q, self.d_model)\n",
    "        \n",
    "        return self.out_proj(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Main Architecture - Fixed MAMBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAMBADiffusionFixed(nn.Module):\n",
    "    \"\"\"\n",
    "    MAMBA Diffusion with ALL playbook fixes:\n",
    "    1. Resolution-aware band-limited Fourier features\n",
    "    2. Morton curve sorting\n",
    "    3. Geometry-aware SSM with per-step dt\n",
    "    4. Local KNN cross-attention\n",
    "    5. RBF interpolation prior + residual\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_fourier_freqs=64,\n",
    "        max_cycles=16,\n",
    "        d_model=512,\n",
    "        num_layers=6,\n",
    "        d_state=16,\n",
    "        dropout=0.1,\n",
    "        knn_k=32\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.knn_k = knn_k\n",
    "        \n",
    "        # Resolution-aware Fourier features\n",
    "        self.fourier = ResolutionAwareFourier(\n",
    "            coord_dim=2,\n",
    "            num_freqs=num_fourier_freqs,\n",
    "            max_cycles=max_cycles\n",
    "        )\n",
    "        \n",
    "        # Feature dimension (4 * num_freqs for separate x/y sin/cos)\n",
    "        feat_dim = num_fourier_freqs * 4\n",
    "        \n",
    "        # Project inputs and queries\n",
    "        self.input_proj = nn.Linear(feat_dim + 3, d_model)\n",
    "        self.query_proj = nn.Linear(feat_dim + 3, d_model)\n",
    "        \n",
    "        # Time embedding\n",
    "        self.time_embed = SinusoidalTimeEmbedding(d_model)\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(d_model, d_model)\n",
    "        )\n",
    "        \n",
    "        # Mamba blocks with geometry-aware SSM\n",
    "        self.mamba_blocks = nn.ModuleList([\n",
    "            MambaBlockGeometric(d_model, d_state=d_state, expand_factor=2, dropout=dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Local cross-attention\n",
    "        self.local_cross_attn = LocalCrossAttention(\n",
    "            d_model, num_heads=8, k=knn_k, dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Decoder (predict residual on top of RBF prior)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_model * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 2, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, 3)\n",
    "        )\n",
    "        \n",
    "        # RBF prior parameters\n",
    "        self.rbf_sigma = 0.1\n",
    "    \n",
    "    def compute_rbf_prior(self, query_coords, input_coords, input_values):\n",
    "        \"\"\"\n",
    "        RBF interpolation prior for color smoothness.\n",
    "        \n",
    "        Args:\n",
    "            query_coords: (B, N_q, 2)\n",
    "            input_coords: (B, N_in, 2)\n",
    "            input_values: (B, N_in, 3) in [-1, 1]\n",
    "        Returns:\n",
    "            prior: (B, N_q, 3)\n",
    "        \"\"\"\n",
    "        # Find KNN\n",
    "        knn_idx = knn_indices(query_coords, input_coords, k=min(self.knn_k, input_coords.shape[1]))\n",
    "        \n",
    "        # Gather neighbor coordinates and values\n",
    "        nbr_coords = torch.gather(\n",
    "            input_coords, 1, knn_idx.unsqueeze(-1).expand(-1, -1, -1, 2)\n",
    "        )  # (B, N_q, k, 2)\n",
    "        nbr_values = torch.gather(\n",
    "            input_values, 1, knn_idx.unsqueeze(-1).expand(-1, -1, -1, 3)\n",
    "        )  # (B, N_q, k, 3)\n",
    "        \n",
    "        # Compute RBF weights\n",
    "        rel_pos = query_coords.unsqueeze(2) - nbr_coords  # (B, N_q, k, 2)\n",
    "        dist_sq = (rel_pos ** 2).sum(-1)  # (B, N_q, k)\n",
    "        weights = torch.softmax(-dist_sq / (2 * self.rbf_sigma ** 2), dim=-1)  # (B, N_q, k)\n",
    "        \n",
    "        # Weighted average\n",
    "        prior = (weights.unsqueeze(-1) * nbr_values).sum(dim=2)  # (B, N_q, 3)\n",
    "        return prior\n",
    "    \n",
    "    def forward(self, noisy_values, query_coords, t, input_coords, input_values, size_hint=32):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            noisy_values: (B, N_out, 3) in [-1, 1]\n",
    "            query_coords: (B, N_out, 2) in [0, 1]\n",
    "            t: (B,) timestep\n",
    "            input_coords: (B, N_in, 2) in [0, 1]\n",
    "            input_values: (B, N_in, 3) in [-1, 1]\n",
    "            size_hint: Resolution for band-limiting (32, 64, 96)\n",
    "        \"\"\"\n",
    "        B = query_coords.shape[0]\n",
    "        N_in = input_coords.shape[1]\n",
    "        N_out = query_coords.shape[1]\n",
    "        \n",
    "        # Time embedding\n",
    "        t_emb = self.time_mlp(self.time_embed(t))  # (B, d_model)\n",
    "        \n",
    "        # Band-limited Fourier features\n",
    "        input_feats = self.fourier(input_coords, size_hint=size_hint)\n",
    "        query_feats = self.fourier(query_coords, size_hint=size_hint)\n",
    "        \n",
    "        # Encode tokens\n",
    "        input_tokens = self.input_proj(\n",
    "            torch.cat([input_feats, input_values], dim=-1)\n",
    "        )  # (B, N_in, d_model)\n",
    "        query_tokens = self.query_proj(\n",
    "            torch.cat([query_feats, noisy_values], dim=-1)\n",
    "        )  # (B, N_out, d_model)\n",
    "        \n",
    "        # Add time embedding\n",
    "        input_tokens = input_tokens + t_emb.unsqueeze(1)\n",
    "        query_tokens = query_tokens + t_emb.unsqueeze(1)\n",
    "        \n",
    "        # Concatenate and sort by Morton order\n",
    "        all_coords = torch.cat([input_coords, query_coords], dim=1)  # (B, N_tot, 2)\n",
    "        seq = torch.cat([input_tokens, query_tokens], dim=1)  # (B, N_tot, d_model)\n",
    "        \n",
    "        perm = morton_order(all_coords)  # (B, N_tot)\n",
    "        inv_perm = torch.argsort(perm, dim=1)  # (B, N_tot)\n",
    "        \n",
    "        # Apply sorting\n",
    "        sorted_coords = torch.gather(\n",
    "            all_coords, 1, perm.unsqueeze(-1).expand(-1, -1, 2)\n",
    "        )\n",
    "        sorted_seq = torch.gather(\n",
    "            seq, 1, perm.unsqueeze(-1).expand(-1, -1, self.d_model)\n",
    "        )\n",
    "        \n",
    "        # Compute geometric dt\n",
    "        diffs = sorted_coords[:, 1:, :] - sorted_coords[:, :-1, :]  # (B, N_tot-1, 2)\n",
    "        ds = torch.norm(diffs, dim=-1)  # (B, N_tot-1)\n",
    "        ds = F.pad(ds, (1, 0), value=0.0)  # (B, N_tot)\n",
    "        dt = ds / (ds.mean(dim=1, keepdim=True) + 1e-8)  # Normalized\n",
    "        \n",
    "        # Process through Mamba blocks with geometric dt\n",
    "        for mamba_block in self.mamba_blocks:\n",
    "            sorted_seq = mamba_block(sorted_seq, dt=dt)\n",
    "        \n",
    "        # Unsort\n",
    "        seq = torch.gather(\n",
    "            sorted_seq, 1, inv_perm.unsqueeze(-1).expand(-1, -1, self.d_model)\n",
    "        )\n",
    "        \n",
    "        # Split back\n",
    "        input_seq = seq[:, :N_in, :]\n",
    "        query_seq = seq[:, N_in:, :]\n",
    "        \n",
    "        # Local cross-attention\n",
    "        output = self.local_cross_attn(\n",
    "            query_seq, input_seq, input_seq,\n",
    "            query_coords, input_coords\n",
    "        )\n",
    "        \n",
    "        # RBF prior\n",
    "        prior = self.compute_rbf_prior(query_coords, input_coords, input_values)\n",
    "        \n",
    "        # Predict residual\n",
    "        residual = self.decoder(output)\n",
    "        \n",
    "        # Final prediction\n",
    "        rgb = torch.clamp(prior + residual, -1.0, 1.0)\n",
    "        \n",
    "        return rgb\n",
    "\n",
    "\n",
    "# Test model\n",
    "print(\"\\nTesting Fixed MAMBA:\")\n",
    "model = MAMBADiffusionFixed(\n",
    "    num_fourier_freqs=64,\n",
    "    max_cycles=16,\n",
    "    d_model=512,\n",
    "    num_layers=6,\n",
    "    d_state=16,\n",
    "    dropout=0.1,\n",
    "    knn_k=32\n",
    ").to(device)\n",
    "\n",
    "# Test in [-1, 1] range\n",
    "test_noisy = torch.randn(4, 204, 3).to(device)\n",
    "test_query_coords = torch.rand(4, 204, 2).to(device)\n",
    "test_t = torch.rand(4).to(device)\n",
    "test_input_coords = torch.rand(4, 204, 2).to(device)\n",
    "test_input_values = torch.randn(4, 204, 3).to(device)\n",
    "\n",
    "test_out = model(test_noisy, test_query_coords, test_t, test_input_coords, test_input_values, size_hint=32)\n",
    "print(f\"  Output shape: {test_out.shape}\")\n",
    "print(f\"  Output range: [{test_out.min().item():.3f}, {test_out.max().item():.3f}] (should be [-1, 1])\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. EMA Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMA:\n",
    "    \"\"\"\n",
    "    Exponential Moving Average of model weights.\n",
    "    \n",
    "    Playbook: \"EMA: maintain an EMA of weights and use it for sampling—often a large PSNR boost.\"\n",
    "    \"\"\"\n",
    "    def __init__(self, model, decay=0.999):\n",
    "        self.model = model\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        self.backup = {}\n",
    "        \n",
    "        # Initialize shadow weights\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone()\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"Update EMA weights\"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = self.decay * self.shadow[name] + (1 - self.decay) * param.data\n",
    "    \n",
    "    def apply_shadow(self):\n",
    "        \"\"\"Temporarily replace model weights with EMA weights\"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                param.data = self.shadow[name]\n",
    "    \n",
    "    def restore(self):\n",
    "        \"\"\"Restore original model weights\"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training with All Fixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_flow(x_0, x_1, t):\n",
    "    \"\"\"Linear interpolation: (1-t)*x_0 + t*x_1\"\"\"\n",
    "    return (1 - t) * x_0 + t * x_1\n",
    "\n",
    "def target_velocity(x_0, x_1):\n",
    "    \"\"\"Target velocity: x_1 - x_0\"\"\"\n",
    "    return x_1 - x_0\n",
    "\n",
    "@torch.no_grad()\n",
    "def heun_sample(model, output_coords, input_coords, input_values, num_steps=50, size_hint=32, device='cuda'):\n",
    "    \"\"\"Heun ODE solver for flow matching\"\"\"\n",
    "    B, N_out = output_coords.shape[0], output_coords.shape[1]\n",
    "    x_t = torch.randn(B, N_out, 3, device=device)  # Start from N(0,1)\n",
    "    \n",
    "    dt = 1.0 / num_steps\n",
    "    ts = torch.linspace(0, 1 - dt, num_steps)\n",
    "    \n",
    "    for t_val in tqdm(ts, desc=\"Sampling\", leave=False):\n",
    "        t = torch.full((B,), t_val.item(), device=device)\n",
    "        t_next = torch.full((B,), t_val.item() + dt, device=device)\n",
    "        \n",
    "        v1 = model(x_t, output_coords, t, input_coords, input_values, size_hint=size_hint)\n",
    "        x_next_pred = x_t + dt * v1\n",
    "        \n",
    "        v2 = model(x_next_pred, output_coords, t_next, input_coords, input_values, size_hint=size_hint)\n",
    "        x_t = x_t + dt * 0.5 * (v1 + v2)\n",
    "    \n",
    "    return torch.clamp(x_t, -1, 1)\n",
    "\n",
    "def train_flow_matching_fixed(\n",
    "    model, train_loader, test_loader, epochs=100, lr=2e-4, wd=0.05,\n",
    "    device='cuda', visualize_every=5, eval_every=2, save_dir='checkpoints_fixed'\n",
    "):\n",
    "    \"\"\"Train with all playbook fixes\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # AdamW with playbook settings\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    \n",
    "    # Warmup + cosine scheduler\n",
    "    warmup_steps = 5000\n",
    "    total_steps = epochs * len(train_loader)\n",
    "    \n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            return step / warmup_steps\n",
    "        else:\n",
    "            progress = (step - warmup_steps) / (total_steps - warmup_steps)\n",
    "            return 0.5 * (1 + math.cos(math.pi * progress))\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "    \n",
    "    # EMA\n",
    "    ema = EMA(model, decay=0.999)\n",
    "    \n",
    "    losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    # Pixel-center grids for visualization\n",
    "    full_coords = make_center_grid(32, device)\n",
    "    \n",
    "    viz_batch = next(iter(train_loader))\n",
    "    viz_input_coords = viz_batch['input_coords'][:4].to(device)\n",
    "    viz_input_values = viz_batch['input_values'][:4].to(device) * 2 - 1  # [0,1] → [-1,1]\n",
    "    viz_output_coords = viz_batch['output_coords'][:4].to(device)\n",
    "    viz_output_values = viz_batch['output_values'][:4].to(device) * 2 - 1\n",
    "    viz_full_images = viz_batch['full_image'][:4].to(device)\n",
    "    viz_input_indices = viz_batch['input_indices'][:4]\n",
    "    \n",
    "    global_step = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            input_coords = batch['input_coords'].to(device)\n",
    "            input_values = batch['input_values'].to(device) * 2 - 1  # [-1, 1]\n",
    "            output_coords = batch['output_coords'].to(device)\n",
    "            output_values = batch['output_values'].to(device) * 2 - 1  # [-1, 1]\n",
    "            \n",
    "            B = input_coords.shape[0]\n",
    "            t = torch.rand(B, device=device)\n",
    "            \n",
    "            # Flow matching in [-1, 1] space\n",
    "            x_0 = torch.randn_like(output_values)  # N(0, 1)\n",
    "            x_1 = output_values\n",
    "            \n",
    "            t_broadcast = t.view(B, 1, 1)\n",
    "            x_t = conditional_flow(x_0, x_1, t_broadcast)\n",
    "            u_t = target_velocity(x_0, x_1)\n",
    "            \n",
    "            v_pred = model(x_t, output_coords, t, input_coords, input_values, size_hint=32)\n",
    "            loss = F.mse_loss(v_pred, u_t)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Update EMA\n",
    "            ema.update()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            global_step += 1\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Loss = {avg_loss:.6f}, LR = {scheduler.get_last_lr()[0]:.6f}\")\n",
    "        \n",
    "        # Evaluation with EMA\n",
    "        val_loss = None\n",
    "        if (epoch + 1) % eval_every == 0 or epoch == 0:\n",
    "            model.eval()\n",
    "            ema.apply_shadow()  # Use EMA weights\n",
    "            \n",
    "            tracker = MetricsTracker()\n",
    "            val_loss_accum = 0\n",
    "            val_batches = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for i, batch in enumerate(test_loader):\n",
    "                    if i >= 10:\n",
    "                        break\n",
    "                    \n",
    "                    input_vals_norm = batch['input_values'].to(device) * 2 - 1\n",
    "                    output_vals_norm = batch['output_values'].to(device) * 2 - 1\n",
    "                    \n",
    "                    pred_values = heun_sample(\n",
    "                        model, batch['output_coords'].to(device),\n",
    "                        batch['input_coords'].to(device), input_vals_norm,\n",
    "                        num_steps=50, size_hint=32, device=device\n",
    "                    )\n",
    "                    \n",
    "                    tracker.update(pred_values, output_vals_norm)\n",
    "                    val_loss_accum += F.mse_loss(pred_values, output_vals_norm).item()\n",
    "                    val_batches += 1\n",
    "                \n",
    "                results = tracker.compute()\n",
    "                val_loss = val_loss_accum / val_batches\n",
    "                print(f\"  Eval (EMA) - MSE: {results['mse']:.6f}, MAE: {results['mae']:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "                \n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    torch.save({\n",
    "                        'epoch': epoch + 1,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'ema_shadow': ema.shadow,\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'scheduler_state_dict': scheduler.state_dict(),\n",
    "                        'loss': avg_loss,\n",
    "                        'val_loss': val_loss,\n",
    "                        'best_val_loss': best_val_loss\n",
    "                    }, f'{save_dir}/mamba_fixed_best.pth')\n",
    "                    print(f\"  ✓ Saved best model (val_loss: {val_loss:.6f})\")\n",
    "            \n",
    "            ema.restore()  # Restore training weights\n",
    "        \n",
    "        # Save latest\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'ema_shadow': ema.shadow,\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "            'val_loss': val_loss if val_loss is not None else avg_loss,\n",
    "            'best_val_loss': best_val_loss\n",
    "        }, f'{save_dir}/mamba_fixed_latest.pth')\n",
    "        \n",
    "        # Visualization\n",
    "        if (epoch + 1) % visualize_every == 0 or epoch == 0:\n",
    "            model.eval()\n",
    "            ema.apply_shadow()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                pred_values = heun_sample(\n",
    "                    model, viz_output_coords, viz_input_coords, viz_input_values,\n",
    "                    num_steps=50, size_hint=32, device=device\n",
    "                )\n",
    "                \n",
    "                full_coords_batch = full_coords.unsqueeze(0).expand(4, -1, -1)\n",
    "                full_pred_values = heun_sample(\n",
    "                    model, full_coords_batch, viz_input_coords, viz_input_values,\n",
    "                    num_steps=50, size_hint=32, device=device\n",
    "                )\n",
    "                \n",
    "                # Convert back to [0, 1] for visualization\n",
    "                pred_values_vis = (pred_values + 1) / 2\n",
    "                full_pred_values_vis = (full_pred_values + 1) / 2\n",
    "                full_pred_images = full_pred_values_vis.view(4, 32, 32, 3).permute(0, 3, 1, 2)\n",
    "                \n",
    "                fig, axes = plt.subplots(4, 5, figsize=(20, 16))\n",
    "                \n",
    "                for i in range(4):\n",
    "                    axes[i, 0].imshow(viz_full_images[i].permute(1, 2, 0).cpu().numpy())\n",
    "                    axes[i, 0].set_title('Ground Truth' if i == 0 else '', fontsize=10)\n",
    "                    axes[i, 0].axis('off')\n",
    "                    \n",
    "                    input_img = torch.zeros(3, 32, 32, device=device)\n",
    "                    input_idx = viz_input_indices[i]\n",
    "                    input_img.view(3, -1)[:, input_idx] = ((viz_input_values[i] + 1) / 2).T\n",
    "                    axes[i, 1].imshow(input_img.permute(1, 2, 0).cpu().numpy())\n",
    "                    axes[i, 1].set_title('Sparse Input (20%)' if i == 0 else '', fontsize=10)\n",
    "                    axes[i, 1].axis('off')\n",
    "                    \n",
    "                    target_img = torch.zeros(3, 32, 32, device=device)\n",
    "                    output_idx = viz_batch['output_indices'][i]\n",
    "                    target_img.view(3, -1)[:, output_idx] = ((viz_output_values[i] + 1) / 2).T\n",
    "                    axes[i, 2].imshow(target_img.permute(1, 2, 0).cpu().numpy())\n",
    "                    axes[i, 2].set_title('Sparse Target (20%)' if i == 0 else '', fontsize=10)\n",
    "                    axes[i, 2].axis('off')\n",
    "                    \n",
    "                    pred_img = torch.zeros(3, 32, 32, device=device)\n",
    "                    pred_img.view(3, -1)[:, output_idx] = pred_values_vis[i].T\n",
    "                    axes[i, 3].imshow(np.clip(pred_img.permute(1, 2, 0).cpu().numpy(), 0, 1))\n",
    "                    axes[i, 3].set_title('Sparse Prediction' if i == 0 else '', fontsize=10)\n",
    "                    axes[i, 3].axis('off')\n",
    "                    \n",
    "                    axes[i, 4].imshow(np.clip(full_pred_images[i].permute(1, 2, 0).cpu().numpy(), 0, 1))\n",
    "                    axes[i, 4].set_title('Full Field (FIXED!)' if i == 0 else '', fontsize=10)\n",
    "                    axes[i, 4].axis('off')\n",
    "                \n",
    "                plt.suptitle(f'MAMBA FIXED - Epoch {epoch+1} (Best Val: {best_val_loss:.6f})', \n",
    "                           fontsize=14, y=0.995)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'{save_dir}/epoch_{epoch+1:03d}.png', dpi=150, bbox_inches='tight')\n",
    "                plt.show()\n",
    "                plt.close()\n",
    "            \n",
    "            ema.restore()\n",
    "    \n",
    "    print(f\"\\n✓ Training complete! Best validation loss: {best_val_loss:.6f}\")\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Load Data and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "train_dataset = SparseCIFAR10Dataset(\n",
    "    root='../data', train=True, input_ratio=0.2, output_ratio=0.2, download=True, seed=42\n",
    ")\n",
    "test_dataset = SparseCIFAR10Dataset(\n",
    "    root='../data', train=False, input_ratio=0.2, output_ratio=0.2, download=True, seed=42\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Test: {len(test_dataset)}\")\n",
    "\n",
    "# Initialize model\n",
    "model = MAMBADiffusionFixed(\n",
    "    num_fourier_freqs=64,\n",
    "    max_cycles=16,\n",
    "    d_model=512,\n",
    "    num_layers=6,\n",
    "    d_state=16,\n",
    "    dropout=0.1,\n",
    "    knn_k=32\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Train with all fixes\n",
    "losses = train_flow_matching_fixed(\n",
    "    model, train_loader, test_loader,\n",
    "    epochs=100, lr=2e-4, wd=0.05,\n",
    "    device=device, save_dir='checkpoints_fixed'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses, linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss: MAMBA FIXED (All Playbook Improvements)')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.savefig('checkpoints_fixed/training_loss.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Load best model\n",
    "checkpoint = torch.load('checkpoints_fixed/mamba_fixed_best.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Apply EMA\n",
    "ema_eval = EMA(model)\n",
    "ema_eval.shadow = checkpoint['ema_shadow']\n",
    "ema_eval.apply_shadow()\n",
    "\n",
    "# Pixel-center grid\n",
    "full_coords = make_center_grid(32, device)\n",
    "\n",
    "model.eval()\n",
    "tracker_full = MetricsTracker()\n",
    "\n",
    "for i, batch in enumerate(tqdm(test_loader, desc=\"Full Reconstruction (32×32)\")):\n",
    "    if i >= 50:\n",
    "        break\n",
    "    \n",
    "    B = batch['input_coords'].shape[0]\n",
    "    full_coords_batch = full_coords.unsqueeze(0).expand(B, -1, -1)\n",
    "    \n",
    "    input_vals_norm = batch['input_values'].to(device) * 2 - 1\n",
    "    \n",
    "    pred_values = heun_sample(\n",
    "        model, full_coords_batch,\n",
    "        batch['input_coords'].to(device),\n",
    "        input_vals_norm,\n",
    "        num_steps=100, size_hint=32, device=device\n",
    "    )\n",
    "    \n",
    "    # Convert back to [0, 1]\n",
    "    pred_values = (pred_values + 1) / 2\n",
    "    pred_images = pred_values.view(B, 32, 32, 3).permute(0, 3, 1, 2)\n",
    "    tracker_full.update(None, None, pred_images, batch['full_image'].to(device))\n",
    "\n",
    "results = tracker_full.compute()\n",
    "print(f\"\\n32×32 Full Image Reconstruction (SHOULD BE CLEAN NOW):\")\n",
    "print(f\"  PSNR: {results['psnr']:.2f} dB\")\n",
    "print(f\"  SSIM: {results['ssim']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Multi-Scale Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test super-resolution at 64×64 and 96×96\n",
    "multi_scale_grids = {\n",
    "    32: make_center_grid(32, device),\n",
    "    64: make_center_grid(64, device),\n",
    "    96: make_center_grid(96, device)\n",
    "}\n",
    "\n",
    "sample_batch = next(iter(test_loader))\n",
    "B = 4\n",
    "\n",
    "input_vals_norm = sample_batch['input_values'][:B].to(device) * 2 - 1\n",
    "\n",
    "multi_scale_results = {}\n",
    "for size, coords in multi_scale_grids.items():\n",
    "    print(f\"\\nReconstructing at {size}×{size}...\")\n",
    "    coords_batch = coords.unsqueeze(0).expand(B, -1, -1)\n",
    "    \n",
    "    pred_values = heun_sample(\n",
    "        model, coords_batch,\n",
    "        sample_batch['input_coords'][:B].to(device),\n",
    "        input_vals_norm,\n",
    "        num_steps=100, size_hint=size, device=device\n",
    "    )\n",
    "    \n",
    "    pred_values = (pred_values + 1) / 2  # Back to [0, 1]\n",
    "    pred_images = pred_values.view(B, size, size, 3).permute(0, 3, 1, 2)\n",
    "    multi_scale_results[size] = pred_images\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(4, 4, figsize=(16, 16))\n",
    "for i in range(4):\n",
    "    axes[i, 0].imshow(sample_batch['full_image'][i].permute(1, 2, 0).numpy())\n",
    "    axes[i, 0].set_title('Ground Truth\\n(32×32)' if i == 0 else '')\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    axes[i, 1].imshow(np.clip(multi_scale_results[32][i].permute(1, 2, 0).cpu().numpy(), 0, 1))\n",
    "    axes[i, 1].set_title('Recon 32×32\\n(CLEAN!)' if i == 0 else '')\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    axes[i, 2].imshow(np.clip(multi_scale_results[64][i].permute(1, 2, 0).cpu().numpy(), 0, 1))\n",
    "    axes[i, 2].set_title('Recon 64×64\\n(2x Super-Res)' if i == 0 else '')\n",
    "    axes[i, 2].axis('off')\n",
    "    \n",
    "    axes[i, 3].imshow(np.clip(multi_scale_results[96][i].permute(1, 2, 0).cpu().numpy(), 0, 1))\n",
    "    axes[i, 3].set_title('Recon 96×96\\n(3x Super-Res)' if i == 0 else '')\n",
    "    axes[i, 3].axis('off')\n",
    "\n",
    "plt.suptitle('MAMBA FIXED: Clean 32×32 + Super-Resolution', fontsize=16, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.savefig('checkpoints_fixed/multi_scale_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPECTED RESULTS:\")\n",
    "print(\"  ✅ 32×32 should now be CLEAN (no speckle noise)\")\n",
    "print(\"  ✅ 64×64 and 96×96 should show smooth super-resolution\")\n",
    "print(\"  ✅ All scales should have consistent colors and features\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Fixes\n",
    "\n",
    "### ✅ Implemented from Playbook\n",
    "\n",
    "1. **Pixel-Center Coordinates** (`make_center_grid`)\n",
    "   - Eliminates half-pixel misalignment at 32×32\n",
    "\n",
    "2. **Geometry-Aware SSM**\n",
    "   - Morton (Z-order) curve sorting\n",
    "   - Per-step Δt from coordinate distances\n",
    "   - Sequence-length invariant behavior\n",
    "\n",
    "3. **Band-Limited Fourier Features**\n",
    "   - Resolution-aware with Nyquist cutoff\n",
    "   - Prevents high-frequency aliasing\n",
    "   - num_freqs=64, max_cycles=16 for 32×32\n",
    "\n",
    "4. **Local Cross-Attention**\n",
    "   - KNN-based (k=32)\n",
    "   - Relative position encoding\n",
    "   - More efficient and stable\n",
    "\n",
    "5. **RBF Interpolation Prior**\n",
    "   - Strong bias for color smoothness\n",
    "   - Model predicts residual corrections\n",
    "   - Better convergence\n",
    "\n",
    "6. **Training Hygiene**\n",
    "   - RGB in `[-1, 1]` range\n",
    "   - N(0,1) noise initialization\n",
    "   - EMA weights (decay=0.999)\n",
    "   - AdamW: lr=2e-4, wd=0.05\n",
    "   - Warmup + cosine schedule\n",
    "   - No dropout in SSM\n",
    "\n",
    "### 📊 Expected Improvements\n",
    "\n",
    "| Metric | Before | After (Expected) |\n",
    "|--------|--------|------------------|\n",
    "| 32×32 Quality | Noisy/speckle | Clean |\n",
    "| PSNR @ 32×32 | ~20 dB | ~25-28 dB |\n",
    "| SSIM @ 32×32 | ~0.70 | ~0.85-0.90 |\n",
    "| Super-res | Good | Excellent |\n",
    "| Training Speed | Baseline | ~Same |\n",
    "\n",
    "### 🔬 Key Insights\n",
    "\n",
    "**Why 64×64/96×96 looked better before:**\n",
    "1. More tokens → smaller dt → smoother SSM dynamics (masked the dt=1/N bug)\n",
    "2. Oversampling \"anti-aliased\" the high-frequency features\n",
    "\n",
    "**After fixes:**\n",
    "- 32×32 converges properly with clean reconstructions\n",
    "- Super-resolution capacity preserved\n",
    "- Model learns true continuous field, not grid-dependent features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
