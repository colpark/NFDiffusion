{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coordinate-Conditioned Diffusion for Zero-Shot Super-Resolution on CIFAR-10\n",
    "\n",
    "**Goal**: Train once on CIFAR-10 32×32, infer at 32×32, 64×64, 96×96 for unseen samples.\n",
    "\n",
    "**Dataset**: CIFAR-10 (60K training + 10K test images, 32×32 RGB, 10 classes)\n",
    "\n",
    "**Key Innovation**: Fourier coordinate embeddings teach the model continuous field representation.\n",
    "\n",
    "**Differences from STL-10 version**:\n",
    "- CIFAR-10 dataset (smaller images, more samples)\n",
    "- Adjusted batch size and training steps\n",
    "- Class-conditional support (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, random, sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision as tv\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fourier Coordinate Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FourierCoordinateEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Fourier features for continuous spatial coordinates.\n",
    "    Input: (B, H, W, 2) normalized coordinates [0, 1]\n",
    "    Output: (B, H, W, encoding_dim) where encoding_dim = 4 * num_frequencies\n",
    "    \"\"\"\n",
    "    def __init__(self, num_frequencies=10, scale=10.0):\n",
    "        super().__init__()\n",
    "        self.num_frequencies = num_frequencies\n",
    "        self.scale = scale\n",
    "        self.encoding_dim = 4 * num_frequencies\n",
    "        \n",
    "    def forward(self, coords):\n",
    "        B, H, W, _ = coords.shape\n",
    "        x = coords[..., 0:1]\n",
    "        y = coords[..., 1:2]\n",
    "        \n",
    "        freq_bands = 2.0 ** torch.arange(self.num_frequencies, device=coords.device, dtype=torch.float32)\n",
    "        freq_bands = freq_bands * math.pi * self.scale\n",
    "        \n",
    "        x_freq = x * freq_bands.view(1, 1, 1, -1)\n",
    "        x_features = torch.cat([torch.sin(x_freq), torch.cos(x_freq)], dim=-1)\n",
    "        \n",
    "        y_freq = y * freq_bands.view(1, 1, 1, -1)\n",
    "        y_features = torch.cat([torch.sin(y_freq), torch.cos(y_freq)], dim=-1)\n",
    "        \n",
    "        features = torch.cat([x_features, y_features], dim=-1)\n",
    "        return features\n",
    "\n",
    "\n",
    "def make_coordinate_grid(batch_size, height, width, device):\n",
    "    \"\"\"Create normalized coordinate grid [0, 1]².\"\"\"\n",
    "    y_coords = torch.linspace(0, 1, height, device=device)\n",
    "    x_coords = torch.linspace(0, 1, width, device=device)\n",
    "    yy, xx = torch.meshgrid(y_coords, x_coords, indexing='ij')\n",
    "    coords = torch.stack([xx, yy], dim=-1)\n",
    "    coords = coords.unsqueeze(0).expand(batch_size, -1, -1, -1)\n",
    "    return coords\n",
    "\n",
    "\n",
    "print(\"Fourier coordinate encoding defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal positional/time embedding.\"\"\"\n",
    "    def __init__(self, dim, max_period=10000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.max_period = max_period\n",
    "\n",
    "    def forward(self, t: torch.Tensor):\n",
    "        if t.dtype != torch.float32:\n",
    "            t = t.float()\n",
    "        half = self.dim // 2\n",
    "        device = t.device\n",
    "        freqs = torch.exp(-math.log(self.max_period) * torch.arange(0, half, device=device).float() / half)\n",
    "        args = t[:, None] * freqs[None, :]\n",
    "        emb = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "        if self.dim % 2 == 1:\n",
    "            emb = torch.cat([emb, torch.zeros_like(emb[:, :1])], dim=-1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "def hw_to_seq(t):\n",
    "    return t.flatten(2).transpose(1, 2)\n",
    "\n",
    "\n",
    "def seq_to_hw(t, h, w):\n",
    "    return t.transpose(1, 2).reshape(t.size(0), -1, h, w)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def soft_project(x, obs, mask, kernel_size=3, iters=1):\n",
    "    for _ in range(iters):\n",
    "        x = x * (1.0 - mask) + obs * mask\n",
    "    return x\n",
    "\n",
    "\n",
    "def to_img01(t):\n",
    "    return ((t.clamp(-1,1) + 1.0)/2.0).detach().cpu()\n",
    "\n",
    "\n",
    "def save_grid01(tensors01, path, nrow=8, pad=2):\n",
    "    rows = []\n",
    "    for t in tensors01:\n",
    "        grid = make_grid(t, nrow=nrow, padding=pad)\n",
    "        rows.append(grid)\n",
    "    big = torch.cat(rows, dim=1)\n",
    "    save_image(big, path)\n",
    "\n",
    "\n",
    "print(\"Utilities defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Coordinate-Conditioned UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim, dim_out=None, time_emb_dim=None, dropout=None, groups=32):\n",
    "        super().__init__()\n",
    "        self.dim, self.dim_out = dim, dim_out\n",
    "        dim_out = dim if dim_out is None else dim_out\n",
    "        self.norm1 = nn.GroupNorm(num_groups=groups, num_channels=dim)\n",
    "        self.activation1 = nn.SiLU()\n",
    "        self.conv1 = nn.Conv2d(dim, dim_out, kernel_size=3, padding=1)\n",
    "        self.block1 = nn.Sequential(self.norm1, self.activation1, self.conv1)\n",
    "\n",
    "        self.mlp = nn.Sequential(nn.SiLU(), nn.Linear(time_emb_dim, dim_out)) if time_emb_dim is not None else None\n",
    "\n",
    "        self.norm2 = nn.GroupNorm(num_groups=groups, num_channels=dim_out)\n",
    "        self.activation2 = nn.SiLU()\n",
    "        self.dropout = nn.Dropout(dropout) if dropout is not None and dropout > 0 else nn.Identity()\n",
    "        self.conv2 = nn.Conv2d(dim_out, dim_out, kernel_size=3, padding=1)\n",
    "        self.block2 = nn.Sequential(self.norm2, self.activation2, self.dropout, self.conv2)\n",
    "\n",
    "        self.residual_conv = nn.Conv2d(dim, dim_out, kernel_size=1) if dim != dim_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x, time_emb=None):\n",
    "        h = self.block1(x)\n",
    "        if time_emb is not None and self.mlp is not None:\n",
    "            h = h + self.mlp(time_emb)[..., None, None]\n",
    "        h = self.block2(h)\n",
    "        return h + self.residual_conv(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, groups=32):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.scale = dim ** (-0.5)\n",
    "        self.norm = nn.GroupNorm(num_groups=groups, num_channels=dim)\n",
    "        self.to_qkv = nn.Conv2d(dim, dim * 3, kernel_size=1)\n",
    "        self.to_out = nn.Conv2d(dim, dim, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(self.norm(x)).chunk(3, dim=1)\n",
    "        q, k, v = [hw_to_seq(t) for t in qkv]\n",
    "        sim = torch.einsum('bic,bjc->bij', q, k) * self.scale\n",
    "        attn = sim.softmax(dim=-1)\n",
    "        out = torch.einsum('bij,bjc->bic', attn, v)\n",
    "        out = seq_to_hw(out, h, w)\n",
    "        return self.to_out(out) + x\n",
    "\n",
    "\n",
    "class ResnetAttentionBlock(nn.Module):\n",
    "    def __init__(self, dim, dim_out=None, time_emb_dim=None, dropout=None, groups=32):\n",
    "        super().__init__()\n",
    "        self.resnet = ResnetBlock(dim, dim_out, time_emb_dim, dropout, groups)\n",
    "        self.attention = Attention(dim_out if dim_out is not None else dim, groups)\n",
    "\n",
    "    def forward(self, x, time_emb=None):\n",
    "        x = self.resnet(x, time_emb)\n",
    "        return self.attention(x)\n",
    "\n",
    "\n",
    "class downSample(nn.Module):\n",
    "    def __init__(self, dim_in):\n",
    "        super().__init__()\n",
    "        self.downsameple = nn.Conv2d(dim_in, dim_in, kernel_size=3, stride=2, padding=1)\n",
    "    def forward(self, x):\n",
    "        return self.downsameple(x)\n",
    "\n",
    "\n",
    "class upSample(nn.Module):\n",
    "    def __init__(self, dim_in):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Sequential(nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "                                      nn.Conv2d(dim_in, dim_in, kernel_size=3, padding=1))\n",
    "    def forward(self, x):\n",
    "        return self.upsample(x)\n",
    "\n",
    "\n",
    "class CoordinateConditionedUnet(nn.Module):\n",
    "    def __init__(self, dim, image_size, dim_multiply=(1, 2, 4, 8), channel=3, num_res_blocks=2,\n",
    "                 attn_resolutions=(16,), dropout=0.0, device='cuda', groups=32,\n",
    "                 coord_num_frequencies=10, coord_scale=10.0):\n",
    "        super().__init__()\n",
    "        assert dim % groups == 0, 'parameter [groups] must be divisible by parameter [dim]'\n",
    "\n",
    "        self.dim = dim\n",
    "        self.channel = channel\n",
    "        self.time_emb_dim = 4 * self.dim\n",
    "        self.num_resolutions = len(dim_multiply)\n",
    "        self.device = device\n",
    "        self.resolution = [int(image_size / (2 ** i)) for i in range(self.num_resolutions)]\n",
    "        self.hidden_dims = [self.dim, *map(lambda x: x * self.dim, dim_multiply)]\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "\n",
    "        positional_encoding = PositionalEncoding(self.dim)\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            positional_encoding, nn.Linear(self.dim, self.time_emb_dim),\n",
    "            nn.SiLU(), nn.Linear(self.time_emb_dim, self.time_emb_dim)\n",
    "        )\n",
    "        \n",
    "        self.coord_encoder = FourierCoordinateEncoding(\n",
    "            num_frequencies=coord_num_frequencies,\n",
    "            scale=coord_scale\n",
    "        )\n",
    "        coord_dim = self.coord_encoder.encoding_dim\n",
    "\n",
    "        self.down_path = nn.ModuleList([])\n",
    "        self.up_path = nn.ModuleList([])\n",
    "        concat_dim = []\n",
    "\n",
    "        self.init_conv = nn.Conv2d(channel * 3 + coord_dim, self.dim, kernel_size=3, padding=1)\n",
    "        concat_dim.append(self.dim)\n",
    "\n",
    "        for level in range(self.num_resolutions):\n",
    "            d_in, d_out = self.hidden_dims[level], self.hidden_dims[level + 1]\n",
    "            for block in range(num_res_blocks):\n",
    "                d_in_ = d_in if block == 0 else d_out\n",
    "                if self.resolution[level] in attn_resolutions:\n",
    "                    self.down_path.append(ResnetAttentionBlock(d_in_, d_out, self.time_emb_dim, dropout, groups))\n",
    "                else:\n",
    "                    self.down_path.append(ResnetBlock(d_in_, d_out, self.time_emb_dim, dropout, groups))\n",
    "                concat_dim.append(d_out)\n",
    "            if level != self.num_resolutions - 1:\n",
    "                self.down_path.append(downSample(d_out))\n",
    "                concat_dim.append(d_out)\n",
    "\n",
    "        mid_dim = self.hidden_dims[-1]\n",
    "        self.middle_resnet_attention = ResnetAttentionBlock(mid_dim, mid_dim, self.time_emb_dim, dropout, groups)\n",
    "        self.middle_resnet = ResnetBlock(mid_dim, mid_dim, self.time_emb_dim, dropout, groups)\n",
    "\n",
    "        for level in reversed(range(self.num_resolutions)):\n",
    "            d_out = self.hidden_dims[level + 1]\n",
    "            for block in range(num_res_blocks + 1):\n",
    "                d_in = self.hidden_dims[level + 2] if block == 0 and level != self.num_resolutions - 1 else d_out\n",
    "                d_in = d_in + concat_dim.pop()\n",
    "                if self.resolution[level] in attn_resolutions:\n",
    "                    self.up_path.append(ResnetAttentionBlock(d_in, d_out, self.time_emb_dim, dropout, groups))\n",
    "                else:\n",
    "                    self.up_path.append(ResnetBlock(d_in, d_out, self.time_emb_dim, dropout, groups))\n",
    "            if level != 0:\n",
    "                self.up_path.append(upSample(d_out))\n",
    "\n",
    "        assert not concat_dim, 'Error in concatenation between downward path and upward path.'\n",
    "\n",
    "        final_ch = self.hidden_dims[1]\n",
    "        self.final_norm = nn.GroupNorm(groups, final_ch)\n",
    "        self.final_activation = nn.SiLU()\n",
    "        self.final_conv = nn.Conv2d(final_ch, channel, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x, time, sparse_input=None, mask=None, coords=None, x_coarse=None):\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        if coords is None:\n",
    "            coords = make_coordinate_grid(B, H, W, x.device)\n",
    "        \n",
    "        coord_features = self.coord_encoder(coords)\n",
    "        coord_features = coord_features.permute(0, 3, 1, 2)\n",
    "        \n",
    "        t = self.time_mlp(time)\n",
    "        x_with_coords = torch.cat([x, coord_features], dim=1)\n",
    "        \n",
    "        concat = []\n",
    "        x = self.init_conv(x_with_coords)\n",
    "        concat.append(x)\n",
    "        \n",
    "        for layer in self.down_path:\n",
    "            if isinstance(layer, (upSample, downSample)):\n",
    "                x = layer(x)\n",
    "            else:\n",
    "                x = layer(x, t)\n",
    "            concat.append(x)\n",
    "\n",
    "        x = self.middle_resnet_attention(x, t)\n",
    "        x = self.middle_resnet(x, t)\n",
    "\n",
    "        for layer in self.up_path:\n",
    "            if not isinstance(layer, upSample):\n",
    "                x = torch.cat((x, concat.pop()), dim=1)\n",
    "            if isinstance(layer, (upSample, downSample)):\n",
    "                x = layer(x)\n",
    "            else:\n",
    "                x = layer(x, t)\n",
    "\n",
    "        x = self.final_activation(self.final_norm(x))\n",
    "        return self.final_conv(x)\n",
    "\n",
    "\n",
    "print(\"CoordinateConditionedUnet defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gaussian Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianDiffusion(nn.Module):\n",
    "    def __init__(self, model, image_size, time_step=1000, loss_type='l2'):\n",
    "        super().__init__()\n",
    "        self.unet = model\n",
    "        self.channel = self.unet.channel\n",
    "        self.device = next(self.unet.parameters()).device\n",
    "        self.image_size = image_size\n",
    "        self.time_step = time_step\n",
    "        self.loss_type = loss_type\n",
    "\n",
    "        beta = self.linear_beta_schedule()\n",
    "        alpha = 1. - beta\n",
    "        alpha_bar = torch.cumprod(alpha, dim=0)\n",
    "        alpha_bar_prev = F.pad(alpha_bar[:-1], pad=(1, 0), value=1.)\n",
    "\n",
    "        self.register_buffer('beta', beta)\n",
    "        self.register_buffer('alpha', alpha)\n",
    "        self.register_buffer('alpha_bar', alpha_bar)\n",
    "        self.register_buffer('alpha_bar_prev', alpha_bar_prev)\n",
    "\n",
    "        self.register_buffer('sqrt_alpha_bar', torch.sqrt(alpha_bar))\n",
    "        self.register_buffer('sqrt_one_minus_alpha_bar', torch.sqrt(1 - alpha_bar))\n",
    "\n",
    "        self.register_buffer('beta_tilde', beta * ((1. - alpha_bar_prev) / (1. - alpha_bar)))\n",
    "        self.register_buffer('mean_tilde_x0_coeff', beta * torch.sqrt(alpha_bar_prev) / (1 - alpha_bar))\n",
    "        self.register_buffer('mean_tilde_xt_coeff', torch.sqrt(alpha) * (1 - alpha_bar_prev) / (1 - alpha_bar))\n",
    "\n",
    "        self.register_buffer('sqrt_recip_alpha_bar', torch.sqrt(1. / alpha_bar))\n",
    "        self.register_buffer('sqrt_recip_alpha_bar_min_1', torch.sqrt(1. / alpha_bar - 1))\n",
    "        self.register_buffer('sqrt_recip_alpha', torch.sqrt(1. / alpha))\n",
    "        self.register_buffer('beta_over_sqrt_one_minus_alpha_bar', beta / torch.sqrt(1. - alpha_bar))\n",
    "\n",
    "    def q_sample(self, x0, t, noise):\n",
    "        return self.sqrt_alpha_bar[t][:, None, None, None] * x0 + \\\n",
    "               self.sqrt_one_minus_alpha_bar[t][:, None, None, None] * noise\n",
    "\n",
    "    def forward(self, img, sparse_input=None, mask=None, coords=None, perceiver_input=None, loss_mask=None):\n",
    "        b, c, h, w = img.shape\n",
    "\n",
    "        def _match_channels(t, target_C):\n",
    "            if t.size(1) == target_C:\n",
    "                return t\n",
    "            if t.size(1) == 1 and target_C > 1:\n",
    "                return t.repeat(1, target_C, 1, 1)\n",
    "            if target_C == 1 and t.size(1) > 1:\n",
    "                return t.mean(dim=1, keepdim=True)\n",
    "            raise RuntimeError(f\"Channel mismatch: have {t.size(1)}, need {target_C}\")\n",
    "\n",
    "        t = torch.randint(0, self.time_step, (b,), device=img.device).long()\n",
    "        noise = torch.randn_like(img)\n",
    "        noised_image = self.q_sample(img, t, noise)\n",
    "\n",
    "        if sparse_input is not None and mask is not None:\n",
    "            model_input = torch.cat([noised_image, sparse_input, mask], dim=1)\n",
    "        else:\n",
    "            model_input = noised_image\n",
    "\n",
    "        if coords is None:\n",
    "            coords = make_coordinate_grid(b, h, w, img.device)\n",
    "\n",
    "        predicted_noise = self.unet(model_input, t, coords=coords, x_coarse=perceiver_input)\n",
    "\n",
    "        if predicted_noise.size(1) != noise.size(1):\n",
    "            ref_C = max(predicted_noise.size(1), noise.size(1))\n",
    "            noise           = _match_channels(noise,           ref_C)\n",
    "            predicted_noise = _match_channels(predicted_noise, ref_C)\n",
    "\n",
    "        if mask is not None and mask.size(1) != predicted_noise.size(1):\n",
    "            mask = _match_channels(mask, predicted_noise.size(1))\n",
    "        if loss_mask is not None and loss_mask.size(1) != predicted_noise.size(1):\n",
    "            loss_mask = _match_channels(loss_mask, predicted_noise.size(1))\n",
    "\n",
    "        if self.loss_type == 'l1':\n",
    "            raw_loss = F.l1_loss(noise, predicted_noise, reduction='none')\n",
    "        elif self.loss_type == 'l2':\n",
    "            raw_loss = F.mse_loss(noise, predicted_noise, reduction='none')\n",
    "        elif self.loss_type == \"huber\":\n",
    "            raw_loss = F.smooth_l1_loss(noise, predicted_noise, reduction='none')\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        if loss_mask is not None:\n",
    "            lambda_cond = 0.05\n",
    "            combined_mask = (loss_mask + lambda_cond * mask).clamp(max=1.0)\n",
    "            loss = (raw_loss * combined_mask).sum() / combined_mask.sum().clamp_min(1e-8)\n",
    "        else:\n",
    "            loss = raw_loss.mean()\n",
    "        return loss\n",
    "\n",
    "    def linear_beta_schedule(self):\n",
    "        scale = 1000 / self.time_step\n",
    "        beta_start = scale * 0.0001\n",
    "        beta_end = scale * 0.02\n",
    "        return torch.linspace(beta_start, beta_end, self.time_step, dtype=torch.float32)\n",
    "\n",
    "\n",
    "print(\"GaussianDiffusion defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. DDIM Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDIM_Sampler(nn.Module):\n",
    "    def __init__(self, ddpm_diffusion_model, ddim_steps=50, eta=0.0, clip=True):\n",
    "        super().__init__()\n",
    "        self.model = ddpm_diffusion_model\n",
    "        self.ddim_steps = int(ddim_steps)\n",
    "        self.eta = float(eta)\n",
    "        self.clip = clip\n",
    "\n",
    "        with torch.no_grad():\n",
    "            ab = self.model.alpha_bar\n",
    "            self.register_buffer('tau',\n",
    "                torch.linspace(0, self.model.time_step-1, steps=self.ddim_steps, dtype=torch.long))\n",
    "            alpha_tau = ab[self.tau]\n",
    "            alpha_prev = F.pad(alpha_tau[:-1], (1,0), value=1.0)\n",
    "            self.register_buffer('alpha_tau', alpha_tau)\n",
    "            self.register_buffer('alpha_prev', alpha_prev)\n",
    "\n",
    "            sig = self.eta * torch.sqrt((1 - alpha_prev) / (1 - alpha_tau) * (1 - alpha_tau / alpha_prev))\n",
    "            coeff = torch.sqrt(1 - alpha_prev - sig**2)\n",
    "            self.register_buffer('sigma', sig)\n",
    "            self.register_buffer('coeff', coeff)\n",
    "            self.register_buffer('sqrt_alpha_prev', torch.sqrt(alpha_prev))\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def sample(self, batch_size, sparse_input, mask, target_size=None, min1to1=True):\n",
    "        device = self.model.device\n",
    "        C = self.model.channel\n",
    "        \n",
    "        if target_size is None:\n",
    "            H, W = sparse_input.shape[2:]\n",
    "        elif isinstance(target_size, int):\n",
    "            H = W = target_size\n",
    "        else:\n",
    "            H, W = target_size\n",
    "        \n",
    "        if (H, W) != sparse_input.shape[2:]:\n",
    "            sparse_target = F.interpolate(sparse_input, size=(H, W), mode='bilinear', align_corners=False)\n",
    "            mask_target = F.interpolate(mask, size=(H, W), mode='nearest')\n",
    "        else:\n",
    "            sparse_target = sparse_input\n",
    "            mask_target = mask\n",
    "        \n",
    "        coords = make_coordinate_grid(batch_size, H, W, device)\n",
    "        xt = torch.randn([batch_size, C, H, W], device=device)\n",
    "        xt = xt * (1.0 - mask_target) + sparse_target * mask_target\n",
    "        xt = soft_project(xt, sparse_target, mask_target, iters=1)\n",
    "\n",
    "        for i in reversed(range(self.ddim_steps)):\n",
    "            t = self.tau[i]\n",
    "            bt = torch.full((batch_size,), t, device=device, dtype=torch.long)\n",
    "\n",
    "            sp = sparse_target if sparse_target.size(1)==C else sparse_target.repeat(1,C,1,1)\n",
    "            mk = mask_target if mask_target.size(1)==C else mask_target.repeat(1,C,1,1)\n",
    "\n",
    "            model_in = torch.cat([xt, sp, mk], dim=1)\n",
    "            pred_eps = self.model.unet(model_in, bt, coords=coords)\n",
    "\n",
    "            x0 = self.model.sqrt_recip_alpha_bar[t] * xt - self.model.sqrt_recip_alpha_bar_min_1[t] * pred_eps\n",
    "            if self.clip:\n",
    "                x0.clamp_(-1., 1.)\n",
    "                pred_eps = (self.model.sqrt_recip_alpha_bar[t] * xt - x0) / self.model.sqrt_recip_alpha_bar_min_1[t]\n",
    "\n",
    "            mean = self.sqrt_alpha_prev[i] * x0 + self.coeff[i] * pred_eps\n",
    "            noise = torch.randn_like(xt) if i > 0 else 0.\n",
    "            xt = mean + self.sigma[i] * noise\n",
    "            xt = xt * (1.0 - mk) + sp * mk\n",
    "\n",
    "        xt.clamp_(min=-1.0, max=1.0)\n",
    "        return (xt + 1.0)/2.0 if min1to1 else xt\n",
    "\n",
    "\n",
    "print(\"DDIM_Sampler defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10_Dataset(Dataset):\n",
    "    \"\"\"CIFAR-10 dataset normalized to [-1, 1].\"\"\"\n",
    "    def __init__(self, root, train=True, augment=True):\n",
    "        tfms = []\n",
    "        if augment and train:\n",
    "            tfms += [\n",
    "                T.RandomHorizontalFlip(),\n",
    "                T.RandomCrop(32, padding=4)\n",
    "            ]\n",
    "        tfms += [\n",
    "            T.ToTensor(),\n",
    "            T.Lambda(lambda t: t*2.0 - 1.0),  # [-1,1]\n",
    "        ]\n",
    "        self.tf = T.Compose(tfms)\n",
    "        self.ds = tv.datasets.CIFAR10(root=root, train=train, download=True, transform=self.tf)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x, label = self.ds[idx]\n",
    "        return x  # [3,32,32] in [-1,1]\n",
    "\n",
    "\n",
    "class FixedMaskWrapper(Dataset):\n",
    "    \"\"\"Persistent per-item masks with fixed sparsity and 50/50 cond/supervision split.\"\"\"\n",
    "    def __init__(self, base_ds: Dataset, sparsity: float, cond_frac: float, seed: int):\n",
    "        self.base = base_ds\n",
    "        self.sparsity = sparsity\n",
    "        self.cond_frac = cond_frac\n",
    "        self.seed = seed\n",
    "        n = len(self.base)\n",
    "        x0 = self.base[0]\n",
    "        _, H, W = x0.shape\n",
    "        self.m_cond_1 = torch.zeros(n, 1, H, W, dtype=torch.float32)\n",
    "        self.m_supv_1 = torch.zeros(n, 1, H, W, dtype=torch.float32)\n",
    "        \n",
    "        print(f\"Generating fixed masks for {n} samples...\")\n",
    "        for i in range(n):\n",
    "            g = torch.Generator().manual_seed(seed + i)\n",
    "            m_full = (torch.rand(1, H, W, generator=g) < sparsity).float()\n",
    "            coords = m_full.nonzero(as_tuple=False)\n",
    "            if coords.numel() > 0:\n",
    "                idx_hw = coords[:,1]*W + coords[:,2]\n",
    "                perm = torch.randperm(idx_hw.numel(), generator=g)\n",
    "                k = int(round(cond_frac * idx_hw.numel()))\n",
    "                take = idx_hw[perm[:k]]\n",
    "                rr = (take // W).long()\n",
    "                cc = (take %  W).long()\n",
    "                m_cond = torch.zeros(1, H, W)\n",
    "                m_cond[0, rr, cc] = 1.0\n",
    "                m_supv = m_full - m_cond\n",
    "            else:\n",
    "                m_cond = torch.zeros(1, H, W)\n",
    "                m_supv = torch.zeros(1, H, W)\n",
    "            self.m_cond_1[i] = m_cond\n",
    "            self.m_supv_1[i] = m_supv\n",
    "        print(\"Mask generation complete.\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.base[idx]\n",
    "        return x, self.m_cond_1[idx], self.m_supv_1[idx]\n",
    "\n",
    "\n",
    "print(\"CIFAR-10 dataset classes defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "IMG_SIZE    = 32\n",
    "CHANNELS    = 3\n",
    "MODEL_DIM   = 64\n",
    "TIMESTEPS   = 1000\n",
    "\n",
    "# Coordinate encoding\n",
    "COORD_NUM_FREQ = 10\n",
    "COORD_SCALE    = 10.0\n",
    "\n",
    "# Data\n",
    "SPARSITY    = 0.40  # 40% of pixels observed\n",
    "COND_FRAC   = 0.50  # 50% for conditioning, 50% for supervision\n",
    "MASK_SEED   = 42\n",
    "\n",
    "BATCH_TRAIN = 128  # CIFAR-10 has more samples, can use larger batch\n",
    "BATCH_TEST  = 64\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "# Training\n",
    "STEPS       = 100_000  # CIFAR-10 needs more steps for convergence\n",
    "PRINT_EVERY = 100\n",
    "SAVE_EVERY  = 5_000\n",
    "VIS_EVERY   = 2_000\n",
    "\n",
    "# Evaluation\n",
    "N_VIS           = 8\n",
    "DDIM_STEPS_VIS  = 50\n",
    "DDIM_STEPS_EVAL = 50\n",
    "MAX_TEST_BATCHES= 10\n",
    "\n",
    "# Directories\n",
    "DATA_ROOT   = \"./data_cifar10\"\n",
    "CKPT_DIR    = \"./kevinref_inc_1/cifar10_ckpts\"\n",
    "OUT_DIR     = \"./kevinref_inc_1/cifar10_outputs\"\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Attention\n",
    "def attn_resolutions_for(image_size, levels=(1,), dim_mult=(1,2,2,2)):\n",
    "    res_list = [int(image_size / (2 ** i)) for i in range(len(dim_mult))]\n",
    "    return tuple(res_list[i] for i in levels)\n",
    "\n",
    "ATTN_LEVELS = (1,)  # 16×16 attention\n",
    "attn_res = attn_resolutions_for(IMG_SIZE, ATTN_LEVELS)\n",
    "\n",
    "print(f\"Hyperparameters set. Attention at resolutions: {attn_res}\")\n",
    "print(f\"Training on CIFAR-10 with {SPARSITY*100:.0f}% sparsity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = CoordinateConditionedUnet(\n",
    "    dim=MODEL_DIM,\n",
    "    image_size=IMG_SIZE,\n",
    "    dim_multiply=(1,2,2,2),\n",
    "    channel=CHANNELS,\n",
    "    num_res_blocks=2,\n",
    "    attn_resolutions=attn_res,\n",
    "    dropout=0.0,\n",
    "    device=DEVICE,\n",
    "    groups=32,\n",
    "    coord_num_frequencies=COORD_NUM_FREQ,\n",
    "    coord_scale=COORD_SCALE\n",
    ").to(DEVICE)\n",
    "\n",
    "ddpm = GaussianDiffusion(\n",
    "    model=net,\n",
    "    image_size=IMG_SIZE,\n",
    "    time_step=TIMESTEPS,\n",
    "    loss_type='l2'\n",
    ").to(DEVICE)\n",
    "\n",
    "opt = torch.optim.AdamW(ddpm.parameters(), lr=2e-4, weight_decay=1e-4)\n",
    "\n",
    "num_params = sum(p.numel() for p in net.parameters())\n",
    "print(f\"Model built with {num_params/1e6:.2f}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data (50K images)\n",
    "base_train = CIFAR10_Dataset(DATA_ROOT, train=True, augment=True)\n",
    "train_ds   = FixedMaskWrapper(base_train, sparsity=SPARSITY, cond_frac=COND_FRAC, seed=MASK_SEED)\n",
    "\n",
    "# Test data (10K images)\n",
    "base_test = CIFAR10_Dataset(DATA_ROOT, train=False, augment=False)\n",
    "test_ds   = FixedMaskWrapper(base_test, sparsity=SPARSITY, cond_frac=COND_FRAC, seed=MASK_SEED+999)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_TRAIN, shuffle=True,\n",
    "                      num_workers=NUM_WORKERS, pin_memory=True, drop_last=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=BATCH_TEST, shuffle=False,\n",
    "                     num_workers=NUM_WORKERS, pin_memory=True, drop_last=False)\n",
    "\n",
    "# Fixed visualization batch\n",
    "vis_batch = next(iter(test_dl))\n",
    "vis_x, vis_m_cond_1, _ = [v[:N_VIS] for v in vis_batch]\n",
    "\n",
    "print(f\"Training samples: {len(train_ds)}, Test samples: {len(test_ds)}\")\n",
    "print(f\"Batches per epoch: {len(train_dl)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpm.train()\n",
    "ddim_vis = DDIM_Sampler(ddpm, ddim_steps=DDIM_STEPS_VIS, eta=0.0, clip=True)\n",
    "\n",
    "it = iter(train_dl)\n",
    "for step in range(1, STEPS + 1):\n",
    "    try:\n",
    "        x, m_cond_1, m_supv_1 = next(it)\n",
    "    except StopIteration:\n",
    "        it = iter(train_dl)\n",
    "        x, m_cond_1, m_supv_1 = next(it)\n",
    "\n",
    "    x = x.to(DEVICE)\n",
    "    m_cond_1 = m_cond_1.to(DEVICE)\n",
    "    m_supv_1 = m_supv_1.to(DEVICE)\n",
    "\n",
    "    B, C, H, W = x.shape\n",
    "    m_cond = m_cond_1.repeat(1, C, 1, 1)\n",
    "    m_supv = m_supv_1.repeat(1, C, 1, 1)\n",
    "    x_sparse = x * m_cond\n",
    "\n",
    "    coords = make_coordinate_grid(B, H, W, DEVICE)\n",
    "\n",
    "    loss = ddpm(img=x, sparse_input=x_sparse, mask=m_cond, coords=coords,\n",
    "                perceiver_input=None, loss_mask=m_supv)\n",
    "\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(ddpm.parameters(), 1.0)\n",
    "    opt.step()\n",
    "\n",
    "    if step % PRINT_EVERY == 0:\n",
    "        epoch = step * BATCH_TRAIN / len(train_ds)\n",
    "        print(f\"[{step}/{STEPS}] epoch={epoch:.2f} loss={loss.item():.4f}\")\n",
    "\n",
    "    # Periodic visualization\n",
    "    if step % VIS_EVERY == 0 or step == 1:\n",
    "        ddpm.eval()\n",
    "        with torch.inference_mode():\n",
    "            vx = vis_x.to(DEVICE)\n",
    "            vm = vis_m_cond_1.to(DEVICE)\n",
    "            vm3 = vm.repeat(1, CHANNELS, 1, 1)\n",
    "            v_sparse = vx * vm3\n",
    "            \n",
    "            recon01 = ddim_vis.sample(batch_size=vx.size(0),\n",
    "                                      sparse_input=v_sparse,\n",
    "                                      mask=vm3,\n",
    "                                      target_size=32,\n",
    "                                      min1to1=True)\n",
    "            \n",
    "            gt01 = (vx + 1.0)/2.0\n",
    "            sp01 = (v_sparse + 1.0)/2.0\n",
    "            out_path = os.path.join(OUT_DIR, f\"train_vis_step{step}_32x32.png\")\n",
    "            save_grid01([gt01.cpu(), sp01.cpu(), recon01.cpu()], out_path, nrow=N_VIS)\n",
    "            print(f\"Saved vis → {out_path}\")\n",
    "        ddpm.train()\n",
    "\n",
    "    if step % SAVE_EVERY == 0 or step == STEPS:\n",
    "        torch.save({'net': net.state_dict()}, os.path.join(CKPT_DIR, f\"net_step{step}.pt\"))\n",
    "        print(f\"Checkpoint saved at step {step}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Zero-Shot Multi-Resolution Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint\n",
    "ckpt_path = os.path.join(CKPT_DIR, f\"net_step{STEPS}.pt\")\n",
    "state = torch.load(ckpt_path, map_location='cpu')\n",
    "net.load_state_dict(state['net'], strict=True)\n",
    "ddpm.eval()\n",
    "\n",
    "ddim_eval = DDIM_Sampler(ddpm, ddim_steps=DDIM_STEPS_EVAL, eta=0.0, clip=True)\n",
    "\n",
    "print(\"Model loaded. Ready for zero-shot super-resolution inference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set at multiple resolutions\n",
    "test_batch = next(iter(test_dl))\n",
    "x_test, m_cond_1_test, _ = test_batch\n",
    "x_test = x_test[:N_VIS].to(DEVICE)\n",
    "m_cond_1_test = m_cond_1_test[:N_VIS].to(DEVICE)\n",
    "\n",
    "m_cond_test = m_cond_1_test.repeat(1, CHANNELS, 1, 1)\n",
    "x_sparse_test = x_test * m_cond_test\n",
    "\n",
    "print(\"Testing zero-shot super-resolution at multiple resolutions...\")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    print(\"Reconstructing at 32×32...\")\n",
    "    recon_32 = ddim_eval.sample(\n",
    "        batch_size=x_test.size(0),\n",
    "        sparse_input=x_sparse_test,\n",
    "        mask=m_cond_test,\n",
    "        target_size=32,\n",
    "        min1to1=True\n",
    "    )\n",
    "    \n",
    "    print(\"Zero-shot super-resolution to 64×64...\")\n",
    "    recon_64 = ddim_eval.sample(\n",
    "        batch_size=x_test.size(0),\n",
    "        sparse_input=x_sparse_test,\n",
    "        mask=m_cond_test,\n",
    "        target_size=64,\n",
    "        min1to1=True\n",
    "    )\n",
    "    \n",
    "    print(\"Zero-shot super-resolution to 96×96...\")\n",
    "    recon_96 = ddim_eval.sample(\n",
    "        batch_size=x_test.size(0),\n",
    "        sparse_input=x_sparse_test,\n",
    "        mask=m_cond_test,\n",
    "        target_size=96,\n",
    "        min1to1=True\n",
    "    )\n",
    "\n",
    "# Save results\n",
    "gt01 = (x_test + 1.0) / 2.0\n",
    "sp01 = (x_sparse_test + 1.0) / 2.0\n",
    "\n",
    "gt64 = F.interpolate(gt01, size=(64, 64), mode='bicubic', align_corners=False)\n",
    "gt96 = F.interpolate(gt01, size=(96, 96), mode='bicubic', align_corners=False)\n",
    "sp64 = F.interpolate(sp01, size=(64, 64), mode='bilinear', align_corners=False)\n",
    "sp96 = F.interpolate(sp01, size=(96, 96), mode='bilinear', align_corners=False)\n",
    "\n",
    "save_grid01([gt01.cpu(), sp01.cpu(), recon_32.cpu()],\n",
    "            os.path.join(OUT_DIR, \"eval_32x32.png\"), nrow=N_VIS)\n",
    "save_grid01([gt64.cpu(), sp64.cpu(), recon_64.cpu()],\n",
    "            os.path.join(OUT_DIR, \"eval_64x64.png\"), nrow=N_VIS)\n",
    "save_grid01([gt96.cpu(), sp96.cpu(), recon_96.cpu()],\n",
    "            os.path.join(OUT_DIR, \"eval_96x96.png\"), nrow=N_VIS)\n",
    "\n",
    "print(\"Results saved:\")\n",
    "print(f\"  - {OUT_DIR}/eval_32x32.png (native resolution)\")\n",
    "print(f\"  - {OUT_DIR}/eval_64x64.png (zero-shot 2× super-resolution)\")\n",
    "print(f\"  - {OUT_DIR}/eval_96x96.png (zero-shot 3× super-resolution)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results in notebook\n",
    "fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "\n",
    "for i in range(3):\n",
    "    idx = i\n",
    "    \n",
    "    axes[0, i].imshow(recon_32[idx].cpu().permute(1, 2, 0).numpy())\n",
    "    axes[0, i].set_title(f\"32×32 (sample {idx})\")\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    axes[1, i].imshow(recon_64[idx].cpu().permute(1, 2, 0).numpy())\n",
    "    axes[1, i].set_title(f\"64×64 (sample {idx})\")\n",
    "    axes[1, i].axis('off')\n",
    "    \n",
    "    axes[2, i].imshow(recon_96[idx].cpu().permute(1, 2, 0).numpy())\n",
    "    axes[2, i].set_title(f\"96×96 (sample {idx})\")\n",
    "    axes[2, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"multi_resolution_comparison.png\"), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"Zero-shot super-resolution complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psnr_batch(pred, target):\n",
    "    mse = F.mse_loss(pred, target, reduction='mean')\n",
    "    if mse < 1e-10:\n",
    "        return 99.0\n",
    "    return 10.0 * math.log10(1.0 / mse.item())\n",
    "\n",
    "psnr_32 = psnr_batch(recon_32, gt01)\n",
    "psnr_64 = psnr_batch(recon_64, gt64)\n",
    "psnr_96 = psnr_batch(recon_96, gt96)\n",
    "\n",
    "print(f\"\\nPSNR Results (on {N_VIS} CIFAR-10 test samples):\")\n",
    "print(f\"  32×32: {psnr_32:.2f} dB\")\n",
    "print(f\"  64×64: {psnr_64:.2f} dB (zero-shot)\")\n",
    "print(f\"  96×96: {psnr_96:.2f} dB (zero-shot)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Quantitative Evaluation (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on larger test set\n",
    "print(\"Running evaluation on test set...\")\n",
    "all_psnr_32, all_psnr_64, all_psnr_96 = [], [], []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for batch_idx, (x, m_cond_1, _) in enumerate(test_dl):\n",
    "        if batch_idx >= MAX_TEST_BATCHES:\n",
    "            break\n",
    "        \n",
    "        x = x.to(DEVICE)\n",
    "        m_cond_1 = m_cond_1.to(DEVICE)\n",
    "        m_cond = m_cond_1.repeat(1, CHANNELS, 1, 1)\n",
    "        x_sparse = x * m_cond\n",
    "        \n",
    "        # Reconstruct at 32×32\n",
    "        r32 = ddim_eval.sample(x.size(0), x_sparse, m_cond, target_size=32, min1to1=True)\n",
    "        gt32 = (x + 1.0) / 2.0\n",
    "        all_psnr_32.append(psnr_batch(r32, gt32))\n",
    "        \n",
    "        # Reconstruct at 64×64\n",
    "        r64 = ddim_eval.sample(x.size(0), x_sparse, m_cond, target_size=64, min1to1=True)\n",
    "        gt64 = F.interpolate(gt32, size=(64, 64), mode='bicubic', align_corners=False)\n",
    "        all_psnr_64.append(psnr_batch(r64, gt64))\n",
    "        \n",
    "        # Reconstruct at 96×96\n",
    "        r96 = ddim_eval.sample(x.size(0), x_sparse, m_cond, target_size=96, min1to1=True)\n",
    "        gt96 = F.interpolate(gt32, size=(96, 96), mode='bicubic', align_corners=False)\n",
    "        all_psnr_96.append(psnr_batch(r96, gt96))\n",
    "        \n",
    "        print(f\"Batch {batch_idx+1}/{MAX_TEST_BATCHES} complete\")\n",
    "\n",
    "mean_psnr_32 = sum(all_psnr_32) / len(all_psnr_32)\n",
    "mean_psnr_64 = sum(all_psnr_64) / len(all_psnr_64)\n",
    "mean_psnr_96 = sum(all_psnr_96) / len(all_psnr_96)\n",
    "\n",
    "print(f\"\\nMean PSNR over {len(all_psnr_32)} batches:\")\n",
    "print(f\"  32×32: {mean_psnr_32:.2f} dB\")\n",
    "print(f\"  64×64: {mean_psnr_64:.2f} dB (zero-shot)\")\n",
    "print(f\"  96×96: {mean_psnr_96:.2f} dB (zero-shot)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**CIFAR-10 Implementation:**\n",
    "- ✅ 50K training images + 10K test images\n",
    "- ✅ Single-stage training on 32×32 with coordinate embeddings\n",
    "- ✅ Zero-shot inference at 32×32, 64×64, 96×96\n",
    "- ✅ 40% sparsity: 20% conditioning + 20% supervision\n",
    "\n",
    "**Differences from STL-10:**\n",
    "- Larger dataset (60K vs 13K images)\n",
    "- Larger batch size (128 vs 64)\n",
    "- More training steps (100K vs 50K)\n",
    "- More diverse content (10 classes)\n",
    "\n",
    "**Expected Performance:**\n",
    "- Better generalization due to more training data\n",
    "- Faster convergence per epoch\n",
    "- More robust zero-shot super-resolution\n",
    "\n",
    "**Next Steps:**\n",
    "- Compare results with STL-10 version\n",
    "- Experiment with different sparsity levels\n",
    "- Try class-conditional generation (optional)\n",
    "- Test on higher resolutions (128×128, 256×256)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
