{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coordinate-Conditioned Diffusion for Zero-Shot Super-Resolution\n",
    "\n",
    "**Goal**: Train once on 32×32, infer at 32×32, 64×64, 96×96 for unseen samples.\n",
    "\n",
    "**Key Innovation**: Fourier coordinate embeddings teach the model continuous field representation instead of discrete pixel reconstruction.\n",
    "\n",
    "**Based on**: `kevinref/goodref.py` with coordinate conditioning modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, random, sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision as tv\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fourier Coordinate Encoding\n",
    "\n",
    "Maps continuous (x, y) coordinates to high-dimensional features for resolution-agnostic processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FourierCoordinateEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Fourier features for continuous spatial coordinates.\n",
    "    Input: (B, H, W, 2) normalized coordinates [0, 1]\n",
    "    Output: (B, H, W, encoding_dim) where encoding_dim = 4 * num_frequencies\n",
    "    \n",
    "    Encoding: [sin(2^0*π*scale*x), cos(2^0*π*scale*x), ..., sin(2^k*π*scale*y), cos(2^k*π*scale*y)]\n",
    "    \"\"\"\n",
    "    def __init__(self, num_frequencies=10, scale=10.0):\n",
    "        super().__init__()\n",
    "        self.num_frequencies = num_frequencies\n",
    "        self.scale = scale\n",
    "        self.encoding_dim = 4 * num_frequencies  # 2 coords × 2 (sin/cos) × num_freq\n",
    "        \n",
    "    def forward(self, coords):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            coords: (B, H, W, 2) normalized [0, 1]\n",
    "        Returns:\n",
    "            features: (B, H, W, encoding_dim)\n",
    "        \"\"\"\n",
    "        B, H, W, _ = coords.shape\n",
    "        \n",
    "        # Split x and y coordinates\n",
    "        x = coords[..., 0:1]  # (B, H, W, 1)\n",
    "        y = coords[..., 1:2]  # (B, H, W, 1)\n",
    "        \n",
    "        # Generate frequency bands: [2^0, 2^1, ..., 2^(k-1)]\n",
    "        freq_bands = 2.0 ** torch.arange(self.num_frequencies, device=coords.device, dtype=torch.float32)\n",
    "        freq_bands = freq_bands * math.pi * self.scale  # (num_frequencies,)\n",
    "        \n",
    "        # Compute sin/cos for x coordinate at all frequencies\n",
    "        x_freq = x * freq_bands.view(1, 1, 1, -1)  # (B, H, W, num_freq)\n",
    "        x_features = torch.cat([torch.sin(x_freq), torch.cos(x_freq)], dim=-1)  # (B, H, W, 2*num_freq)\n",
    "        \n",
    "        # Compute sin/cos for y coordinate at all frequencies\n",
    "        y_freq = y * freq_bands.view(1, 1, 1, -1)  # (B, H, W, num_freq)\n",
    "        y_features = torch.cat([torch.sin(y_freq), torch.cos(y_freq)], dim=-1)  # (B, H, W, 2*num_freq)\n",
    "        \n",
    "        # Concatenate all features\n",
    "        features = torch.cat([x_features, y_features], dim=-1)  # (B, H, W, 4*num_freq)\n",
    "        \n",
    "        return features\n",
    "\n",
    "\n",
    "def make_coordinate_grid(batch_size, height, width, device):\n",
    "    \"\"\"\n",
    "    Create normalized coordinate grid [0, 1]².\n",
    "    \n",
    "    Args:\n",
    "        batch_size: int\n",
    "        height, width: int\n",
    "        device: torch.device\n",
    "    \n",
    "    Returns:\n",
    "        coords: (batch_size, height, width, 2) normalized to [0, 1]\n",
    "    \"\"\"\n",
    "    # Create normalized coordinates: [0, 1]\n",
    "    y_coords = torch.linspace(0, 1, height, device=device)\n",
    "    x_coords = torch.linspace(0, 1, width, device=device)\n",
    "    \n",
    "    # Create meshgrid\n",
    "    yy, xx = torch.meshgrid(y_coords, x_coords, indexing='ij')\n",
    "    \n",
    "    # Stack and expand for batch\n",
    "    coords = torch.stack([xx, yy], dim=-1)  # (H, W, 2)\n",
    "    coords = coords.unsqueeze(0).expand(batch_size, -1, -1, -1)  # (B, H, W, 2)\n",
    "    \n",
    "    return coords\n",
    "\n",
    "\n",
    "# Test coordinate encoding\n",
    "print(\"Testing Fourier Coordinate Encoding...\")\n",
    "coord_encoder = FourierCoordinateEncoding(num_frequencies=10, scale=10.0)\n",
    "test_coords = make_coordinate_grid(2, 32, 32, DEVICE)\n",
    "test_features = coord_encoder(test_coords)\n",
    "print(f\"Input coords shape: {test_coords.shape}\")  # (2, 32, 32, 2)\n",
    "print(f\"Output features shape: {test_features.shape}\")  # (2, 32, 32, 40)\n",
    "print(f\"Encoding dimension: {coord_encoder.encoding_dim}\")  # 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Utilities (from goodref.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal positional/time embedding -> (B, dim) given integer timesteps.\"\"\"\n",
    "    def __init__(self, dim, max_period=10000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.max_period = max_period\n",
    "\n",
    "    def forward(self, t: torch.Tensor):\n",
    "        if t.dtype != torch.float32:\n",
    "            t = t.float()\n",
    "        half = self.dim // 2\n",
    "        device = t.device\n",
    "        freqs = torch.exp(-math.log(self.max_period) * torch.arange(0, half, device=device).float() / half)\n",
    "        args = t[:, None] * freqs[None, :]\n",
    "        emb = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "        if self.dim % 2 == 1:\n",
    "            emb = torch.cat([emb, torch.zeros_like(emb[:, :1])], dim=-1)\n",
    "        return emb  # (B, dim)\n",
    "\n",
    "\n",
    "def hw_to_seq(t):  # (B, C, H, W) -> (B, HW, C)\n",
    "    return t.flatten(2).transpose(1, 2)\n",
    "\n",
    "\n",
    "def seq_to_hw(t, h, w):  # (B, HW, C) -> (B, C, H, W)\n",
    "    return t.transpose(1, 2).reshape(t.size(0), -1, h, w)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def soft_project(x, obs, mask, kernel_size=3, iters=1):\n",
    "    \"\"\"Enforce observed pixels exactly.\"\"\"\n",
    "    for _ in range(iters):\n",
    "        x = x * (1.0 - mask) + obs * mask\n",
    "    return x\n",
    "\n",
    "\n",
    "def to_img01(t):\n",
    "    return ((t.clamp(-1,1) + 1.0)/2.0).detach().cpu()\n",
    "\n",
    "\n",
    "def save_grid01(tensors01, path, nrow=6, pad=2):\n",
    "    \"\"\"\n",
    "    tensors01: list of [B,3,H,W] in [0,1] (same B).\n",
    "    Saves a vertical stack of grids (GT / Sparse / Recon).\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for t in tensors01:\n",
    "        grid = make_grid(t, nrow=nrow, padding=pad)\n",
    "        rows.append(grid)\n",
    "    big = torch.cat(rows, dim=1)  # stack vertically\n",
    "    save_image(big, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Coordinate-Conditioned UNet\n",
    "\n",
    "**Key modifications from goodref.py:**\n",
    "1. Input channels expanded: `channel * 3 + coord_encoding_dim`\n",
    "2. Forward accepts `coords` parameter\n",
    "3. Coordinate features concatenated with image, sparse, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim, dim_out=None, time_emb_dim=None, dropout=None, groups=32):\n",
    "        super().__init__()\n",
    "        self.dim, self.dim_out = dim, dim_out\n",
    "        dim_out = dim if dim_out is None else dim_out\n",
    "        self.norm1 = nn.GroupNorm(num_groups=groups, num_channels=dim)\n",
    "        self.activation1 = nn.SiLU()\n",
    "        self.conv1 = nn.Conv2d(dim, dim_out, kernel_size=3, padding=1)\n",
    "        self.block1 = nn.Sequential(self.norm1, self.activation1, self.conv1)\n",
    "\n",
    "        self.mlp = nn.Sequential(nn.SiLU(), nn.Linear(time_emb_dim, dim_out)) if time_emb_dim is not None else None\n",
    "\n",
    "        self.norm2 = nn.GroupNorm(num_groups=groups, num_channels=dim_out)\n",
    "        self.activation2 = nn.SiLU()\n",
    "        self.dropout = nn.Dropout(dropout) if dropout is not None and dropout > 0 else nn.Identity()\n",
    "        self.conv2 = nn.Conv2d(dim_out, dim_out, kernel_size=3, padding=1)\n",
    "        self.block2 = nn.Sequential(self.norm2, self.activation2, self.dropout, self.conv2)\n",
    "\n",
    "        self.residual_conv = nn.Conv2d(dim, dim_out, kernel_size=1) if dim != dim_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x, time_emb=None):\n",
    "        h = self.block1(x)\n",
    "        if time_emb is not None and self.mlp is not None:\n",
    "            h = h + self.mlp(time_emb)[..., None, None]\n",
    "        h = self.block2(h)\n",
    "        return h + self.residual_conv(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, groups=32):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.scale = dim ** (-0.5)\n",
    "        self.norm = nn.GroupNorm(num_groups=groups, num_channels=dim)\n",
    "        self.to_qkv = nn.Conv2d(dim, dim * 3, kernel_size=1)\n",
    "        self.to_out = nn.Conv2d(dim, dim, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(self.norm(x)).chunk(3, dim=1)\n",
    "        q, k, v = [hw_to_seq(t) for t in qkv]  # (B, HW, C)\n",
    "        sim = torch.einsum('bic,bjc->bij', q, k) * self.scale\n",
    "        attn = sim.softmax(dim=-1)\n",
    "        out = torch.einsum('bij,bjc->bic', attn, v)\n",
    "        out = seq_to_hw(out, h, w)\n",
    "        return self.to_out(out) + x\n",
    "\n",
    "\n",
    "class ResnetAttentionBlock(nn.Module):\n",
    "    def __init__(self, dim, dim_out=None, time_emb_dim=None, dropout=None, groups=32):\n",
    "        super().__init__()\n",
    "        self.resnet = ResnetBlock(dim, dim_out, time_emb_dim, dropout, groups)\n",
    "        self.attention = Attention(dim_out if dim_out is not None else dim, groups)\n",
    "\n",
    "    def forward(self, x, time_emb=None):\n",
    "        x = self.resnet(x, time_emb)\n",
    "        return self.attention(x)\n",
    "\n",
    "\n",
    "class downSample(nn.Module):\n",
    "    def __init__(self, dim_in):\n",
    "        super().__init__()\n",
    "        self.downsameple = nn.Conv2d(dim_in, dim_in, kernel_size=3, stride=2, padding=1)\n",
    "    def forward(self, x):\n",
    "        return self.downsameple(x)\n",
    "\n",
    "\n",
    "class upSample(nn.Module):\n",
    "    def __init__(self, dim_in):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Sequential(nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "                                      nn.Conv2d(dim_in, dim_in, kernel_size=3, padding=1))\n",
    "    def forward(self, x):\n",
    "        return self.upsample(x)\n",
    "\n",
    "\n",
    "class CoordinateConditionedUnet(nn.Module):\n",
    "    \"\"\"\n",
    "    UNet with coordinate conditioning for zero-shot super-resolution.\n",
    "    \n",
    "    Key differences from goodref.py Unet:\n",
    "    1. Accepts coordinate embeddings in forward()\n",
    "    2. Input channels: channel * 3 + coord_encoding_dim (was channel * 3)\n",
    "    3. Creates coordinate encoder for Fourier features\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, image_size, dim_multiply=(1, 2, 4, 8), channel=3, num_res_blocks=2,\n",
    "                 attn_resolutions=(16,), dropout=0.0, device='cuda', groups=32,\n",
    "                 coord_num_frequencies=10, coord_scale=10.0):\n",
    "        super().__init__()\n",
    "        assert dim % groups == 0, 'parameter [groups] must be divisible by parameter [dim]'\n",
    "\n",
    "        self.dim = dim\n",
    "        self.channel = channel\n",
    "        self.time_emb_dim = 4 * self.dim\n",
    "        self.num_resolutions = len(dim_multiply)\n",
    "        self.device = device\n",
    "        self.resolution = [int(image_size / (2 ** i)) for i in range(self.num_resolutions)]\n",
    "        self.hidden_dims = [self.dim, *map(lambda x: x * self.dim, dim_multiply)]\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "\n",
    "        # Time embedding\n",
    "        positional_encoding = PositionalEncoding(self.dim)\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            positional_encoding, nn.Linear(self.dim, self.time_emb_dim),\n",
    "            nn.SiLU(), nn.Linear(self.time_emb_dim, self.time_emb_dim)\n",
    "        )\n",
    "        \n",
    "        # Coordinate encoder\n",
    "        self.coord_encoder = FourierCoordinateEncoding(\n",
    "            num_frequencies=coord_num_frequencies,\n",
    "            scale=coord_scale\n",
    "        )\n",
    "        coord_dim = self.coord_encoder.encoding_dim\n",
    "\n",
    "        self.down_path = nn.ModuleList([])\n",
    "        self.up_path = nn.ModuleList([])\n",
    "        concat_dim = []\n",
    "\n",
    "        # KEY CHANGE: Input is concatenated [xt, sparse, mask, coord_features]\n",
    "        # Original: channel * 3\n",
    "        # Modified: channel * 3 + coord_encoding_dim\n",
    "        self.init_conv = nn.Conv2d(channel * 3 + coord_dim, self.dim, kernel_size=3, padding=1)\n",
    "        concat_dim.append(self.dim)\n",
    "\n",
    "        # Downward path\n",
    "        for level in range(self.num_resolutions):\n",
    "            d_in, d_out = self.hidden_dims[level], self.hidden_dims[level + 1]\n",
    "            for block in range(num_res_blocks):\n",
    "                d_in_ = d_in if block == 0 else d_out\n",
    "                if self.resolution[level] in attn_resolutions:\n",
    "                    self.down_path.append(ResnetAttentionBlock(d_in_, d_out, self.time_emb_dim, dropout, groups))\n",
    "                else:\n",
    "                    self.down_path.append(ResnetBlock(d_in_, d_out, self.time_emb_dim, dropout, groups))\n",
    "                concat_dim.append(d_out)\n",
    "            if level != self.num_resolutions - 1:\n",
    "                self.down_path.append(downSample(d_out))\n",
    "                concat_dim.append(d_out)\n",
    "\n",
    "        # Middle\n",
    "        mid_dim = self.hidden_dims[-1]\n",
    "        self.middle_resnet_attention = ResnetAttentionBlock(mid_dim, mid_dim, self.time_emb_dim, dropout, groups)\n",
    "        self.middle_resnet = ResnetBlock(mid_dim, mid_dim, self.time_emb_dim, dropout, groups)\n",
    "\n",
    "        # Upward path\n",
    "        for level in reversed(range(self.num_resolutions)):\n",
    "            d_out = self.hidden_dims[level + 1]\n",
    "            for block in range(num_res_blocks + 1):\n",
    "                d_in = self.hidden_dims[level + 2] if block == 0 and level != self.num_resolutions - 1 else d_out\n",
    "                d_in = d_in + concat_dim.pop()\n",
    "                if self.resolution[level] in attn_resolutions:\n",
    "                    self.up_path.append(ResnetAttentionBlock(d_in, d_out, self.time_emb_dim, dropout, groups))\n",
    "                else:\n",
    "                    self.up_path.append(ResnetBlock(d_in, d_out, self.time_emb_dim, dropout, groups))\n",
    "            if level != 0:\n",
    "                self.up_path.append(upSample(d_out))\n",
    "\n",
    "        assert not concat_dim, 'Error in concatenation between downward path and upward path.'\n",
    "\n",
    "        # Final output\n",
    "        final_ch = self.hidden_dims[1]\n",
    "        self.final_norm = nn.GroupNorm(groups, final_ch)\n",
    "        self.final_activation = nn.SiLU()\n",
    "        self.final_conv = nn.Conv2d(final_ch, channel, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x, time, sparse_input=None, mask=None, coords=None, x_coarse=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, C, H, W) - noisy image (can be any H, W)\n",
    "            time: (B,) - timesteps\n",
    "            sparse_input: (B, C, H, W) - sparse observations\n",
    "            mask: (B, C, H, W) - observation mask\n",
    "            coords: (B, H, W, 2) - normalized coordinates [0, 1], optional\n",
    "            x_coarse: unused (for compatibility)\n",
    "        \n",
    "        Returns:\n",
    "            predicted_noise: (B, C, H, W)\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        # Generate coordinates if not provided\n",
    "        if coords is None:\n",
    "            coords = make_coordinate_grid(B, H, W, x.device)\n",
    "        \n",
    "        # Encode coordinates: (B, H, W, 2) → (B, H, W, coord_dim)\n",
    "        coord_features = self.coord_encoder(coords)  # (B, H, W, 40)\n",
    "        \n",
    "        # Reshape to channel-first: (B, H, W, coord_dim) → (B, coord_dim, H, W)\n",
    "        coord_features = coord_features.permute(0, 3, 1, 2)\n",
    "        \n",
    "        # Time embedding\n",
    "        t = self.time_mlp(time)\n",
    "\n",
    "        # Concatenate input with coordinate features\n",
    "        # x is already concatenated as [xt, sparse_input, mask] from caller\n",
    "        # We need to add coordinate features\n",
    "        x_with_coords = torch.cat([x, coord_features], dim=1)\n",
    "        \n",
    "        # Initial convolution\n",
    "        concat = []\n",
    "        x = self.init_conv(x_with_coords)\n",
    "        concat.append(x)\n",
    "        \n",
    "        # Downward path\n",
    "        for layer in self.down_path:\n",
    "            if isinstance(layer, (upSample, downSample)):\n",
    "                x = layer(x)\n",
    "            else:\n",
    "                x = layer(x, t)\n",
    "            concat.append(x)\n",
    "\n",
    "        # Middle\n",
    "        x = self.middle_resnet_attention(x, t)\n",
    "        x = self.middle_resnet(x, t)\n",
    "\n",
    "        # Upward path\n",
    "        for layer in self.up_path:\n",
    "            if not isinstance(layer, upSample):\n",
    "                x = torch.cat((x, concat.pop()), dim=1)\n",
    "            if isinstance(layer, (upSample, downSample)):\n",
    "                x = layer(x)\n",
    "            else:\n",
    "                x = layer(x, t)\n",
    "\n",
    "        # Final output\n",
    "        x = self.final_activation(self.final_norm(x))\n",
    "        return self.final_conv(x)\n",
    "\n",
    "\n",
    "print(\"CoordinateConditionedUnet defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gaussian Diffusion with Coordinate Conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianDiffusion(nn.Module):\n",
    "    def __init__(self, model, image_size, time_step=1000, loss_type='l2'):\n",
    "        super().__init__()\n",
    "        self.unet = model\n",
    "        self.channel = self.unet.channel\n",
    "        self.device = next(self.unet.parameters()).device\n",
    "        self.image_size = image_size\n",
    "        self.time_step = time_step\n",
    "        self.loss_type = loss_type\n",
    "\n",
    "        beta = self.linear_beta_schedule()\n",
    "        alpha = 1. - beta\n",
    "        alpha_bar = torch.cumprod(alpha, dim=0)\n",
    "        alpha_bar_prev = F.pad(alpha_bar[:-1], pad=(1, 0), value=1.)\n",
    "\n",
    "        self.register_buffer('beta', beta)\n",
    "        self.register_buffer('alpha', alpha)\n",
    "        self.register_buffer('alpha_bar', alpha_bar)\n",
    "        self.register_buffer('alpha_bar_prev', alpha_bar_prev)\n",
    "\n",
    "        self.register_buffer('sqrt_alpha_bar', torch.sqrt(alpha_bar))\n",
    "        self.register_buffer('sqrt_one_minus_alpha_bar', torch.sqrt(1 - alpha_bar))\n",
    "\n",
    "        self.register_buffer('beta_tilde', beta * ((1. - alpha_bar_prev) / (1. - alpha_bar)))\n",
    "        self.register_buffer('mean_tilde_x0_coeff', beta * torch.sqrt(alpha_bar_prev) / (1 - alpha_bar))\n",
    "        self.register_buffer('mean_tilde_xt_coeff', torch.sqrt(alpha) * (1 - alpha_bar_prev) / (1 - alpha_bar))\n",
    "\n",
    "        self.register_buffer('sqrt_recip_alpha_bar', torch.sqrt(1. / alpha_bar))\n",
    "        self.register_buffer('sqrt_recip_alpha_bar_min_1', torch.sqrt(1. / alpha_bar - 1))\n",
    "        self.register_buffer('sqrt_recip_alpha', torch.sqrt(1. / alpha))\n",
    "        self.register_buffer('beta_over_sqrt_one_minus_alpha_bar', beta / torch.sqrt(1. - alpha_bar))\n",
    "\n",
    "    def q_sample(self, x0, t, noise):\n",
    "        return self.sqrt_alpha_bar[t][:, None, None, None] * x0 + \\\n",
    "               self.sqrt_one_minus_alpha_bar[t][:, None, None, None] * noise\n",
    "\n",
    "    def forward(self, img, sparse_input=None, mask=None, coords=None, perceiver_input=None, loss_mask=None):\n",
    "        \"\"\"\n",
    "        Training forward pass with coordinate conditioning.\n",
    "        \n",
    "        Args:\n",
    "            img: (B, C, H, W) ground truth\n",
    "            sparse_input: (B, C, H, W) sparse observations\n",
    "            mask: (B, C, H, W) observation mask\n",
    "            coords: (B, H, W, 2) coordinates, optional (will be generated)\n",
    "            loss_mask: (B, C, H, W) supervision mask\n",
    "        \"\"\"\n",
    "        b, c, h, w = img.shape\n",
    "\n",
    "        def _match_channels(t, target_C):\n",
    "            if t.size(1) == target_C:\n",
    "                return t\n",
    "            if t.size(1) == 1 and target_C > 1:\n",
    "                return t.repeat(1, target_C, 1, 1)\n",
    "            if target_C == 1 and t.size(1) > 1:\n",
    "                return t.mean(dim=1, keepdim=True)\n",
    "            raise RuntimeError(f\"Channel mismatch: have {t.size(1)}, need {target_C}\")\n",
    "\n",
    "        t = torch.randint(0, self.time_step, (b,), device=img.device).long()\n",
    "        noise = torch.randn_like(img)\n",
    "        noised_image = self.q_sample(img, t, noise)\n",
    "\n",
    "        if sparse_input is not None and mask is not None:\n",
    "            model_input = torch.cat([noised_image, sparse_input, mask], dim=1)\n",
    "        else:\n",
    "            model_input = noised_image\n",
    "\n",
    "        # Generate coordinates if not provided\n",
    "        if coords is None:\n",
    "            coords = make_coordinate_grid(b, h, w, img.device)\n",
    "\n",
    "        predicted_noise = self.unet(model_input, t, coords=coords, x_coarse=perceiver_input)\n",
    "\n",
    "        if predicted_noise.size(1) != noise.size(1):\n",
    "            ref_C = max(predicted_noise.size(1), noise.size(1))\n",
    "            noise           = _match_channels(noise,           ref_C)\n",
    "            predicted_noise = _match_channels(predicted_noise, ref_C)\n",
    "\n",
    "        if mask is not None and mask.size(1) != predicted_noise.size(1):\n",
    "            mask = _match_channels(mask, predicted_noise.size(1))\n",
    "        if loss_mask is not None and loss_mask.size(1) != predicted_noise.size(1):\n",
    "            loss_mask = _match_channels(loss_mask, predicted_noise.size(1))\n",
    "\n",
    "        if self.loss_type == 'l1':\n",
    "            raw_loss = F.l1_loss(noise, predicted_noise, reduction='none')\n",
    "        elif self.loss_type == 'l2':\n",
    "            raw_loss = F.mse_loss(noise, predicted_noise, reduction='none')\n",
    "        elif self.loss_type == \"huber\":\n",
    "            raw_loss = F.smooth_l1_loss(noise, predicted_noise, reduction='none')\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        if loss_mask is not None:\n",
    "            lambda_cond = 0.05\n",
    "            combined_mask = (loss_mask + lambda_cond * mask).clamp(max=1.0)\n",
    "            loss = (raw_loss * combined_mask).sum() / combined_mask.sum().clamp_min(1e-8)\n",
    "        else:\n",
    "            loss = raw_loss.mean()\n",
    "        return loss\n",
    "\n",
    "    def linear_beta_schedule(self):\n",
    "        scale = 1000 / self.time_step\n",
    "        beta_start = scale * 0.0001\n",
    "        beta_end = scale * 0.02\n",
    "        return torch.linspace(beta_start, beta_end, self.time_step, dtype=torch.float32)\n",
    "\n",
    "\n",
    "print(\"GaussianDiffusion defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. DDIM Sampler with Coordinate Conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDIM_Sampler(nn.Module):\n",
    "    \"\"\"\n",
    "    DDIM sampler with coordinate conditioning for arbitrary resolution inference.\n",
    "    \"\"\"\n",
    "    def __init__(self, ddpm_diffusion_model, ddim_steps=50, eta=0.0, clip=True):\n",
    "        super().__init__()\n",
    "        self.model = ddpm_diffusion_model\n",
    "        self.ddim_steps = int(ddim_steps)\n",
    "        self.eta = float(eta)\n",
    "        self.clip = clip\n",
    "\n",
    "        with torch.no_grad():\n",
    "            ab = self.model.alpha_bar\n",
    "            self.register_buffer('tau',\n",
    "                torch.linspace(0, self.model.time_step-1, steps=self.ddim_steps, dtype=torch.long))\n",
    "            alpha_tau = ab[self.tau]\n",
    "            alpha_prev = F.pad(alpha_tau[:-1], (1,0), value=1.0)\n",
    "            self.register_buffer('alpha_tau', alpha_tau)\n",
    "            self.register_buffer('alpha_prev', alpha_prev)\n",
    "\n",
    "            sig = self.eta * torch.sqrt((1 - alpha_prev) / (1 - alpha_tau) * (1 - alpha_tau / alpha_prev))\n",
    "            coeff = torch.sqrt(1 - alpha_prev - sig**2)\n",
    "            self.register_buffer('sigma', sig)\n",
    "            self.register_buffer('coeff', coeff)\n",
    "            self.register_buffer('sqrt_alpha_prev', torch.sqrt(alpha_prev))\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def sample(self, batch_size, sparse_input, mask, target_size=None, min1to1=True):\n",
    "        \"\"\"\n",
    "        Sample at arbitrary resolution (zero-shot super-resolution).\n",
    "        \n",
    "        Args:\n",
    "            batch_size: int\n",
    "            sparse_input: (B, C, H_in, W_in) - sparse observations at input resolution\n",
    "            mask: (B, C, H_in, W_in) - observation mask at input resolution\n",
    "            target_size: int or tuple - target output resolution (H_out, W_out)\n",
    "                         If None, uses input resolution\n",
    "            min1to1: bool - return in [0,1] if True, else [-1,1]\n",
    "        \n",
    "        Returns:\n",
    "            samples: (B, C, H_out, W_out)\n",
    "        \"\"\"\n",
    "        device = self.model.device\n",
    "        C = self.model.channel\n",
    "        \n",
    "        # Determine target resolution\n",
    "        if target_size is None:\n",
    "            H, W = sparse_input.shape[2:]\n",
    "        elif isinstance(target_size, int):\n",
    "            H = W = target_size\n",
    "        else:\n",
    "            H, W = target_size\n",
    "        \n",
    "        # Upsample sparse input and mask to target resolution\n",
    "        if (H, W) != sparse_input.shape[2:]:\n",
    "            sparse_target = F.interpolate(sparse_input, size=(H, W), mode='bilinear', align_corners=False)\n",
    "            mask_target = F.interpolate(mask, size=(H, W), mode='nearest')\n",
    "        else:\n",
    "            sparse_target = sparse_input\n",
    "            mask_target = mask\n",
    "        \n",
    "        # Generate coordinate grid at target resolution\n",
    "        coords = make_coordinate_grid(batch_size, H, W, device)\n",
    "        \n",
    "        # Initialize noise at target resolution\n",
    "        xt = torch.randn([batch_size, C, H, W], device=device)\n",
    "        xt = xt * (1.0 - mask_target) + sparse_target * mask_target\n",
    "        xt = soft_project(xt, sparse_target, mask_target, iters=1)\n",
    "\n",
    "        # DDIM sampling loop\n",
    "        for i in reversed(range(self.ddim_steps)):\n",
    "            t = self.tau[i]\n",
    "            bt = torch.full((batch_size,), t, device=device, dtype=torch.long)\n",
    "\n",
    "            # Ensure channel match\n",
    "            sp = sparse_target if sparse_target.size(1)==C else sparse_target.repeat(1,C,1,1)\n",
    "            mk = mask_target if mask_target.size(1)==C else mask_target.repeat(1,C,1,1)\n",
    "\n",
    "            model_in = torch.cat([xt, sp, mk], dim=1)\n",
    "            pred_eps = self.model.unet(model_in, bt, coords=coords)\n",
    "\n",
    "            # x0 prediction\n",
    "            x0 = self.model.sqrt_recip_alpha_bar[t] * xt - self.model.sqrt_recip_alpha_bar_min_1[t] * pred_eps\n",
    "            if self.clip:\n",
    "                x0.clamp_(-1., 1.)\n",
    "                pred_eps = (self.model.sqrt_recip_alpha_bar[t] * xt - x0) / self.model.sqrt_recip_alpha_bar_min_1[t]\n",
    "\n",
    "            mean = self.sqrt_alpha_prev[i] * x0 + self.coeff[i] * pred_eps\n",
    "            noise = torch.randn_like(xt) if i > 0 else 0.\n",
    "            xt = mean + self.sigma[i] * noise\n",
    "\n",
    "            # Re-project to observation manifold\n",
    "            xt = xt * (1.0 - mk) + sp * mk\n",
    "\n",
    "        xt.clamp_(min=-1.0, max=1.0)\n",
    "        return (xt + 1.0)/2.0 if min1to1 else xt\n",
    "\n",
    "\n",
    "print(\"DDIM_Sampler defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STL10_32(Dataset):\n",
    "    def __init__(self, root, split='train', augment=True):\n",
    "        tfms = []\n",
    "        if augment and split == 'train':\n",
    "            tfms += [T.RandomHorizontalFlip()]\n",
    "        tfms += [\n",
    "            T.Resize((32,32), interpolation=T.InterpolationMode.BICUBIC),\n",
    "            T.ToTensor(),\n",
    "            T.Lambda(lambda t: t*2.0 - 1.0),   # [-1,1]\n",
    "        ]\n",
    "        self.tf = T.Compose(tfms)\n",
    "        self.ds = tv.datasets.STL10(root=root, split=split, download=True, transform=self.tf)\n",
    "    def __len__(self): return len(self.ds)\n",
    "    def __getitem__(self, idx):\n",
    "        x, _ = self.ds[idx]\n",
    "        return x  # [3,32,32] in [-1,1]\n",
    "\n",
    "\n",
    "class FixedMaskWrapper(Dataset):\n",
    "    \"\"\"Persistent per-item masks with fixed sparsity and 50/50 cond/supervision split.\"\"\"\n",
    "    def __init__(self, base_ds: Dataset, sparsity: float, cond_frac: float, seed: int):\n",
    "        self.base = base_ds\n",
    "        self.sparsity = sparsity\n",
    "        self.cond_frac = cond_frac\n",
    "        self.seed = seed\n",
    "        n = len(self.base)\n",
    "        x0 = self.base[0]\n",
    "        _, H, W = x0.shape\n",
    "        self.m_cond_1 = torch.zeros(n, 1, H, W, dtype=torch.float32)\n",
    "        self.m_supv_1 = torch.zeros(n, 1, H, W, dtype=torch.float32)\n",
    "        for i in range(n):\n",
    "            g = torch.Generator().manual_seed(seed + i)\n",
    "            m_full = (torch.rand(1, H, W, generator=g) < sparsity).float()\n",
    "            coords = m_full.nonzero(as_tuple=False)\n",
    "            if coords.numel() > 0:\n",
    "                idx_hw = coords[:,1]*W + coords[:,2]\n",
    "                perm = torch.randperm(idx_hw.numel(), generator=g)\n",
    "                k = int(round(cond_frac * idx_hw.numel()))\n",
    "                take = idx_hw[perm[:k]]\n",
    "                rr = (take // W).long()\n",
    "                cc = (take %  W).long()\n",
    "                m_cond = torch.zeros(1, H, W)\n",
    "                m_cond[0, rr, cc] = 1.0\n",
    "                m_supv = m_full - m_cond\n",
    "            else:\n",
    "                m_cond = torch.zeros(1, H, W)\n",
    "                m_supv = torch.zeros(1, H, W)\n",
    "            self.m_cond_1[i] = m_cond\n",
    "            self.m_supv_1[i] = m_supv\n",
    "    def __len__(self): return len(self.base)\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.base[idx]\n",
    "        return x, self.m_cond_1[idx], self.m_supv_1[idx]\n",
    "\n",
    "\n",
    "print(\"Dataset classes defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "IMG_SIZE    = 32\n",
    "CHANNELS    = 3\n",
    "MODEL_DIM   = 64\n",
    "TIMESTEPS   = 1000\n",
    "\n",
    "# Coordinate encoding\n",
    "COORD_NUM_FREQ = 10\n",
    "COORD_SCALE    = 10.0\n",
    "\n",
    "# Data\n",
    "SPARSITY    = 0.40\n",
    "COND_FRAC   = 0.50\n",
    "MASK_SEED   = 12345\n",
    "\n",
    "BATCH_TRAIN = 64\n",
    "BATCH_TEST  = 32\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "# Training\n",
    "STEPS       = 50_000  # Adjust as needed\n",
    "PRINT_EVERY = 100\n",
    "SAVE_EVERY  = 2_000\n",
    "VIS_EVERY   = 1000\n",
    "\n",
    "# Evaluation\n",
    "N_VIS           = 6\n",
    "DDIM_STEPS_VIS  = 50\n",
    "DDIM_STEPS_EVAL = 50\n",
    "MAX_TEST_BATCHES= 5\n",
    "\n",
    "# Directories\n",
    "DATA_ROOT   = \"./data_stl10\"\n",
    "CKPT_DIR    = \"./kevinref_inc_1/ckpts\"\n",
    "OUT_DIR     = \"./kevinref_inc_1/outputs\"\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Attention\n",
    "def attn_resolutions_for(image_size, levels=(1,), dim_mult=(1,2,2,2)):\n",
    "    res_list = [int(image_size / (2 ** i)) for i in range(len(dim_mult))]\n",
    "    return tuple(res_list[i] for i in levels)\n",
    "\n",
    "ATTN_LEVELS = (1,)\n",
    "attn_res = attn_resolutions_for(IMG_SIZE, ATTN_LEVELS)\n",
    "\n",
    "print(f\"Hyperparameters set. Attention at resolutions: {attn_res}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build coordinate-conditioned UNet\n",
    "net = CoordinateConditionedUnet(\n",
    "    dim=MODEL_DIM,\n",
    "    image_size=IMG_SIZE,\n",
    "    dim_multiply=(1,2,2,2),\n",
    "    channel=CHANNELS,\n",
    "    num_res_blocks=2,\n",
    "    attn_resolutions=attn_res,\n",
    "    dropout=0.0,\n",
    "    device=DEVICE,\n",
    "    groups=32,\n",
    "    coord_num_frequencies=COORD_NUM_FREQ,\n",
    "    coord_scale=COORD_SCALE\n",
    ").to(DEVICE)\n",
    "\n",
    "ddpm = GaussianDiffusion(\n",
    "    model=net,\n",
    "    image_size=IMG_SIZE,\n",
    "    time_step=TIMESTEPS,\n",
    "    loss_type='l2'\n",
    ").to(DEVICE)\n",
    "\n",
    "opt = torch.optim.AdamW(ddpm.parameters(), lr=2e-4, weight_decay=1e-4)\n",
    "\n",
    "print(f\"Model built with {sum(p.numel() for p in net.parameters())/1e6:.2f}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "base_train = STL10_32(DATA_ROOT, split='train', augment=True)\n",
    "train_ds   = FixedMaskWrapper(base_train, sparsity=SPARSITY, cond_frac=COND_FRAC, seed=MASK_SEED)\n",
    "\n",
    "# Test data\n",
    "base_test = STL10_32(DATA_ROOT, split='test', augment=False)\n",
    "test_ds   = FixedMaskWrapper(base_test, sparsity=SPARSITY, cond_frac=COND_FRAC, seed=MASK_SEED+777)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_TRAIN, shuffle=True,\n",
    "                      num_workers=NUM_WORKERS, pin_memory=True, drop_last=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=BATCH_TEST, shuffle=False,\n",
    "                     num_workers=NUM_WORKERS, pin_memory=True, drop_last=False)\n",
    "\n",
    "# Fixed visualization batch\n",
    "vis_batch = next(iter(test_dl))\n",
    "vis_x, vis_m_cond_1, _ = [v[:N_VIS] for v in vis_batch]\n",
    "\n",
    "print(f\"Training samples: {len(train_ds)}, Test samples: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training Loop\n",
    "\n",
    "Single-stage training with coordinate conditioning. The model learns continuous field representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpm.train()\n",
    "ddim_vis = DDIM_Sampler(ddpm, ddim_steps=DDIM_STEPS_VIS, eta=0.0, clip=True)\n",
    "\n",
    "it = iter(train_dl)\n",
    "for step in range(1, STEPS + 1):\n",
    "    try:\n",
    "        x, m_cond_1, m_supv_1 = next(it)\n",
    "    except StopIteration:\n",
    "        it = iter(train_dl)\n",
    "        x, m_cond_1, m_supv_1 = next(it)\n",
    "\n",
    "    x = x.to(DEVICE)                       # [B,3,32,32]\n",
    "    m_cond_1 = m_cond_1.to(DEVICE)         # [B,1,32,32]\n",
    "    m_supv_1 = m_supv_1.to(DEVICE)         # [B,1,32,32]\n",
    "\n",
    "    B, C, H, W = x.shape\n",
    "    m_cond = m_cond_1.repeat(1, C, 1, 1)   # [B,3,H,W]\n",
    "    m_supv = m_supv_1.repeat(1, C, 1, 1)   # [B,3,H,W]\n",
    "    x_sparse = x * m_cond\n",
    "\n",
    "    # Generate coordinates for training resolution\n",
    "    coords = make_coordinate_grid(B, H, W, DEVICE)\n",
    "\n",
    "    # Forward pass with coordinates\n",
    "    loss = ddpm(img=x, sparse_input=x_sparse, mask=m_cond, coords=coords,\n",
    "                perceiver_input=None, loss_mask=m_supv)\n",
    "\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(ddpm.parameters(), 1.0)\n",
    "    opt.step()\n",
    "\n",
    "    if step % PRINT_EVERY == 0:\n",
    "        print(f\"[{step}/{STEPS}] loss={loss.item():.4f}\")\n",
    "\n",
    "    # Periodic visualization at 32×32\n",
    "    if step % VIS_EVERY == 0 or step == 1:\n",
    "        ddpm.eval()\n",
    "        with torch.inference_mode():\n",
    "            vx = vis_x.to(DEVICE)\n",
    "            vm = vis_m_cond_1.to(DEVICE)\n",
    "            vm3 = vm.repeat(1, CHANNELS, 1, 1)\n",
    "            v_sparse = vx * vm3\n",
    "            \n",
    "            # Reconstruct at native 32×32\n",
    "            recon01 = ddim_vis.sample(batch_size=vx.size(0),\n",
    "                                      sparse_input=v_sparse,\n",
    "                                      mask=vm3,\n",
    "                                      target_size=32,\n",
    "                                      min1to1=True)\n",
    "            \n",
    "            gt01 = (vx + 1.0)/2.0\n",
    "            sp01 = (v_sparse + 1.0)/2.0\n",
    "            out_path = os.path.join(OUT_DIR, f\"train_vis_step{step}_32x32.png\")\n",
    "            save_grid01([gt01.cpu(), sp01.cpu(), recon01.cpu()], out_path, nrow=min(N_VIS, 6))\n",
    "            print(f\"Saved vis → {out_path}\")\n",
    "        ddpm.train()\n",
    "\n",
    "    if step % SAVE_EVERY == 0 or step == STEPS:\n",
    "        torch.save({'net': net.state_dict()}, os.path.join(CKPT_DIR, f\"net_step{step}.pt\"))\n",
    "        print(f\"Checkpoint saved at step {step}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Zero-Shot Multi-Resolution Inference\n",
    "\n",
    "Test the trained model at 32×32, 64×64, and 96×96 on **unseen** test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint\n",
    "ckpt_path = os.path.join(CKPT_DIR, f\"net_step{STEPS}.pt\")\n",
    "state = torch.load(ckpt_path, map_location='cpu')\n",
    "net.load_state_dict(state['net'], strict=True)\n",
    "ddpm.eval()\n",
    "\n",
    "ddim_eval = DDIM_Sampler(ddpm, ddim_steps=DDIM_STEPS_EVAL, eta=0.0, clip=True)\n",
    "\n",
    "print(\"Model loaded. Ready for zero-shot super-resolution inference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on first test batch at multiple resolutions\n",
    "test_batch = next(iter(test_dl))\n",
    "x_test, m_cond_1_test, _ = test_batch\n",
    "x_test = x_test[:N_VIS].to(DEVICE)\n",
    "m_cond_1_test = m_cond_1_test[:N_VIS].to(DEVICE)\n",
    "\n",
    "m_cond_test = m_cond_1_test.repeat(1, CHANNELS, 1, 1)\n",
    "x_sparse_test = x_test * m_cond_test\n",
    "\n",
    "print(\"Testing zero-shot super-resolution at multiple resolutions...\")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    # 32×32 (native training resolution)\n",
    "    print(\"Reconstructing at 32×32...\")\n",
    "    recon_32 = ddim_eval.sample(\n",
    "        batch_size=x_test.size(0),\n",
    "        sparse_input=x_sparse_test,\n",
    "        mask=m_cond_test,\n",
    "        target_size=32,\n",
    "        min1to1=True\n",
    "    )\n",
    "    \n",
    "    # 64×64 (zero-shot 2× super-resolution)\n",
    "    print(\"Zero-shot super-resolution to 64×64...\")\n",
    "    recon_64 = ddim_eval.sample(\n",
    "        batch_size=x_test.size(0),\n",
    "        sparse_input=x_sparse_test,\n",
    "        mask=m_cond_test,\n",
    "        target_size=64,\n",
    "        min1to1=True\n",
    "    )\n",
    "    \n",
    "    # 96×96 (zero-shot 3× super-resolution)\n",
    "    print(\"Zero-shot super-resolution to 96×96...\")\n",
    "    recon_96 = ddim_eval.sample(\n",
    "        batch_size=x_test.size(0),\n",
    "        sparse_input=x_sparse_test,\n",
    "        mask=m_cond_test,\n",
    "        target_size=96,\n",
    "        min1to1=True\n",
    "    )\n",
    "\n",
    "# Save results\n",
    "gt01 = (x_test + 1.0) / 2.0\n",
    "sp01 = (x_sparse_test + 1.0) / 2.0\n",
    "\n",
    "# Upsample ground truth for comparison\n",
    "gt64 = F.interpolate(gt01, size=(64, 64), mode='bicubic', align_corners=False)\n",
    "gt96 = F.interpolate(gt01, size=(96, 96), mode='bicubic', align_corners=False)\n",
    "sp64 = F.interpolate(sp01, size=(64, 64), mode='bilinear', align_corners=False)\n",
    "sp96 = F.interpolate(sp01, size=(96, 96), mode='bilinear', align_corners=False)\n",
    "\n",
    "# Save grids\n",
    "save_grid01([gt01.cpu(), sp01.cpu(), recon_32.cpu()],\n",
    "            os.path.join(OUT_DIR, \"eval_32x32.png\"), nrow=N_VIS)\n",
    "save_grid01([gt64.cpu(), sp64.cpu(), recon_64.cpu()],\n",
    "            os.path.join(OUT_DIR, \"eval_64x64.png\"), nrow=N_VIS)\n",
    "save_grid01([gt96.cpu(), sp96.cpu(), recon_96.cpu()],\n",
    "            os.path.join(OUT_DIR, \"eval_96x96.png\"), nrow=N_VIS)\n",
    "\n",
    "print(\"Results saved:\")\n",
    "print(f\"  - {OUT_DIR}/eval_32x32.png (native resolution)\")\n",
    "print(f\"  - {OUT_DIR}/eval_64x64.png (zero-shot 2× super-resolution)\")\n",
    "print(f\"  - {OUT_DIR}/eval_96x96.png (zero-shot 3× super-resolution)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results in notebook\n",
    "fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "\n",
    "for i in range(3):\n",
    "    # Show one example at each resolution\n",
    "    idx = i\n",
    "    \n",
    "    # 32×32\n",
    "    axes[0, i].imshow(recon_32[idx].cpu().permute(1, 2, 0).numpy())\n",
    "    axes[0, i].set_title(f\"32×32 (sample {idx})\")\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # 64×64\n",
    "    axes[1, i].imshow(recon_64[idx].cpu().permute(1, 2, 0).numpy())\n",
    "    axes[1, i].set_title(f\"64×64 (sample {idx})\")\n",
    "    axes[1, i].axis('off')\n",
    "    \n",
    "    # 96×96\n",
    "    axes[2, i].imshow(recon_96[idx].cpu().permute(1, 2, 0).numpy())\n",
    "    axes[2, i].set_title(f\"96×96 (sample {idx})\")\n",
    "    axes[2, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"multi_resolution_comparison.png\"), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"Zero-shot super-resolution complete!\")\n",
    "print(f\"Trained once at 32×32, successfully inferred at 32×32, 64×64, and 96×96.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Metrics (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psnr_batch(pred, target):\n",
    "    \"\"\"Compute PSNR for batch in [0,1] range.\"\"\"\n",
    "    mse = F.mse_loss(pred, target, reduction='mean')\n",
    "    if mse < 1e-10:\n",
    "        return 99.0\n",
    "    return 10.0 * math.log10(1.0 / mse.item())\n",
    "\n",
    "# Compute PSNR at each resolution\n",
    "psnr_32 = psnr_batch(recon_32, gt01)\n",
    "psnr_64 = psnr_batch(recon_64, gt64)\n",
    "psnr_96 = psnr_batch(recon_96, gt96)\n",
    "\n",
    "print(f\"\\nPSNR Results (on {N_VIS} test samples):\")\n",
    "print(f\"  32×32: {psnr_32:.2f} dB\")\n",
    "print(f\"  64×64: {psnr_64:.2f} dB (zero-shot)\")\n",
    "print(f\"  96×96: {psnr_96:.2f} dB (zero-shot)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What we achieved:**\n",
    "1. ✅ Single-stage training on 32×32 STL-10 images\n",
    "2. ✅ Coordinate conditioning via Fourier positional encoding\n",
    "3. ✅ Zero-shot inference at arbitrary resolutions (32×32, 64×64, 96×96)\n",
    "4. ✅ Sparse field reconstruction + super-resolution in one model\n",
    "\n",
    "**Key innovations:**\n",
    "- `FourierCoordinateEncoding`: Maps (x, y) → high-dim features for continuous representation\n",
    "- `CoordinateConditionedUnet`: Accepts coordinate embeddings alongside image data\n",
    "- `DDIM_Sampler`: Modified for arbitrary target resolution with coordinate grids\n",
    "\n",
    "**Next steps:**\n",
    "- Train for more steps (50K → 100K) for better quality\n",
    "- Experiment with coordinate encoding frequencies/scales\n",
    "- Test on other datasets and sparsity levels\n",
    "- Add multi-scale training (random crops) for robustness"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
